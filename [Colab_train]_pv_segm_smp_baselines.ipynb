{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Cfy_JHIes22"
   },
   "source": [
    "# Baseline Computer Vision methods for Planetary-scale training for National or Regional PV Solar Panel surveys\n",
    "\n",
    "*CCOM6120: Computer Vision*  \n",
    "**Alejandro Vega Nogales**  \n",
    "*Data Scientist @ Maxar Puerto Rico*   \n",
    "*CCOM MS Student*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TNu-qJKye5YU",
    "outputId": "3dd46f05-2103-44cd-8ca1-06d782ebfa95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CoLab\n",
      "Mounted at /content/drive\n",
      " .   ..  '='   .config\t datasets   drive   .env   report   sample_data   utils\n"
     ]
    }
   ],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    import os\n",
    "    import random\n",
    "    import shutil\n",
    "    print('Running on CoLab')\n",
    "    from google.colab import drive\n",
    "    from google.colab import userdata\n",
    "    import secrets\n",
    "\n",
    "    # enable TPU accelerators\n",
    "    # import torch\n",
    "    # import torch_xla.core.xla_model as xm\n",
    "    # used to access colab runtime from our IDE\n",
    "    # !pip install colabcode\n",
    "    # from colabcode import ColabCode\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "    # create dirs to mirror local file tree\n",
    "    os.makedirs('/content/datasets/raw/labels', exist_ok=True)\n",
    "    os.makedirs('/content/datasets/raw/images', exist_ok=True)\n",
    "\n",
    "    # copy over report folder for figures and other assets, and utils scripts\n",
    "    os.makedirs('/content/report', exist_ok=True)\n",
    "    if not os.path.exists('/content/report/assets'):\n",
    "        shutil.copytree('/content/drive/MyDrive/CCOM_MS_Training_Datasets_Logs/report/assets', '/content/report/assets')\n",
    "    if not os.path.exists('/content/utils'):\n",
    "        shutil.copytree('/content/drive/MyDrive/CCOM_MS_Training_Datasets_Logs/utils', '/content/utils')\n",
    "    if not os.path.exists('./.env'):\n",
    "        shutil.copy('/content/drive/MyDrive/CCOM_MS_Training_Datasets_Logs/.env', './.env')\n",
    "\n",
    "    !ls -a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "XeL-jGK_JEUO"
   },
   "outputs": [],
   "source": [
    "\n",
    "colab_train_requirements = [\n",
    "    'numpy>=2.0.0',\n",
    "    'rasterio' # we'll be sourcing from several tiff datasets\n",
    "    , '\"segmentation_models_pytorch>=0.5\"' # for all our segmentation models; need ver >= 0.5 for ViT models\n",
    "    , 'ultralytics' # for potential YOLO finetuning\n",
    "    , '\"lightning>=2.5.1\"'\n",
    "    , 'seedir' # package for creating, editing, and reading folder tree diagrams; not used here but imported in utils\n",
    "    , 'datahugger' # same as above\n",
    "    , 'dotenv'\n",
    "    , 'coremltools'\n",
    "    , 'cubo',\n",
    "    'rioxarray'\n",
    "    ,'pystac_client'\n",
    "]\n",
    "\n",
    "!pip install {' '.join(colab_train_requirements)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oSryrgXves23"
   },
   "outputs": [],
   "source": [
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "# %% --- 1. Setup & Imports ---\n",
    "# Standard libraries\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import json\n",
    "import shutil\n",
    "import traceback\n",
    "import random\n",
    "from glob import glob\n",
    "\n",
    "# Geospatial libraries\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterio.transform import Affine # For geotransforms\n",
    "import cubo\n",
    "import rioxarray # cubo often returns DataArrays that need rioxarray for CRS handling\n",
    "import pystac_client # For STAC API interaction with cubo\n",
    "\n",
    "# ML/DL libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "import segmentation_models_pytorch as smp\n",
    "from segmentation_models_pytorch.losses import DiceLoss, SoftBCEWithLogitsLoss\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger, TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "from torchmetrics import MetricCollection\n",
    "from torchmetrics.classification import BinaryJaccardIndex, BinaryAccuracy, BinaryPrecision, BinaryRecall, BinaryF1Score\n",
    "from lightning.pytorch import Trainer\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, IFrame\n",
    "\n",
    "from utils.fetch_and_preprocess import (\n",
    "    yolo_labels_to_binary_mask,\n",
    "    process_yolo_label_directory\n",
    ")\n",
    "from utils.torch_datasets import PVSegmentationDataset, PVSegmentationDataModule\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# --- Suppress specific warnings ---\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"rasterio\")\n",
    "warnings.filterwarnings(\"ignore\", message=\".* Shapely GEOS version .*\")\n",
    "# Suppress the NotGeoreferencedWarning specifically\n",
    "warnings.filterwarnings(\"ignore\", category=rasterio.errors.NotGeoreferencedWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "s0gqLv6nfOwV"
   },
   "outputs": [],
   "source": [
    "\n",
    "# !pip install -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PXBB4uP-es22"
   },
   "source": [
    "## Research Problem & Motivation\n",
    "\n",
    "### Importance of monitoring and tracking Renewable Energy Installations\n",
    "\n",
    "As the world transitions to renewable energy sources, solar photovoltaic (PV) installations are growing exponentially worldwide. Accurate mapping and monitoring of these installations  \n",
    "is crucial for energy planning, grid management, carbon accounting, and sustainable development. However, traditional methods of tracking PV installations rely on incomplete permit data,  \n",
    "manual surveys, or voluntary reporting—all of which present significant gaps in coverage and accuracy.\n",
    "\n",
    "### Global Significance\n",
    "\n",
    "Accurate mapping of solar PV installations has far-reaching implications:\n",
    "\n",
    "- **Energy transition monitoring**:     Tracking actual deployment rates of solar PV against climate targets\n",
    "- **Grid integration**:\n",
    "    - Precise location of distributed energy resources for power system planning\n",
    "    - Short-term forecasting of PV power generated (via solar flux estimates) for countries with high PV integration in their grid is essential for maintaining grid stability\n",
    "- **Environmental impact assessment**:\n",
    "    - Understanding land use changes and habitat effects of renewable energy development\n",
    "- **Socioeconomic analysis**:\n",
    "    - Studying adoption patterns across different communities to inform equitable energy transition policies\n",
    "- **Sustainable Development Goals**:\n",
    "   - Contributing to efforts that can directly address the UN's SDG 7 (Affordable and Clean Energy) and SDG 13 (Climate Action)\n",
    "\n",
    "### The role of Remote Sensing and Computer Vision\n",
    "\n",
    "Remote sensing and computer vision techniques can provide a scalable and efficient solution to monitor and map solar PV installations. By leveraging high-resolution satellite imagery, we can automate the detection and classification of solar panels, enabling large-scale assessments of solar energy deployment.\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "01DIO6IWes23"
   },
   "source": [
    "### The Challenge of PV Segmentation\n",
    "\n",
    "Satellite imagery offers a promising solution for automated detection of PV installations at regional and global scales. Yet, several challenges make this a non-trivial computer vision problem:\n",
    "\n",
    "1. **Multi-scale challenge**: PV installations vary dramatically in size, from small residential rooftop panels (a few m²) to utility-scale solar farms (several km²)\n",
    "2. **Visual variability**: PV panels appear differently depending on panel type, orientation, age, viewing angle, and illumination conditions\n",
    "3. **Resolution trade-offs**: As demonstrated in Clark et al.'s study (2023), detection performance is strongly affected by image resolution, creating a compromise between coverage area and detection accuracy\n",
    "4. **Class imbalance**: PV installations typically occupy a small fraction of any given geographic area, creating extreme class imbalance in training data\n",
    "5. **Data scarcity**: High-quality labeled datasets for training are limited and geographically biased toward certain regions\n",
    "\n",
    "<figure style=\"text-align: center\">\n",
    "<img src=\"report/assets/figures/Munich_2021-06-18_WV03_HD_16x9.jpg\" style=\"width:70%; height:auto;\">\n",
    "<figcaption align = \"center\"> 31cm native resolution vs simulated \"15.5\"cm spatial resolution from our dataset</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 990,
     "referenced_widgets": [
      "ab563228053d4483930b3a055dbb7512",
      "790aad4163374922920a62596c14f39f"
     ]
    },
    "id": "jFtcBKsSasdy",
    "outputId": "cc3092a7-a643-4243-878e-46bbe626b818"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab563228053d4483930b3a055dbb7512",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x00H\\x00H\\x00\\x00\\xff\\xdb\\x00C\\x00\\x02\\x01\\x01\\x02\\…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# html in markdown doesn't seem to render in colab; display images for presentation via iPython\n",
    "img = widgets.Image(value=open(\"report/assets/figures/Munich_2021-06-18_WV03_HD_16x9.jpg\", \"rb\").read(), format='jpg', width='95%', height='auto')\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K7H8XsoFes23"
   },
   "source": [
    "### Related Work and Background\n",
    "\n",
    "In their seminal work with very high resolution (VHR) satellite imagery *(< 1 meter/pixel*), Cecilia Clark and Fabio Pacifici (2023) demonstrated that resolution significantly impacts detection performance, with our employer's (Maxar Intelligence) [\"HD Technology\" product](https://blog.maxar.com/tech-and-tradecraft/2022/maxars-hd-technology-provides-measurable-improvements-in-machine-learning-applications) (proprietary upscaling algorithm capable of simulating 15.5cm GSD) delivering substantially better results than native resolution (31cm) imagery. This **resolution-performance trade-off** informs our approach to developing models that can work effectively across varying image resolutions. They summarize the challenges succinctly below:\n",
    "\n",
    "**\"Residential solar panels are considered small, weak targets even in VHR satellite imagery due to the average number of pixels per object, variation among  \n",
    "objects, and complex context**. *Existing satellite imagery datasets often include large-scale, or non-residential, solar panel annotations* due to resolution  \n",
    "of the imagery and therefore ability to detect small objects. There are available datasets of VHR imagery to support accurate detection\n",
    "of small-scale and residential installations, but **the imagery is generally sourced from aerial platforms.**\"\n",
    "\n",
    "By establishing baseline segmentation techniques in preparation for more advanced models, this research contributes to the broader goal of creating comprehensive, accurate, and timely inventories of global PV installations—a critical capability for managing the ongoing energy transition.\n",
    "\n",
    "<!-- <figure style=\"text-align: center\">\n",
    "<img src=\"https://github.com/avega17/CCOM_MS_Spring_2025_EO_PV_research/blob/main/report/assets/figures/maxar_hd_vs_native_comparison.jpg?raw=1\" style=\"width:70%; height:auto;\">\n",
    "</figure>\n",
    "<figure style=\"text-align: center\">\n",
    "<img src=\"https://github.com/avega17/CCOM_MS_Spring_2025_EO_PV_research/blob/main/report/assets/figures/maxar_hd_vs_native_comparison2.jpg?raw=1\" style=\"width:70%; height:auto;\">\n",
    "<figcaption align = \"center\"> 31cm native resolution vs simulated \"15.5\"cm spatial resolution from our dataset</figcaption> -->\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "450b0332bec54d2eb515ae3b5342c2ae",
      "dfbcb8cf4df44919974eb717221d1e6c",
      "e29e78348af049fb9488b118e0b481eb",
      "7f8510d346e444d6a844b1b9a4014531",
      "495ff8c2d24b4313b1b871a1f3e594f3",
      "525ce3fbb1a2450eafaed9a5e17faa50"
     ]
    },
    "id": "tHWvdjmNb-HV",
    "outputId": "50b745b2-f077-43e5-e684-a3091f41b9e1"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "450b0332bec54d2eb515ae3b5342c2ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Image(value=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x00\\xf0\\x00\\xf0\\x00\\x00\\xff\\xdb\\x00C…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# simulate 2x1 div like below\n",
    "img1 = widgets.Image(value=open(\"report/assets/figures/maxar_hd_vs_native_comparison.jpg\", \"rb\").read(), format='jpg', width='90%', height='auto')\n",
    "img2 = widgets.Image(value=open(\"report/assets/figures/maxar_hd_vs_native_comparison2.jpg\", \"rb\").read(), format='jpg', width='90%', height='auto')\n",
    "imgs_rows = widgets.VBox([img1, img2])\n",
    "# add caption\n",
    "\n",
    "display(imgs_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z8DGTPO9es23"
   },
   "source": [
    "### Research Questions\n",
    "\n",
    "This project aims to address the following key questions:\n",
    "\n",
    "1. How can we leverage *SOTA DL architectures* to improve PV segmentation accuracy across diverse geographic regions including **regions with sparse data**?\n",
    "\n",
    "2. Can *PyTorch's Lightning framework* enable more **efficient experimentation across multiple model architectures** to efficiently identify optimal approaches for this domain-specific problem?\n",
    "\n",
    "3. What **combination of data augmentation strategies, model architectures, loss functions, and training approaches** best addresses the unique challenges of PV segmentation?\n",
    "\n",
    "4. How can we optimize models to work effectively across **different spatial resolutions** while maximizing the area that can be covered in operational settings?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r0Fa10SGes23"
   },
   "source": [
    "<!-- <figure style=\"text-align: center\">\n",
    "<div style=\"display: flex; flex-direction: row; gap: 20px; width: 100%;\">\n",
    "    <div style=\"flex: 1; text-align: center;\">\n",
    "        <img src=\"report/assets/figures/Planet_MS_global_renewables_watch_lo_res_comparison.png\" style=\"width: 100%; height: auto;\">\n",
    "        <p>Imagery available in Global Renewables Watch atlas</p>\n",
    "    </div>\n",
    "    <div style=\"flex: 1; text-align: center;\">\n",
    "        <img src=\"report/assets/figures/maxar_mgp_renewables_planet_comparison.png\" style=\"width: 100%; height: auto;\">\n",
    "        <p> Blue + RedEdge + NIR composite from Maxar Geospatial Platform </p>\n",
    "    </div>\n",
    "</div>\n",
    "<figcaption align = \"center\">  PV imagery: PlanetLabs Dove vs Maxar Worldview-3  </figcaption>\n",
    "</figure> -->\n",
    "\n",
    "\n",
    "<!-- <div style=\"display: flex; flex-direction: row; gap: 20px; width: 100%;\">\n",
    "    <div style=\"flex: 1; text-align: center;\">\n",
    "        <img src=\"report/assets/figures/unetplusplus_architecture.png\" style=\"width: 100%; height: auto;\">\n",
    "        <p>UNet++ Architecture</p>\n",
    "    </div>\n",
    "    <div style=\"flex: 1; text-align: center;\">\n",
    "        <img src=\"report/assets/figures/deeplabv3plus_architecture.png\" style=\"width: 100%; height: auto;\">\n",
    "        <p>DeepLabV3+ Architecture</p>\n",
    "    </div>\n",
    "</div> -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 676,
     "referenced_widgets": [
      "4eb294cc7ec141ff928184fbb181bdf8",
      "1a78d9f6339348a5aab198975ba47021",
      "4428245d00a74eebb6cef68d6718798e",
      "7cb51c19393b4844a348063a1f704a1b",
      "140645207fdc4a74b41695991987be82",
      "f8d285a41d3d497a8b7f42f2a681e56b",
      "574ea374e63d493b9a166e97dcb7b878",
      "425a6eb80b0b46cb9709173f8998d467",
      "7944445abc9c4d7aa22f09498b0283a5",
      "8a47f9d00cc846cab947bc67c7f79e70",
      "ef1c2c06c3704dc9adfcb8711fca8760"
     ]
    },
    "id": "qmZX5eY-dOqn",
    "outputId": "e73d3f0b-d4a9-4502-df77-81ba584f0e1d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eb294cc7ec141ff928184fbb181bdf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x07\\xf1\\x00\\x00\\x05;\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# simulate 1x2 div layout like above\n",
    "img1 = widgets.Image(value=open('report/assets/figures/Planet_MS_global_renewables_watch_lo_res_comparison.png', \"rb\").read(), format='png', width='50%', height='auto')\n",
    "img2 = widgets.Image(value=open('report/assets/figures/maxar_mgp_renewables_planet_comparison.png', \"rb\").read(), format='png', width='50%', height='auto')\n",
    "imgs_cols = widgets.HBox([img1, img2], layout=widgets.Layout(justify_content='center'))\n",
    "# add html caption\n",
    "fig_caption = widgets.HTML('<p>PV imagery: PlanetLabs Dove vs Maxar Worldview-3</p>')\n",
    "figure = widgets.VBox([imgs_cols, fig_caption])\n",
    "# see Box layout manipulation docs: https://ipywidgets.readthedocs.io/en/stable/examples/Widget%20Layout.html#the-flexbox-layout\n",
    "figure.layout.display = 'flex'\n",
    "figure.layout.flex_flow = 'column'\n",
    "figure.layout.align_items = 'center'\n",
    "figure.layout.width = '100%'\n",
    "display(figure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VxwWDGwOes23"
   },
   "source": [
    "# PV Segmentation with Torch Lightning & SMP\n",
    "\n",
    "This notebook provides a framework for training and evaluating deep learning models for solar photovoltaic (PV) panel segmentation from satellite imagery. It leverages the power and flexibility of PyTorch Lightning for streamlined training and `segmentation-models-pytorch` (SMP) for easy access to a wide variety of cutting-edge model architectures.\n",
    "\n",
    "This notebook will demonstrate the benefits of using PyTorch Lightning for organizing code and simplifying complex training workflows, and leveraging `segmentation-models-pytorch` (SMP) allows for rapid experimentation with different model backbones and architectures.\n",
    "\n",
    "**Key Goals for this Notebook:**\n",
    "1.  **Data Preparation:** Set up a PyTorch Lightning DataModule to efficiently load and preprocess image patches and their corresponding masks derived from the datasets listed above (or similar).\n",
    "2.  **Model Definition:** Define a reusable PyTorch LightningModule that can accommodate various segmentation architectures from `segmentation-models-pytorch`.\n",
    "3.  **Training:** Execute training loops, leveraging PyTorch Lightning's features for hardware acceleration (including Apple Silicon MPS), logging, and checkpointing.\n",
    "4.  **Evaluation:** Assess model performance using relevant metrics and visualize predictions.\n",
    "5.  **Logging with Weights & Biases:** Track experiments and log metrics using Weights & Biases.\n",
    "\n",
    "**Assumptions for this Notebook:**\n",
    "*   Image patches and their corresponding binary masks are assumed to be pre-prepared (e.g., using the `fetch-pv-datasets-ESDA.ipynb` notebook or similar methods) and stored in specified directories.\n",
    "*   The notebook is designed to be adaptable for different datasets, such as those derived from Maxar imagery with YOLO labels converted to masks, or other datasets providing pixel-coordinate labels that have been rasterized to masks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lRlWZDP5aegU",
    "outputId": "07304d0d-e818-44ca-9f89-6870afe21f11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA L4\n",
      "Mon May 12 13:48:27 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA L4                      Off |   00000000:00:03.0 Off |                    0 |\n",
      "| N/A   47C    P8             13W /   72W |       3MiB /  23034MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# display gpu runtime to inform model and encoder choice\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bSizpPcbK2gB",
    "outputId": "5e93b82e-414a-44e5-ade0-9a079c29ce05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ItemsView(environ({'SHELL': '/bin/bash', 'NV_LIBCUBLAS_VERSION': '12.5.3.2-1', 'NVIDIA_VISIBLE_DEVICES': 'all', 'COLAB_JUPYTER_TRANSPORT': 'ipc', 'NV_NVML_DEV_VERSION': '12.5.82-1', 'NV_CUDNN_PACKAGE_NAME': 'libcudnn9-cuda-12', 'CGROUP_MEMORY_EVENTS': '/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events', 'NV_LIBNCCL_DEV_PACKAGE': 'libnccl-dev=2.22.3-1+cuda12.5', 'NV_LIBNCCL_DEV_PACKAGE_VERSION': '2.22.3-1', 'VM_GCE_METADATA_HOST': '169.254.169.253', 'HOSTNAME': '8156b25042a2', 'LANGUAGE': 'en_US', 'TBE_RUNTIME_ADDR': '172.28.0.1:8011', 'COLAB_TPU_1VM': '', 'GCE_METADATA_TIMEOUT': '3', 'NVIDIA_REQUIRE_CUDA': 'cuda>=12.5 brand=unknown,driver>=470,driver<471 brand=grid,driver>=470,driver<471 brand=tesla,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=vapps,driver>=470,driver<471 brand=vpc,driver>=470,driver<471 brand=vcs,driver>=470,driver<471 brand=vws,driver>=470,driver<471 brand=cloudgaming,driver>=470,driver<471 brand=unknown,driver>=535,driver<536 brand=grid,driver>=535,driver<536 brand=tesla,driver>=535,driver<536 brand=nvidia,driver>=535,driver<536 brand=quadro,driver>=535,driver<536 brand=quadrortx,driver>=535,driver<536 brand=nvidiartx,driver>=535,driver<536 brand=vapps,driver>=535,driver<536 brand=vpc,driver>=535,driver<536 brand=vcs,driver>=535,driver<536 brand=vws,driver>=535,driver<536 brand=cloudgaming,driver>=535,driver<536 brand=unknown,driver>=550,driver<551 brand=grid,driver>=550,driver<551 brand=tesla,driver>=550,driver<551 brand=nvidia,driver>=550,driver<551 brand=quadro,driver>=550,driver<551 brand=quadrortx,driver>=550,driver<551 brand=nvidiartx,driver>=550,driver<551 brand=vapps,driver>=550,driver<551 brand=vpc,driver>=550,driver<551 brand=vcs,driver>=550,driver<551 brand=vws,driver>=550,driver<551 brand=cloudgaming,driver>=550,driver<551', 'NV_LIBCUBLAS_DEV_PACKAGE': 'libcublas-dev-12-5=12.5.3.2-1', 'NV_NVTX_VERSION': '12.5.82-1', 'COLAB_JUPYTER_IP': '172.28.0.12', 'NV_CUDA_CUDART_DEV_VERSION': '12.5.82-1', 'NV_LIBCUSPARSE_VERSION': '12.5.1.3-1', 'COLAB_LANGUAGE_SERVER_PROXY_ROOT_URL': 'http://172.28.0.1:8013/', 'NV_LIBNPP_VERSION': '12.3.0.159-1', 'NCCL_VERSION': '2.22.3-1', 'KMP_LISTEN_PORT': '6000', 'TF_FORCE_GPU_ALLOW_GROWTH': 'true', 'ENV': '/root/.bashrc', 'PWD': '/', 'TBE_EPHEM_CREDS_ADDR': '172.28.0.1:8009', 'COLAB_LANGUAGE_SERVER_PROXY_REQUEST_TIMEOUT': '30s', 'TBE_CREDS_ADDR': '172.28.0.1:8008', 'NV_CUDNN_PACKAGE': 'libcudnn9-cuda-12=9.2.1.18-1', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'COLAB_JUPYTER_TOKEN': '', 'LAST_FORCED_REBUILD': '20250424', 'NV_NVPROF_DEV_PACKAGE': 'cuda-nvprof-12-5=12.5.82-1', 'NV_LIBNPP_PACKAGE': 'libnpp-12-5=12.3.0.159-1', 'NV_LIBNCCL_DEV_PACKAGE_NAME': 'libnccl-dev', 'TCLLIBPATH': '/usr/share/tcltk/tcllib1.20', 'NV_LIBCUBLAS_DEV_VERSION': '12.5.3.2-1', 'NVIDIA_PRODUCT_NAME': 'CUDA', 'COLAB_KERNEL_MANAGER_PROXY_HOST': '172.28.0.12', 'UV_BUILD_CONSTRAINT': '', 'NV_LIBCUBLAS_DEV_PACKAGE_NAME': 'libcublas-dev-12-5', 'NV_CUDA_CUDART_VERSION': '12.5.82-1', 'COLAB_WARMUP_DEFAULTS': '1', 'HOME': '/root', 'LANG': 'en_US.UTF-8', 'CUDA_VERSION': '12.5.1', 'CLOUDSDK_CONFIG': '/content/.config', 'NV_LIBCUBLAS_PACKAGE': 'libcublas-12-5=12.5.3.2-1', 'NV_CUDA_NSIGHT_COMPUTE_DEV_PACKAGE': 'cuda-nsight-compute-12-5=12.5.1-1', 'UV_SYSTEM_PYTHON': 'true', 'COLAB_RELEASE_TAG': 'release-colab_20250508-060105_RC00', 'KMP_TARGET_PORT': '9000', 'KMP_EXTRA_ARGS': '--logtostderr --listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https://colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-l4-s-uadyckqwxx6g --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true ', 'UV_INSTALL_DIR': '/usr/local/bin', 'NV_LIBNPP_DEV_PACKAGE': 'libnpp-dev-12-5=12.3.0.159-1', 'COLAB_LANGUAGE_SERVER_PROXY_LSP_DIRS': '/datalab/web/pyright/typeshed-fallback/stdlib,/usr/local/lib/python3.10/dist-packages', 'NV_LIBCUBLAS_PACKAGE_NAME': 'libcublas-12-5', 'COLAB_KERNEL_MANAGER_PROXY_PORT': '6000', 'CLOUDSDK_PYTHON': 'python3', 'NV_LIBNPP_DEV_VERSION': '12.3.0.159-1', 'NO_GCE_CHECK': 'False', 'PYTHONPATH': '/env/python', 'NV_LIBCUSPARSE_DEV_VERSION': '12.5.1.3-1', 'LIBRARY_PATH': '/usr/local/cuda/lib64/stubs', 'NV_CUDNN_VERSION': '9.2.1.18-1', 'SHLVL': '0', 'NV_CUDA_LIB_VERSION': '12.5.1-1', 'COLAB_LANGUAGE_SERVER_PROXY': '/usr/colab/bin/language_service', 'NVARCH': 'x86_64', 'UV_CONSTRAINT': '', 'PYTHONUTF8': '1', 'NV_CUDNN_PACKAGE_DEV': 'libcudnn9-dev-cuda-12=9.2.1.18-1', 'NV_LIBNCCL_PACKAGE': 'libnccl2=2.22.3-1+cuda12.5', 'LD_LIBRARY_PATH': '/usr/local/lib/python3.11/dist-packages/cv2/../../lib64:/usr/lib64-nvidia', 'COLAB_GPU': '1', 'NV_CUDA_NSIGHT_COMPUTE_VERSION': '12.5.1-1', 'GCS_READ_CACHE_BLOCK_SIZE_MB': '16', 'NV_NVPROF_VERSION': '12.5.82-1', 'LC_ALL': 'en_US.UTF-8', 'COLAB_FILE_HANDLER_ADDR': 'localhost:3453', 'PATH': '/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin', 'NV_LIBNCCL_PACKAGE_NAME': 'libnccl2', 'COLAB_DEBUG_ADAPTER_MUX_PATH': '/usr/local/bin/dap_multiplexer', 'NV_LIBNCCL_PACKAGE_VERSION': '2.22.3-1', 'PYTHONWARNINGS': 'ignore:::pip._internal.cli.base_command', 'DEBIAN_FRONTEND': 'noninteractive', 'COLAB_BACKEND_VERSION': 'next', 'OLDPWD': '/', '_PYVIZ_COMMS_INSTALLED': '1', 'PYDEVD_USE_FRAME_EVAL': 'NO', 'JPY_PARENT_PID': '131', 'TERM': 'xterm-color', 'CLICOLOR': '1', 'PAGER': 'cat', 'GIT_PAGER': 'cat', 'MPLBACKEND': 'module://matplotlib_inline.backend_inline', 'ENABLE_DIRECTORYPREFETCHER': '1', 'USE_AUTH_EPHEM': '1', 'GDAL_CURL_CA_BUNDLE': '/usr/local/lib/python3.11/dist-packages/certifi/cacert.pem', 'PROJ_CURL_CA_BUNDLE': '/usr/local/lib/python3.11/dist-packages/certifi/cacert.pem', 'PYTORCH_NVML_BASED_CUDA_CHECK': '1', 'KMP_DUPLICATE_LIB_OK': 'True', 'KMP_INIT_AT_FORK': 'FALSE', 'DATA_PATH': 'datasets', 'MAX_LABEL_MB': '300', 'GEOM_OVERLAP_THRESHOLD': '0.75', 'DUCKDB_DIR': 'datasets/db', 'PV_DB_TABLE': 'global_consolidated_pv', 'DIVISIONS_DB_TABLE': 'overture_division_areas', 'GPQ_ZSTD_COMPRESSION': '10', 'GPQ_ROW_GROUP_SIZE': '10000', 'ST_METADATA_PATH': '/Users/alejandovega/Documents/grad_studies/UPRRP_CCOM_MS/tesis/PV_segmentation_and_energy_forecasting/CCOM_MS_Spring_2025_EO_PV_research', 'MAXAR_PV_CHIPS_PATH': 'datasets/raw/images/maxar_hd_south_germany_PV_OD/', 'MAXAR_PV_YOLO_LABELS': 'datasets/raw/labels/maxar_hd_south_germany_PV_OD/', 'MODEL_CHECKPOINTS_DIR': 'datasets/model_artifacts/checkpoints', 'WANDB_API_KEY': '500ad64f202b2c3446a8f4c2e2fd628a2f80f564', 'WANDB_PROJECT': 'CV-PV segmentation baselines', 'GEE_API_KEY': 'AIzaSyAhhC0CEVRdobkRqUyCfWgxBARm_Cv6ZHY', 'MAXAR_USERNAME': 'alejandro.vega@maxar.com', 'MAXAR_PASSWORD': 'Displace1-Debatable-Taco', 'MAXAR_API_KEY': '', 'CUDA_MODULE_LOADING': 'LAZY'}))\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "print(os.environ.items())\n",
    "\n",
    "gdrive_prefix = '/content/drive/MyDrive/CCOM_MS_Training_Datasets_Logs'\n",
    "dataset_paths = {\n",
    "    'Maxar Germany VHR MSI': {\n",
    "        'chips': ('gdrive', 'datasets/raw/images/maxar_hd_south_germany_PV_OD.zip'),\n",
    "        'labels': ('gdrive', 'datasets/raw/labels/maxar_hd_south_germany_PV_OD.zip')\n",
    "    },\n",
    "    'France crowdsourced aerials, rooftop': {\n",
    "        'chips': ('zenodo', 'tmp'),\n",
    "        'labels': ('gdrive', 'datasets/raw/labels/fra_west_eur_pv_installations_2023')\n",
    "    },\n",
    "    'USA Cali, USGS ortho aerials': {\n",
    "        'chips': ('figshare', 'datasets/raw/images/usa_cali_usgs_pv_2016'),\n",
    "        'labels': ('gdrive', 'datasets/raw/labels/usa_cali_usgs_pv_2016')\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "te1gBWx4SUqh",
    "outputId": "80f50766-52d2-4147-e803-d4a1eb080c4d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'NVIDIA L4'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381,
     "referenced_widgets": [
      "e625d0edca0b4d39870fe0db08aa80ac",
      "2f205a6f99d44e99ab3b0bb463a392c6",
      "10b59470983649ccaf3b73103690de25",
      "c985dbc603124f2890d54263a13820d3",
      "a15881ff72e24f528336eecb32d02329",
      "eeeaf88a3091474991df7b4b0cd10eeb",
      "6f7651f2884e4c91b87f94ee98dc18f7",
      "8c558a7dedd4400db48d56e53dc51563",
      "aa230c2e5d3c47f98a25828752eef8dd",
      "372c299f611642829acc7af54010bd68",
      "d181151dfb0a4d4ca0f6462d77f0d15d",
      "d9486b799f9946bb9db438303052643c"
     ]
    },
    "id": "aR3zbNpJes24",
    "outputId": "93c6e543-7c72-48bb-a05f-842682c2455a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA L4 has default arch UnetPlusPlus and encoder resnet152\n",
      "Select Model Architecture:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e625d0edca0b4d39870fe0db08aa80ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(description='Select model arch:', index=1, layout=Layout(width='400px'), options=('Unet', 'UnetPl…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c985dbc603124f2890d54263a13820d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Select encoder (backend):', index=4, layout=Layout(width='400px'), options=('resnet18', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f7651f2884e4c91b87f94ee98dc18f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<p style=\"margin-bottom:10px\">\\n    Select which Maxar imagery resolution to use for model trainin…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "372c299f611642829acc7af54010bd68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Resolution:', layout=Layout(width='300px'), options=(('Native 31cm Source (416px)', 'nat…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %% --- 1.5 Dataset resolution and model+encoder selection ---\n",
    "\n",
    "# User lighter combos locally and larger for colab\n",
    "default_arch = \"Linknet\"\n",
    "default_encoder = \"resnet18\"\n",
    "# use larger models and encoder based on available accelerator\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    # if using cuda gpu\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_name = torch.cuda.get_device_name(0)\n",
    "        if gpu_name == 'NVIDIA A100-SXM4-40GB':\n",
    "            default_arch = 'Segformer'\n",
    "            default_encoder = 'mit_b5' # 81M\n",
    "        elif gpu_name == 'NVIDIA L4':\n",
    "            default_arch = \"UnetPlusPlus\"\n",
    "            default_encoder = \"resnet152\" #\n",
    "        elif gpu_name == 'Tesla T4':\n",
    "            default_arch = 'Linknet'\n",
    "            default_encoder = 'mobilenet_v2'\n",
    "        print(f\"{gpu_name} has default arch {default_arch} and encoder {default_encoder}\")\n",
    "# Create radio buttons for selecting model architecture\n",
    "model_arch_radio = widgets.RadioButtons(\n",
    "    options=[\n",
    "        'Unet', 'UnetPlusPlus',\n",
    "        'FPN', 'Linknet', 'PSPNet',\n",
    "        'DeepLabV3Plus', 'MAnet', 'PAN',\n",
    "        'Segformer', 'DPT'\n",
    "    ],\n",
    "    value=default_arch,\n",
    "    description='Select model arch:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='400px')\n",
    ")\n",
    "\n",
    "# Create radio buttons for selecting encoder\n",
    "encoder_radio = widgets.Dropdown(\n",
    "    options=['resnet18', 'resnet34', 'resnet50', 'resnet101', 'resnet152',\n",
    "             'resnext50_32x4d', 'resnext101_32x8d', 'resnext101_64x4d',\n",
    "             'resnext101_32x8d',\n",
    "             'mobilenet_v2',\n",
    "             'inceptionresnetv2',\n",
    "             'efficientnet-b0', 'efficientnet-b2', 'efficientnet-b4', 'efficientnet-b6', 'efficientnet-b7',\n",
    "             'timm-tf_efficientnet_lite4',\n",
    "             'vgg16', 'vgg19', 'vgg16_bn',\n",
    "             'densenet121', 'densenet169', 'densenet201', 'densenet161',\n",
    "            # ViT encoders compatible with DPT\n",
    "             'tu-fastvit_sa36.apple_dist_in1k',\n",
    "             'tu-efficientformer_l3.snap_dist_in1k',\n",
    "            #  mit\n",
    "             'mit_b0', 'mit_b1', 'mit_b2', 'mit_b3', 'mit_b4', 'mit_b5',\n",
    "            # swin\n",
    "             'tu-swin_base_patch4_window7_224.ms_in22k_ft_in1k',\n",
    "             'tu-swin_large_patch4_window7_224.ms_in22k_ft_in1k',\n",
    "             'tu-swinv2_large_window12to16_192to256.ms_in22k_ft_in1k',\n",
    "            # mobile\n",
    "             'mobileone_s0', 'mobileone_s2', 'mobileone_s3', 'mobileone_s5',\n",
    "             'tu-vitamin_large2_224.datacomp1b_clip',\n",
    "             'tu-vit_medium_patch16_clip_224.tinyclip_yfcc15m',\n",
    "             'tu-xcit_large_24_p8_384.fb_dist_in1k',\n",
    "             'tu-twins_svt_base.in1k',\n",
    "             ],\n",
    "    value=default_encoder,\n",
    "    description='Select encoder (backend):',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='400px')\n",
    ")\n",
    "\n",
    "\n",
    "# Prioritizing lighter versions for initial testing\n",
    "# MODEL_CONFIGURATIONS = {\n",
    "#     \"UnetPlusPlus_ResNet18\": {\"arch\": \"UnetPlusPlus\", \"encoder\": \"resnet18\"},\n",
    "#     \"DeepLabV3Plus_MobileNetV2\": {\"arch\": \"DeepLabV3Plus\", \"encoder\": \"mobilenet_v2\"},\n",
    "#     \"FPN_EfficientNetB0\": {\"arch\": \"FPN\", \"encoder\": \"efficientnet-b0\"},\n",
    "#     \"MAnet_ResNet18\": {\"arch\": \"MAnet\", \"encoder\": \"resnet18\"},\n",
    "#     # DPT and Segformer are generally heavier, can be uncommented later\n",
    "#     # \"Segformer_MiTB0\": {\"arch\": \"Segformer\", \"encoder\": \"mit_b0\"},\n",
    "#     # \"DPT_Hybrid_Tiny\": {\"arch\": \"DPT\", \"encoder\": \"vit_tiny_patch16_224\"} # Check SMP for exact ViT encoder names for DPT\n",
    "# }\n",
    "\n",
    "# gemini cloud recommendations:\n",
    "# \"UnetPlusPlus_ResNet50\": {\"arch\": \"UnetPlusPlus\", \"encoder\": \"resnet50\"},\n",
    "#     \"DeepLabV3Plus_EfficientNetB5\": {\"arch\": \"DeepLabV3Plus\", \"encoder\": \"efficientnet-b5\"},\n",
    "#     \"FPN_ConvNeXt_Small\": {\"arch\": \"FPN\", \"encoder\": \"convnext_small\"},\n",
    "#     \"DPT_ViTBase\": {\"arch\": \"DPT\", \"encoder\": \"vit_base_patch16_224\"}, # Or larger ViT variant\n",
    "#     \"Segformer_MiTB3\": {\"arch\": \"Segformer\", \"encoder\": \"mit_b3\"}, # Or mit_b4, mit_b5\n",
    "\n",
    "MODEL_CONFIGURATIONS = {f\"{model_arch_radio.value}_{encoder_radio.value}\": {\"arch\": model_arch_radio.value, \"encoder\": encoder_radio.value}}\n",
    "# see options from timm library here: https://smp.readthedocs.io/en/latest/encoders.html\n",
    "# see DPT specific ViT options here: https://smp.readthedocs.io/en/latest/encoders_dpt.html\n",
    "ENCODER_WEIGHTS = 'imagenet' # other option is simply imagenet subset 'imagenet21k'\n",
    "\n",
    "# Optional: update selections interactively\n",
    "def on_arch_change(change):\n",
    "    print(f\"Selected Model Architecture: {change['new']}\")\n",
    "    global MODEL_CONFIGURATIONS\n",
    "    MODEL_CONFIGURATIONS = {f\"{change['new']}_{encoder_radio.value}\": {\"arch\": change['new'], \"encoder\": encoder_radio.value}}\n",
    "    print(f\"Using model configurations: {MODEL_CONFIGURATIONS}\")\n",
    "\n",
    "def on_encoder_change(change):\n",
    "    print(f\"Selected Encoder: {change['new']}\")\n",
    "    global MODEL_CONFIGURATIONS\n",
    "    MODEL_CONFIGURATIONS = {f\"{model_arch_radio.value}_{change['new']}\": {\"arch\": model_arch_radio.value, \"encoder\": change['new']}}\n",
    "    print(f\"Using model configurations: {MODEL_CONFIGURATIONS}\")\n",
    "\n",
    "model_arch_radio.observe(on_arch_change, names='value')\n",
    "encoder_radio.observe(on_encoder_change, names='value')\n",
    "\n",
    "# Display the radio button widgets\n",
    "print(\"Select Model Architecture:\")\n",
    "display(model_arch_radio)\n",
    "display(encoder_radio)\n",
    "\n",
    "# choose native or transformed \"hd\" chips and labels\n",
    "MASK_DIR = Path(os.getenv('MAXAR_PV_YOLO_LABELS', 'datasets/raw/labels/maxar_hd_south_germany_PV_OD'))\n",
    "# MASK_DIR = MASK_DIR / 'labels_hd' if SELECTED_RESOLUTION_TYPE == 'hd' else MASK_DIR / 'labels_native'\n",
    "IMAGE_PATCH_DIR = Path(os.getenv('MAXAR_PV_CHIPS_PATH', 'datasets/raw/images/maxar_hd_south_germany_PV_OD'))\n",
    "# IMAGE_PATCH_DIR = IMAGE_PATCH_DIR / 'image_chips_hd' if SELECTED_RESOLUTION_TYPE == 'hd' else IMAGE_PATCH_DIR / 'image_chips_native'\n",
    "CHECKPOINT_DIR = Path(os.getenv('MODEL_CHECKPOINTS_DIR', 'datasets/model_artifacts/checkpoints'))\n",
    "\n",
    "# print(f\"Using {IMAGE_PATCH_DIR} for image patches with {len(os.listdir(IMAGE_PATCH_DIR))} images.\")\n",
    "# print(f\"Using {MASK_DIR} for labels with {len(os.listdir(MASK_DIR))} masks.\")\n",
    "\n",
    "SELECTED_RESOLUTION_TYPE = 'native' # Default: 'native', 'hd', or 'hd_512' (custom for downscaled HD)\n",
    "PATCH_SIZE_PIXELS = 832 if SELECTED_RESOLUTION_TYPE == 'hd' else 416 # Adjust based on resolution\n",
    "# Find default value for dropdown\n",
    "default_dropdown_value = 'native_416' # Fallback\n",
    "# source folder and the target model input size.\n",
    "resolution_options = [\n",
    "    ('Native 31cm Source (416px)', 'native_416'),\n",
    "    ('HD 15.5cm Source (832px)', 'hd_832'),\n",
    "    ('HD Source downscaled (512px)', 'hd_512'),\n",
    "    ('HD Source downscaled (416px)', 'hd_416'),\n",
    "    ('Native Source (256px input - downscaled)', 'native_256')\n",
    "]\n",
    "for name, val_opt in resolution_options:\n",
    "    res_type_opt = val_opt.split('_')[0]\n",
    "    patch_size_opt = int(val_opt.split('_')[1])\n",
    "    if res_type_opt == SELECTED_RESOLUTION_TYPE and patch_size_opt == PATCH_SIZE_PIXELS:\n",
    "        default_dropdown_value = val_opt\n",
    "        break\n",
    "\n",
    "# Create a dropdown widget for resolution selection\n",
    "resolution_dropdown = widgets.Dropdown(\n",
    "    options=resolution_options,\n",
    "    value=default_dropdown_value,\n",
    "    description='Resolution:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='300px')\n",
    ")\n",
    "\n",
    "# Create a label widget to explain the purpose\n",
    "description_html = widgets.HTML(\n",
    "    value=\"\"\"<p style=\"margin-bottom:10px\">\n",
    "    Select which Maxar imagery resolution to use for model training.\n",
    "    HD refers to Maxar's proprietary upscaling algorithm that simulates 15.5cm ground sample distance.\n",
    "    </p>\"\"\"\n",
    ")\n",
    "\n",
    "def on_resolution_change(change):\n",
    "    global SELECTED_RESOLUTION_TYPE, PATCH_SIZE_PIXELS, MASK_DIR, IMAGE_PATCH_DIR\n",
    "    value = change['new'] # e.g., 'hd_512'\n",
    "\n",
    "    SELECTED_RESOLUTION_TYPE = value.split('_')[0] # 'hd' or 'native' (source data type)\n",
    "    PATCH_SIZE_PIXELS = int(value.split('_')[1])   # Model input size\n",
    "    print(f\"Source Data Type: {SELECTED_RESOLUTION_TYPE.upper()} | Model Input Size: {PATCH_SIZE_PIXELS}x{PATCH_SIZE_PIXELS}px\")\n",
    "    print(f\"Expecting mask labels in: {MASK_DIR} | Expecting image chips in: {IMAGE_PATCH_DIR}\")\n",
    "\n",
    "resolution_dropdown.observe(on_resolution_change, names='value')\n",
    "\n",
    "# Display the widgets\n",
    "display(description_html)\n",
    "display(resolution_dropdown)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 199,
     "referenced_widgets": [
      "8c26d387ba7f4b0d8e478afd27dc7c2a",
      "94089f70a98d408fa220708b908cc462",
      "95ddfb019416446e90e1ed8e63a24b3d",
      "894adc77e2f544f9bb6ca8a6a52757f4",
      "dd700469a2dc408fb3330c99bf580510",
      "ba17dd52d2ca420d9c24de91ec0ac051",
      "91509d54ed9440d399c56d9784517194",
      "d361e57dbbcc48bcbfbc29b7de4a35f1",
      "d29666a9e0464d669936a4b92f540648",
      "cf573829b0f841a091dc97ddaa280561",
      "d999f1054a294585b4d8d383dde29992",
      "c0581a3c140148429d0c9a3206bd97f5",
      "2464335edf1c4b7ab449d35ca96fdbbe",
      "170c7a0f691948c29f8767d9e9b90461",
      "2ab47fd22c104d0f84a7d2b12a495caf",
      "4074769a62a7456aae009efd4d923db1",
      "4c903bb790364f4dbe34e32c4a1c27b0",
      "9a8f3aee11ee40999b3d27cafce57643"
     ]
    },
    "id": "DnFfTrnRanBh",
    "outputId": "b4bc7238-f603-4005-e53a-3b3299c058f4"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c26d387ba7f4b0d8e478afd27dc7c2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=16, description='Batch Size:', max=128, min=16, step=8, style=SliderStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "894adc77e2f544f9bb6ca8a6a52757f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=30, description='Epochs:', min=30, step=5, style=SliderStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91509d54ed9440d399c56d9784517194",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=0.2, description='Val Split Ratio:', max=0.5, min=0.1, step=0.05, style=SliderStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf573829b0f841a091dc97ddaa280561",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=2, description='Accum N Grad. Batches:', max=8, min=2, step=2, style=SliderStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2464335edf1c4b7ab449d35ca96fdbbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Torch Compile Mode:', index=2, options=('False', 'True', 'reduce-overhead', 'max-autotun…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4074769a62a7456aae009efd4d923db1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Train Dataset:', options=('Maxar Germany VHR MSI', 'France crowdsourced aerials, rooftop…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# add some hyperparam sliders\n",
    "LEARNING_RATE = 1e-4\n",
    "BATCH_SIZE = 8 # Adjust based on GPU/MPS memory\n",
    "NUM_EPOCHS = 25 # Start with very few epochs for initial weekend testing\n",
    "VAL_SPLIT_RATIO = 0.2\n",
    "USE_TORCH_COMPILE = 'reduce-overhead' # Options: False, True (uses default), 'reduce-overhead', 'max-autotune'\n",
    "# will run N forward/backward passes with your small batch size and accumulate the gradients before performing an optimizer step\n",
    "# It simulates a batch size of N * actual_batch_size for the weight update, but **only actual_batch_size for VRAM usage** per step.\n",
    "ACCUM_GRAD_N_BATCHES = 2\n",
    "\n",
    "# experiments logging and monitoring\n",
    "WANDB_PROJECT_NAME = os.getenv(\"WANDB_PROJECT\", \"CV-PV segmentation baselines\")\n",
    "DATASET = os.getenv(\"DATASET\", \"Maxar Germany VHR MSI\")\n",
    "# WANDB_ENTITY = os.getenv(\"WANDB_ENTITY\", \"your_wandb_entity\") # Set your wandb entity here\n",
    "\n",
    "# batch_size int slider with step 8\n",
    "batch_size_slider = widgets.IntSlider(\n",
    "    value=BATCH_SIZE,\n",
    "    min=16,\n",
    "    max=128,\n",
    "    step=8,\n",
    "    description='Batch Size:',\n",
    "    style={'description_width': 'initial'})\n",
    "\n",
    "epochs_slider = widgets.IntSlider(\n",
    "    value=NUM_EPOCHS,\n",
    "    min=30,\n",
    "    max=100,\n",
    "    step=5,\n",
    "    description='Epochs:',\n",
    "    style={'description_width': 'initial'})\n",
    "\n",
    "split_ratio = widgets.FloatSlider(\n",
    "    value=VAL_SPLIT_RATIO,\n",
    "    min=0.1,\n",
    "    max=0.5,\n",
    "    step=0.05,\n",
    "    description='Val Split Ratio:',\n",
    "    style={'description_width': 'initial'})\n",
    "\n",
    "grad_batches = widgets.IntSlider(\n",
    "    value=ACCUM_GRAD_N_BATCHES,\n",
    "    min=2,\n",
    "    max=8,\n",
    "    step=2,\n",
    "    description='Accum N Grad. Batches:',\n",
    "    style={'description_width': 'initial'})\n",
    "\n",
    "compile_mode = widgets.Dropdown(\n",
    "    options=['False', 'True', 'reduce-overhead', 'max-autotune'],\n",
    "    value=USE_TORCH_COMPILE,\n",
    "    description='Torch Compile Mode:',\n",
    "    style={'description_width': 'initial'})\n",
    "\n",
    "train_dataset_name = widgets.Dropdown(\n",
    "    options=list(dataset_paths.keys()),\n",
    "    value=DATASET,\n",
    "    description='Train Dataset:',\n",
    "    style={'description_width': 'initial'})\n",
    "\n",
    "# observe functions\n",
    "def on_batch_size_change(change):\n",
    "    global BATCH_SIZE\n",
    "    BATCH_SIZE = change['new']\n",
    "\n",
    "def on_epochs_change(change):\n",
    "    global NUM_EPOCHS\n",
    "    NUM_EPOCHS = change['new']\n",
    "\n",
    "def on_split_ratio_change(change):\n",
    "    global VAL_SPLIT_RATIO\n",
    "    VAL_SPLIT_RATIO = change['new']\n",
    "\n",
    "def on_grad_batches_change(change):\n",
    "    global ACCUM_GRAD_N_BATCHES\n",
    "    ACCUM_GRAD_N_BATCHES = change['new']\n",
    "\n",
    "def on_compile_mode_change(change):\n",
    "    global USE_TORCH_COMPILE\n",
    "    USE_TORCH_COMPILE = change['new']\n",
    "\n",
    "def on_train_dataset_change(change):\n",
    "    global DATASET\n",
    "    DATASET = change['new']\n",
    "\n",
    "batch_size_slider.observe(on_batch_size_change, names='value')\n",
    "epochs_slider.observe(on_epochs_change, names='value')\n",
    "split_ratio.observe(on_split_ratio_change, names='value')\n",
    "grad_batches.observe(on_grad_batches_change, names='value')\n",
    "compile_mode.observe(on_compile_mode_change, names='value')\n",
    "train_dataset_name.observe(on_train_dataset_change, names='value')\n",
    "\n",
    "# Display the widgets\n",
    "display(batch_size_slider)\n",
    "display(epochs_slider)\n",
    "display(split_ratio)\n",
    "display(grad_batches)\n",
    "display(compile_mode)\n",
    "display(train_dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WmGcfGwAXofI",
    "outputId": "4461537c-81b5-4194-9489-ad58ea0bfcb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using native resolution for training.\n",
      "('gdrive', 'datasets/raw/images/maxar_hd_south_germany_PV_OD.zip')\n",
      "unpacking zip in dir: datasets/raw/labels/maxar_hd_south_germany_PV_OD.zip to datasets/raw/labels/maxar_hd_south_germany_PV_OD\n",
      "Searching for dirs in: datasets/raw/labels/maxar_hd_south_germany_PV_OD/maxar_hd_south_germany_PV_OD\n",
      "Moving from datasets/raw/labels/maxar_hd_south_germany_PV_OD/maxar_hd_south_germany_PV_OD/labels_native to datasets/raw/labels/maxar_hd_south_germany_PV_OD/labels_native\n",
      "Moving from datasets/raw/labels/maxar_hd_south_germany_PV_OD/maxar_hd_south_germany_PV_OD/labels_hd to datasets/raw/labels/maxar_hd_south_germany_PV_OD/labels_hd\n",
      "Unpacking zip in dir (datasets/raw/images/maxar_hd_south_germany_PV_OD.zip) to datasets/raw/images/maxar_hd_south_germany_PV_OD\n"
     ]
    }
   ],
   "source": [
    "# TODO: change to func from util later\n",
    "def dataset_fetch(dataset_name):\n",
    "\n",
    "    print(dataset_paths[dataset_name]['chips'])\n",
    "    chips_src, chips_uri = dataset_paths[dataset_name]['chips']\n",
    "    labels_src, labels_uri = dataset_paths[dataset_name]['labels']\n",
    "\n",
    "    chips_local, labels_local = None, None\n",
    "    if labels_src == 'gdrive':\n",
    "        if labels_uri.endswith('.zip'):\n",
    "            # use same path without zip extension\n",
    "            labels_local = labels_uri[:-4]\n",
    "            # drop last dir in uri as it'll be inherited from zip file\n",
    "            # labels_uri = '/'.join(labels_uri.split('/')[:-1])\n",
    "            print(f\"unpacking zip in dir: {labels_uri} to {labels_local}\")\n",
    "            shutil.unpack_archive(os.path.join(gdrive_prefix, labels_uri), labels_local)\n",
    "            nested_ds_dir = os.path.join(labels_local, labels_local.split('/')[-1])\n",
    "            print(f\"Searching for dirs in: {nested_ds_dir}\")\n",
    "            # move the images and labels dirs\n",
    "            for d in glob(f\"{nested_ds_dir}/labels_*\"):\n",
    "                print(f\"Moving from {d} to {labels_local}/{d.split('/')[-1]}\")\n",
    "                shutil.move(d, f\"{labels_local}/{d.split('/')[-1]}\")\n",
    "        elif not os.path.exists(MASK_DIR):\n",
    "            shutil.copytree(os.path.join(gdrive_prefix, labels_uri), labels_uri, dirs_exist_ok=True)\n",
    "    if chips_src == 'gdrive':\n",
    "        if chips_uri.endswith('.zip') and not os.path.exists(chips_uri[:-4]):\n",
    "            # use same path without zip extension\n",
    "            chips_local = chips_uri[:-4]\n",
    "            print(f\"Unpacking zip in dir ({chips_uri}) to {chips_local}\")\n",
    "            shutil.unpack_archive(os.path.join(gdrive_prefix, chips_uri), chips_local)\n",
    "            nested_ds_dir = os.path.join(chips_local, chips_local.split('/')[-1])\n",
    "            print(f\"Searching for dirs in: {nested_ds_dir}\")\n",
    "            for d in glob(f\"{nested_ds_dir}/image_*\"):\n",
    "                print(f\"Moving from {d} to {chips_local}/{d.split('/')[-1]}\")\n",
    "                shutil.move(d, f\"{chips_local}/{d.split('/')[-1]}\")\n",
    "        elif not os.path.exists(IMAGE_PATCH_DIR):\n",
    "            shutil.copytree(os.path.join(gdrive_prefix, chips_uri), chips_uri, dirs_exist_ok=True)\n",
    "\n",
    "    return chips_local, labels_local\n",
    "\n",
    "# %% --- 2. Configuration and Setup ---\n",
    "\n",
    "# --- Data Parameters ---\n",
    "# These paths point to pre-prepared image patches and their corresponding masks\n",
    "# For initial testing with datasets like Maxar's (Clark et al.) or Jiang et al.,\n",
    "\n",
    "print(f\"Using {SELECTED_RESOLUTION_TYPE} resolution for training.\")\n",
    "\n",
    "chips_dir, labels_dir = dataset_fetch(DATASET)\n",
    "# os.makedirs(os.path.join(chips_dir), f'image_chips_{SELECTED_RESOLUTION_TYPE}')\n",
    "# os.makedirs(os.path.join(labels_dir), f'labels_{SELECTED_RESOLUTION_TYPE}')\n",
    "# chips_dir = '/content/datasets/raw/images/maxar_hd_south_germany_PV_OD/image_chips_native'\n",
    "# chips_dir = 'datasets/raw/images/maxar_hd_south_germany_PV_OD/'\n",
    "# labels_dir = '/content/datasets/raw/labels/maxar_hd_south_germany_PV_OD/image_chips_native'\n",
    "# labels_dir = 'datasets/raw/labels/maxar_hd_south_germany_PV_OD/'\n",
    "# choose native or transformed \"hd\" chips and labels\n",
    "MASK_DIR = Path(labels_dir)\n",
    "MASK_DIR = MASK_DIR / 'labels_hd' if SELECTED_RESOLUTION_TYPE == 'hd' else MASK_DIR / 'labels_native'\n",
    "IMAGE_PATCH_DIR = Path(chips_dir)\n",
    "IMAGE_PATCH_DIR = IMAGE_PATCH_DIR / 'image_chips_hd' if SELECTED_RESOLUTION_TYPE == 'hd' else IMAGE_PATCH_DIR / 'image_chips_native'\n",
    "CHECKPOINT_DIR = Path(os.getenv('MODEL_CHECKPOINTS_DIR', 'datasets/model_artifacts/checkpoints'))\n",
    "\n",
    "print(f\"Using {IMAGE_PATCH_DIR} for image patches with {len(os.listdir(IMAGE_PATCH_DIR))} images.\")\n",
    "print(f\"Using {MASK_DIR} for labels with {len(os.listdir(MASK_DIR))} masks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jRv5dgY8hwen"
   },
   "outputs": [],
   "source": [
    "MASK_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J-yHajFLIslD"
   },
   "outputs": [],
   "source": [
    "MODEL_CONFIGURATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JdnYIbBQasL-"
   },
   "outputs": [],
   "source": [
    "\n",
    "NUM_WORKERS = os.cpu_count() // 2 if os.cpu_count() else 0 # For DataLoaders\n",
    "RES_DOWNSCALE = 0.615385 # 512x512 for hd; 256x256 for native\n",
    "\n",
    "print(f\"Using chip size of {PATCH_SIZE_PIXELS} pixels and loading with {NUM_WORKERS} workers.\")\n",
    "\n",
    "# --- Model & Training Hyperparameters (Common) ---\n",
    "IN_CHANNELS = 3 # RGB. Change to 4 if using RGB+NIR\n",
    "NUM_CLASSES = 1 # Binary segmentation (PV vs background)\n",
    "TARGET_ACTIVATION = 'sigmoid' # Output activation for the model (sigmoid is recommended for binary segmentation, softmax for multi-class)\n",
    "\n",
    "# --- MPS/GPU Configuration ---\n",
    "# Check for MPS availability (Apple Silicon GPU)\n",
    "# gpu_name = None # update to acc_name\n",
    "if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    ACCELERATOR = \"mps\"\n",
    "    DEVICES = 1\n",
    "    PRECISION_TRAINER = \"32\" # MPS generally prefers 32-bit for stability, though 16-mixed might work for some ops\n",
    "    print(\"MPS (Apple Silicon GPU) backend is available and will be used.\")\n",
    "elif torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    ACCELERATOR = \"gpu\"\n",
    "    DEVICES = 'auto' # Or specify number of GPUs [0, 1] or \"auto\"\n",
    "    if gpu_name == 'NVIDIA A100-SXM4-40GB':\n",
    "        # PRECISION_TRAINER = '16-mixed' # other options are '16-true', '32', '64' (and their -true variantes): https://lightning.ai/docs/pytorch/stable/common/precision_basic.html\n",
    "        # PRECISION_TRAINER = '32' # test for stability\n",
    "        PRECISION_TRAINER = 'bf16-mixed'\n",
    "    elif gpu_name == 'NVIDIA L4': # ~22GB VRAM\n",
    "        # BATCH_SIZE = max(BATCH_SIZE, 32)\n",
    "        PRECISION_TRAINER = '16-mixed'\n",
    "    elif gpu_name == 'Tesla T4':\n",
    "        PRECISION_TRAINER = '32' # lacks tensor core and mixed precision\n",
    "        # if BATCH_SIZE > 16: #15GB VRAM\n",
    "        #     BATCH_SIZE = 16 # it seems to struggle with size 32 even for smaller archs\n",
    "        BATCH_SIZE = max(BATCH_SIZE, 16)\n",
    "    # print(f\"Using GPU: {torch.cuda.get_device_name(0)}\"\n",
    "else:\n",
    "    ACCELERATOR = \"cpu\"\n",
    "    DEVICES = 1\n",
    "    PRECISION_TRAINER = \"32\"\n",
    "    print(\"No GPU or MPS found. CPU will be used (training will be slow).\")\n",
    "\n",
    "print(f\"Using {ACCELERATOR} accelerator with {DEVICES} devices.\")\n",
    "print(f\"Using {PRECISION_TRAINER} precision for {gpu_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dvUadBC_es24"
   },
   "outputs": [],
   "source": [
    "# torch.backends.mps.allow_tf32 = True # Enable TensorFloat-32 for MPS (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Goy52jmYes24"
   },
   "outputs": [],
   "source": [
    "# %% --- 3 Data Prep: YOLO BBox to Mask Conversion Utilities ---\n",
    "# These functions are for preprocessing YOLO bounding box labels into binary segmentation masks.\n",
    "# It's recommended to run this as a one-time step to prepare your mask dataset\n",
    "OUTPUT_MASKS_PARENT_DIR = MASK_DIR.parent\n",
    "# 2. chip dimensions for native and HD types\n",
    "MAXAR_CHIP_DIMS = {\n",
    "    'native': (416, 416), # height, width\n",
    "    'hd': (832, 832)\n",
    "}\n",
    "\n",
    "input_chips_dir = IMAGE_PATCH_DIR\n",
    "input_labels_dir = MASK_DIR\n",
    "output_masks_dir = OUTPUT_MASKS_PARENT_DIR / f\"masks_{SELECTED_RESOLUTION_TYPE}\"\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_masks_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Processing {SELECTED_RESOLUTION_TYPE}\")\n",
    "# check if marks_dir exists and is not empty\n",
    "if os.path.exists(MASK_DIR) and os.listdir(MASK_DIR):\n",
    "    # check if the dir contains the same number of files as the image dir\n",
    "    if len(os.listdir(output_masks_dir)) == len(os.listdir(input_chips_dir)):\n",
    "        print(f\"Labels already exist in {MASK_DIR}.\")\n",
    "    else:\n",
    "        # pre-process yolo labels\n",
    "        print(f\"Converting YOLO labels to binary masks in {MASK_DIR}.\")\n",
    "        process_yolo_label_directory(input_labels_dir, input_chips_dir, output_masks_dir, MAXAR_CHIP_DIMS, SELECTED_RESOLUTION_TYPE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OTkqgFhCes24"
   },
   "outputs": [],
   "source": [
    "def display_image_and_mask(image_dir, mask_dir):\n",
    "    \"\"\"Loads a random image and mask, displays them, and shows the masked image.\"\"\"\n",
    "\n",
    "    image_files = os.listdir(image_dir)\n",
    "    mask_files = os.listdir(mask_dir)\n",
    "\n",
    "    # Ensure image and mask files correspond\n",
    "    if len(image_files) != len(mask_files):\n",
    "        print(\"Number of image files and mask files do not match.\")\n",
    "        return\n",
    "    common_names = [os.path.basename(os.path.splitext(img)[0]) for img in image_files]\n",
    "\n",
    "    random_name = random.choice(common_names)\n",
    "    image_path = os.path.join(image_dir, random_name + \".tif\")  # Adjust extension if needed\n",
    "    mask_path = os.path.join(mask_dir, random_name + \".png\")  # Adjust extension if needed\n",
    "\n",
    "    # Load image and mask\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    mask = Image.open(mask_path).convert(\"L\")  # Ensure mask is grayscale\n",
    "\n",
    "    # Resize mask if necessary\n",
    "    if image.size != mask.size:\n",
    "        mask = mask.resize(image.size, Image.NEAREST)\n",
    "\n",
    "    # Convert to NumPy arrays for easier manipulation\n",
    "    image_np = np.array(image)\n",
    "    mask_np = np.array(mask)\n",
    "\n",
    "    # Apply mask to image\n",
    "    masked_image = np.zeros_like(image_np)\n",
    "    masked_image[mask_np > 0] = image_np[mask_np > 0]  # Keep pixels where mask is not 0\n",
    "    # crop to buffer around mask bbox\n",
    "\n",
    "\n",
    "    # Display\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
    "\n",
    "    axes[0, 0].imshow(image_np)\n",
    "    axes[0, 0].set_title(\"Image\")\n",
    "    axes[0, 0].axis(\"off\")\n",
    "\n",
    "    axes[0, 1].imshow(mask_np, cmap=\"gray\")\n",
    "    axes[0, 1].set_title(\"Mask\")\n",
    "    axes[0, 1].axis(\"off\")\n",
    "\n",
    "    axes[1, 0].imshow(masked_image)\n",
    "    axes[1, 0].set_title(\"Masked Image\")\n",
    "    axes[1, 0].axis(\"off\")\n",
    "\n",
    "    # Remove the empty subplot (bottom right)\n",
    "    fig.delaxes(axes[1, 1])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3JkedARSes24"
   },
   "outputs": [],
   "source": [
    "display_image_and_mask(input_chips_dir, output_masks_dir)\n",
    "\n",
    "# Explore effect of larger/better encoders vs increasing spatial resolution/chip size (higher memory cost!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XgEn6XQJes24"
   },
   "source": [
    "## Deep Learning with PyTorch Lightning\n",
    "\n",
    "PyTorch Lightning is a lightweight PyTorch wrapper that significantly simplifies the process of training deep learning models. It provides a structured framework that abstracts away much of the boilerplate code typically associated with PyTorch training loops, allowing researchers and developers to focus more on the model architecture and data.\n",
    "\n",
    "**Key Advantages of PyTorch Lightning:**\n",
    "\n",
    "*   **Reduced Boilerplate:** Lightning handles the engineering aspects of training, such as the training loop, validation loop, and test loop. This means you write less code for common tasks.\n",
    "*   **Organized Code:** It promotes a clean and organized code structure through its core components: `LightningModule` and `LightningDataModule`.\n",
    "    *   The `LightningModule` encapsulates all model-related code (architecture, optimizers, training steps, validation steps, etc.).\n",
    "    *   The `LightningDataModule` handles all data-related operations (data loading, transformations, splitting, batching).\n",
    "*   **Simplified Training & Iteration:** With the boilerplate handled, iterating on different model architectures or hyperparameters becomes much faster and more straightforward.\n",
    "*   **Callbacks:** It has a rich ecosystem of callbacks (for checkpointing, early stopping, learning rate monitoring, etc.) that integrate easily into the training process and make managing experiments easier.\n",
    "    * For example, the `ModelCheckpoint` callback can save the best model based on validation metrics, and the `EarlyStopping` callback can halt training if the model stops improving.\n",
    "*   **Hardware Agnostic:** Lightning makes it easy to train models on CPUs, GPUs (single or multiple), and TPUs with minimal code changes.\n",
    "    - You can specify the `accelerator` (e.g., *\"gpu\"*, **\"mps\"**, *\"tpu\"*, \"cpu\") and `devices` (e.g., number of GPUs) directly as arguments for the `Trainer`\n",
    "*   **Scalability:** It seamlessly supports distributed training (multi-GPU, multi-node) and mixed-precision training (`precision='16-mixed'`)\n",
    "    - crucial for training large models like *Geospatial Foundation Models* on large datasets\n",
    "    <!-- - Lightning [Fabric](https://pytorch-lightning.readthedocs.io/en/stable/extensions/fabric.html) is a new feature that allows for even more flexibility in scaling and distributed training -->\n",
    "*   **Logging & Monitoring:** Lightning integrates with various logging frameworks (like TensorBoard, **Weights & Biases**, etc.) to track metrics and visualize training progress.\n",
    "*   **Reproducibility:** By organizing code and managing training details, Lightning helps in creating more reproducible experiments.\n",
    "\n",
    "As highlighted in the [PyTorch Lightning tutorial by DataCamp](https://www.datacamp.com/tutorial/pytorch-lightning-tutorial), its broad yet useful abstractions allow for quick training and iteration on multiple model architectures and facilitate scaling to multi-GPU or cloud environments. This notebook will leverage these features to efficiently train our PV segmentation models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BD3ASaCdzzv3"
   },
   "outputs": [],
   "source": [
    "img = widgets.Image(value=open(\"report/assets/figures/xkcd_python.png\", \"rb\").read(), format='png', width='75%', height='auto')\n",
    "# add html caption\n",
    "fig_caption = widgets.HTML('<p> What modern Python DL workflows can feel like when things go right. Illustration from xkcd. </p>')\n",
    "figure = widgets.VBox([img, fig_caption])\n",
    "# see Box layout manipulation docs: https://ipywidgets.readthedocs.io/en/stable/examples/Widget%20Layout.html#the-flexbox-layout\n",
    "figure.layout.display = 'flex'\n",
    "figure.layout.flex_flow = 'column'\n",
    "figure.layout.align_items = 'center'\n",
    "figure.layout.width = '100%'\n",
    "display(figure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uHCS-K8kes24"
   },
   "source": [
    "## The Composability of the PyTorch Ecosystem for Data Handling\n",
    "\n",
    "The PyTorch ecosystem offers a highly composable and flexible set of tools for data loading and preprocessing, which are essential for any deep learning pipeline. Key components include `Dataset`, `DataLoader`, and various transformation libraries.\n",
    "\n",
    "**Core Components:**\n",
    "\n",
    "*   **`torch.utils.data.Dataset`:** This is an abstract class representing a dataset. To create a custom dataset, you typically inherit from `Dataset` and override two methods:\n",
    "    *   `__len__(self)`: Should return the size of the dataset.\n",
    "    *   `__getitem__(self, idx)`: Should return the sample (e.g., an image and its corresponding mask) at the given index `idx`. This is where you load and preprocess individual data points.\n",
    "\n",
    "*   **`torch.utils.data.DataLoader`:** This utility wraps an iterable around the `Dataset` to enable easy access to the samples. It handles many crucial aspects of data loading efficiently:\n",
    "    *   **Batching:** Groups multiple samples into batches.\n",
    "    *   **Shuffling:** Randomly shuffles the data at every epoch to prevent model bias.\n",
    "    *   **Parallel Loading:** Uses multiple worker processes (`num_workers`) to load data in parallel, which can significantly speed up training by preventing the GPU from waiting for data.\n",
    "    *   **Memory Pinning (`pin_memory`):** When using GPUs, setting `pin_memory=True` can speed up data transfer from CPU to GPU memory.\n",
    "\n",
    "*   **Transformation Libraries (e.g., `torchvision.transforms`, `albumentations`):**\n",
    "    *   **`torchvision.transforms`:** Provides common image transformations (resizing, cropping, normalization, conversion to tensor, etc.). These are often composed together using `transforms.Compose`.\n",
    "    *   **`albumentations`:** A powerful library specifically designed for image augmentation. It offers a wide variety of augmentations (flips, rotations, color adjustments, noise, blurs, etc.) and is highly optimized for performance. It integrates well with PyTorch and other frameworks.\n",
    "\n",
    "**Composability in Action:**\n",
    "\n",
    "These components are designed to work together seamlessly. A typical workflow involves:\n",
    "1.  Creating a custom `Dataset` class to load and apply initial transformations to individual image-mask pairs.\n",
    "2.  Wrapping this `Dataset` instance with a `DataLoader` to manage batching, shuffling, and parallel loading.\n",
    "3.  The `DataLoader` then provides an iterator that yields batches of data (images and masks) ready to be fed into the model during training or evaluation.\n",
    "\n",
    "This modular approach, as generally seen in the PyTorch world and highlighted in guides like the [PyTorch Segmentation Models practical guide](https://medium.com/@heyamit10/pytorch-segmentation-models-a-practical-guide-5bf973a32e30), makes the data pipeline flexible, maintainable, and efficient. In this notebook, we use `PVSegmentationDataset` (a custom `Dataset`) and `PVSegmentationDataModule` (which internally uses `DataLoader` and `transforms`) to manage our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dTuJIq9VoI9Z"
   },
   "outputs": [],
   "source": [
    "# Utility file containing Dataset and DataModule classes for PV segmentation\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import rasterio\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# Suppress rasterio warnings for non-georeferenced TIFFs\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"rasterio\")\n",
    "warnings.filterwarnings(\"ignore\", category=rasterio.errors.NotGeoreferencedWarning)\n",
    "\n",
    "# prefixed with '_'to avoid overwriting utils import needed for multi-worker batch loading\n",
    "class _PVSegmentationDataset(Dataset):\n",
    "    def __init__(self, image_paths, mask_paths, transform=None, mask_transform=None,\n",
    "                 target_size=(832, 832), in_channels=3):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.transform = transform\n",
    "        self.mask_transform = mask_transform\n",
    "        self.target_size = target_size  # Expects a tuple (height, width)\n",
    "        self.in_channels = in_channels\n",
    "        if len(self.image_paths) != len(self.mask_paths):\n",
    "            raise ValueError(f\"Mismatch between number of images ({len(self.image_paths)}) and masks ({len(self.mask_paths)})\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, mask_path = self.image_paths[idx], self.mask_paths[idx]\n",
    "        try:\n",
    "            if str(img_path).lower().endswith(('.tif', '.tiff')):\n",
    "                with rasterio.open(img_path) as src:\n",
    "                    num_bands_to_read = min(self.in_channels, src.count)\n",
    "                    image_data = src.read(list(range(1, num_bands_to_read + 1)))\n",
    "                    if image_data.shape[0] < self.in_channels:\n",
    "                        padding = np.zeros((self.in_channels - image_data.shape[0], src.height, src.width), dtype=image_data.dtype)\n",
    "                        image_data = np.concatenate((image_data, padding), axis=0)\n",
    "                    image = np.moveaxis(image_data, 0, -1)  # HWC\n",
    "                    # Scale to 0-255 if not already, common for satellite imagery\n",
    "                    if image.max() > 255.0 or image.dtype != np.uint8:\n",
    "                        if image.max() > 1.0:  # Heuristic for scaling larger integer types\n",
    "                             image = (image / image.max() * 255).astype(np.uint8)\n",
    "                        else:  # Assume float 0-1\n",
    "                             image = (image * 255).astype(np.uint8)\n",
    "                    image_pil = Image.fromarray(image)\n",
    "            else:  # PNG, JPG\n",
    "                image_pil = Image.open(img_path)\n",
    "                # Ensure correct number of channels\n",
    "                if self.in_channels == 3 and image_pil.mode != 'RGB': image_pil = image_pil.convert('RGB')\n",
    "                elif self.in_channels == 1 and image_pil.mode != 'L': image_pil = image_pil.convert('L')\n",
    "                elif self.in_channels == 4 and image_pil.mode != 'RGBA': image_pil = image_pil.convert('RGBA')\n",
    "\n",
    "            if image_pil.size != (self.target_size[1], self.target_size[0]):  # PIL size is (width, height)\n",
    "                image_pil = image_pil.resize((self.target_size[1], self.target_size[0]), Image.BILINEAR)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}. Using zeros.\")\n",
    "            zero_img_data = np.zeros((self.target_size[0], self.target_size[1], self.in_channels), dtype=np.uint8)\n",
    "            image_pil = Image.fromarray(zero_img_data)\n",
    "        try:\n",
    "            mask_pil = Image.open(mask_path).convert('L')\n",
    "            if mask_pil.size != (self.target_size[1], self.target_size[0]):\n",
    "                mask_pil = mask_pil.resize((self.target_size[1], self.target_size[0]), Image.NEAREST)\n",
    "            mask_np = (np.array(mask_pil) > 127).astype(np.float32)  # Threshold common for masks\n",
    "            mask_pil = Image.fromarray(mask_np, mode='F')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading mask {mask_path}: {e}. Using zeros.\")\n",
    "            mask_pil = Image.fromarray(np.zeros(self.target_size, dtype=np.float32), mode='F')\n",
    "\n",
    "        image_tensor = self.transform(image_pil) if self.transform else transforms.ToTensor()(image_pil)\n",
    "        mask_tensor = self.mask_transform(mask_pil) if self.mask_transform else transforms.ToTensor()(mask_pil)\n",
    "        return image_tensor, mask_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qYiPU4NGogaZ"
   },
   "outputs": [],
   "source": [
    "# prefixed with '_'to avoid overwriting utils import needed for multi-worker batch loading\n",
    "class _PVSegmentationDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, image_dir: str, mask_dir: str, batch_size: int, num_workers: int,\n",
    "                 val_split_ratio: float, seed: int = 42,\n",
    "                 patch_size_pixels_dm: int = 256, in_channels_dm: int = 3):\n",
    "        super().__init__()\n",
    "        self.image_dir = Path(image_dir)\n",
    "        self.mask_dir = Path(mask_dir)\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.val_split_ratio = val_split_ratio\n",
    "        self.seed = seed\n",
    "        self.patch_size_tuple = (patch_size_pixels_dm, patch_size_pixels_dm)\n",
    "        self.in_channels = in_channels_dm\n",
    "\n",
    "        self.imagenet_mean = [0.485, 0.456, 0.406][:self.in_channels]\n",
    "        self.imagenet_std = [0.229, 0.224, 0.225][:self.in_channels]\n",
    "        if self.in_channels == 1:\n",
    "            self.imagenet_mean, self.imagenet_std = [0.449], [0.226]\n",
    "\n",
    "        self.train_transform = transforms.Compose([\n",
    "            # NOTE: this is source of sometimes confusing img vs mask viz!\n",
    "            transforms.RandomHorizontalFlip(), transforms.RandomVerticalFlip(),\n",
    "            transforms.ToTensor(), transforms.Normalize(mean=self.imagenet_mean, std=self.imagenet_std)])\n",
    "        self.val_transform = transforms.Compose([\n",
    "            transforms.ToTensor(), transforms.Normalize(mean=self.imagenet_mean, std=self.imagenet_std)])\n",
    "        self.mask_transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    def prepare_data(self):\n",
    "        if not self.image_dir.exists():\n",
    "            raise FileNotFoundError(f\"Image dir not found: {self.image_dir}\")\n",
    "        if not self.mask_dir.exists():\n",
    "            raise FileNotFoundError(f\"Mask dir not found: {self.mask_dir}\")\n",
    "        print(f\"Data check: Image dir '{self.image_dir}', Mask dir '{self.mask_dir}' exist.\")\n",
    "\n",
    "    def setup(self, stage: str = None):\n",
    "        img_exts = ['*.tif', '*.png', '*.jpg', '*.jpeg']\n",
    "        all_img_paths = []\n",
    "        # print(f\"Searching for images in {self.image_dir}\")\n",
    "        for ext in img_exts:\n",
    "            all_img_paths.extend(list(self.image_dir.glob(ext)))\n",
    "        all_img_paths = sorted([p for p in all_img_paths if p.is_file()])\n",
    "\n",
    "        valid_img_p, all_mask_p, skipped = [], [], 0\n",
    "        for img_p in all_img_paths:\n",
    "            found_m = next((self.mask_dir / (img_p.stem + m_ext) for m_ext in ['.png', '.tif', '.jpg', '.jpeg']\n",
    "                          if (self.mask_dir / (img_p.stem + m_ext)).exists()), None)\n",
    "            if found_m:\n",
    "                valid_img_p.append(img_p)\n",
    "                all_mask_p.append(found_m)\n",
    "            else:\n",
    "                skipped += 1\n",
    "\n",
    "        if skipped > 0:\n",
    "            print(f\"Warning: Skipped {skipped} images due to missing masks during DataModule setup.\")\n",
    "        if not valid_img_p:\n",
    "            raise ValueError(\"No valid image/mask pairs found in DataModule setup.\")\n",
    "\n",
    "        ds_size = len(valid_img_p)\n",
    "        val_s = int(ds_size * self.val_split_ratio)\n",
    "        train_s = ds_size - val_s\n",
    "\n",
    "        if train_s <= 0:\n",
    "            raise ValueError(f\"Not enough data for training. Train size: {train_s}\")\n",
    "\n",
    "        indices = list(range(ds_size))\n",
    "        if val_s > 0:\n",
    "            train_idx, val_idx = random_split(indices, [train_s, val_s], generator=torch.Generator().manual_seed(self.seed))\n",
    "        else:\n",
    "            print(\"Warning: Validation split resulted in 0 validation samples. Using all data for training. No validation will be performed.\")\n",
    "            train_idx = indices\n",
    "            val_idx = []\n",
    "\n",
    "        train_img_list = [valid_img_p[i] for i in train_idx]\n",
    "        train_mask_list = [all_mask_p[i] for i in train_idx]\n",
    "        val_img_list = [valid_img_p[i] for i in val_idx]\n",
    "        val_mask_list = [all_mask_p[i] for i in val_idx]\n",
    "\n",
    "        self.train_dataset = PVSegmentationDataset(\n",
    "            train_img_list, train_mask_list,\n",
    "            self.train_transform, self.mask_transform,\n",
    "            self.patch_size_tuple, self.in_channels\n",
    "        )\n",
    "\n",
    "        self.val_dataset = PVSegmentationDataset(\n",
    "            val_img_list, val_mask_list,\n",
    "            self.val_transform, self.mask_transform,\n",
    "            self.patch_size_tuple, self.in_channels\n",
    "        ) if val_img_list else None\n",
    "\n",
    "        print(f\"Setup: Train: {len(self.train_dataset)}, Val: {len(self.val_dataset) if self.val_dataset else 'None'}\")\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,  # Always set pin_memory to True for faster data transfer\n",
    "            persistent_workers=(self.num_workers > 0)\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "            persistent_workers=(self.num_workers > 0)\n",
    "        ) if self.val_dataset else None\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "            persistent_workers=(self.num_workers > 0)\n",
    "        ) if self.val_dataset else None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OSY47IZ4es24"
   },
   "source": [
    "## `segmentation-models-pytorch` (SMP): A Rich Toolkit for Segmentation\n",
    "\n",
    "`segmentation-models-pytorch` (SMP) is a Python library built on PyTorch that provides a high-level API for image segmentation tasks. It simplifies the implementation of various state-of-the-art segmentation architectures and allows for easy integration of pre-trained encoders.\n",
    "\n",
    "**Key Features of SMP:**\n",
    "\n",
    "*   **Variety of Architectures:** SMP offers a collection of popular and effective segmentation architectures, including:\n",
    "    *   Unet\n",
    "    *   Unet++\n",
    "    *   MAnet\n",
    "    *   Linknet\n",
    "    *   FPN (Feature Pyramid Network)\n",
    "    *   PSPNet (Pyramid Scene Parsing Network)\n",
    "    *   DeepLabV3 / DeepLabV3+\n",
    "    *   PAN (Pyramid Attention Network)\n",
    "    *   DPT (Dense Prediction Transformer)\n",
    "    *   SegFormer\n",
    "*   **Pre-trained Encoders:** One of the most powerful features of SMP is its seamless integration with a vast number of pre-trained encoders (backbones). This is largely facilitated by its use of libraries like `timm` (PyTorch Image Models by Ross Wightman).\n",
    "    *   This allows you to use encoders like ResNets (resnet18, resnet34, resnet50, etc.), EfficientNets (efficientnet-b0 to b7), MobileNets, ViTs (Vision Transformers), and many others, often with weights pre-trained on ImageNet.\n",
    "    *   Using pre-trained encoders can significantly speed up convergence and improve performance, especially when working with limited datasets.\n",
    "*   **Ease of Use:** Creating a segmentation model is typically a one-liner: `smp.Unet(encoder_name='resnet34', encoder_weights='imagenet', in_channels=3, classes=1)`.\n",
    "*   **Flexibility:** You can easily switch between different architectures and encoders to experiment and find the best combination for your specific task.\n",
    "*   **Loss Functions and Metrics:** SMP also includes common loss functions (e.g., DiceLoss, JaccardLoss, FocalLoss) and metrics relevant to segmentation.\n",
    "\n",
    "As mentioned in the [PyTorch Segmentation Models practical guide](https://medium.com/@heyamit10/pytorch-segmentation-models-a-practical-guide-5bf973a32e30), SMP's strength lies in providing ready-to-use segmentation models with a wide choice of decoders and a huge variety of pre-trained encoders from `timm`. This composability allows for rapid prototyping and benchmarking of different approaches.\n",
    "\n",
    "In this notebook, we define a `PVSegmentationTask` (a `LightningModule`) that utilizes `smp.create_model` to dynamically build segmentation models based on the configurations specified, allowing us to easily test different architectures and encoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4VoH8Pmoes25"
   },
   "outputs": [],
   "source": [
    "# %% --- 4. PyTorch Lightning Module ---\n",
    "# PVSegmentationTask class definition (largely the same as previous version)\n",
    "class PVSegmentationTask(pl.LightningModule):\n",
    "    def __init__(self, model_arch: str, encoder_name: str, encoder_weights: str,\n",
    "                 in_channels: int, num_classes: int, activation: str,\n",
    "                 learning_rate: float = 1e-4, loss_weights: tuple = (0.5, 0.5)):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters() # Saves all __init__ args to self.hparams\n",
    "        # ========== segmentation-models-pytorch block ==========\n",
    "        self.model = smp.create_model(\n",
    "            arch=self.hparams.model_arch,\n",
    "            encoder_name=self.hparams.encoder_name,\n",
    "            encoder_weights=self.hparams.encoder_weights,\n",
    "            in_channels=self.hparams.in_channels,\n",
    "            classes=self.hparams.num_classes,\n",
    "            activation=self.hparams.activation\n",
    "        )\n",
    "        # ========== segmentation-models-pytorch block ==========\n",
    "        self.dice_loss = DiceLoss(mode='binary', from_logits=(self.hparams.activation is None))\n",
    "        self.bce_loss = SoftBCEWithLogitsLoss() # Always expects logits\n",
    "        metrics_args = {\"threshold\": 0.5}\n",
    "        metrics_collection = MetricCollection({\n",
    "            'iou': BinaryJaccardIndex(**metrics_args), 'f1': BinaryF1Score(**metrics_args), 'accuracy': BinaryAccuracy(**metrics_args),\n",
    "            'precision': BinaryPrecision(**metrics_args), 'recall': BinaryRecall(**metrics_args)})\n",
    "        self.train_metrics = metrics_collection.clone(prefix='train_')\n",
    "        self.val_metrics = metrics_collection.clone(prefix='val_')\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def _calculate_loss(self, y_pred, y_true):\n",
    "        y_true = y_true.float()\n",
    "        if self.hparams.activation is None: # Model outputs logits\n",
    "            bce = self.bce_loss(y_pred, y_true)\n",
    "            dice = self.dice_loss(y_pred, y_true) # DiceLoss will apply sigmoid internally if from_logits=True\n",
    "        else: # Model outputs probabilities (e.g., after sigmoid)\n",
    "            eps = 1e-7 # Epsilon to prevent log(0)\n",
    "            y_pred_c = torch.clamp(y_pred, eps, 1.0 - eps) # Clamp predictions\n",
    "            logits = torch.log(y_pred_c / (1.0 - y_pred_c + eps) + eps) # Inverse sigmoid, add eps for stability\n",
    "            bce = self.bce_loss(logits, y_true)\n",
    "            dice = self.dice_loss(y_pred, y_true) # DiceLoss expects probs here\n",
    "        return self.hparams.loss_weights[0] * bce + self.hparams.loss_weights[1] * dice\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y_true = batch\n",
    "        y_pred = self(x)\n",
    "        loss = self._calculate_loss(y_pred, y_true)\n",
    "        self.train_metrics.update(y_pred, y_true.int())\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        self.log_dict(self.train_metrics.compute(), logger=True, prog_bar=False)\n",
    "        self.train_metrics.reset()\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y_true = batch\n",
    "        y_pred = self(x)\n",
    "        loss = self._calculate_loss(y_pred, y_true)\n",
    "        self.val_metrics.update(y_pred, y_true.int())\n",
    "        # self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        # Log val_iou specifically for checkpointing and LR scheduler\n",
    "        # This will be computed and logged by on_validation_epoch_end\n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        if not self.trainer.sanity_checking: # Skip logging during sanity check\n",
    "            metrics = self.val_metrics.compute() # Log all val metrics\n",
    "            self.log_dict(metrics, logger=True, on_step=False, on_epoch=True, prog_bar=False)\n",
    "            # Log specific metrics for checkpointing and LR scheduler\n",
    "            # self.log('val_iou', metrics['iou'], on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "            self.val_metrics.reset()\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        # Monitor val_iou for ReduceLROnPlateau since mode is 'max'\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=5)\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": {\"scheduler\": scheduler, \"monitor\": \"val_iou\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aN1L8DvSes25",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sanity check for data loader (NOTE: sometimes seeing some cases of images seemingly flipped [on y-axis] )\n",
    "data_module = PVSegmentationDataModule(\n",
    "    image_dir=input_chips_dir, mask_dir=output_masks_dir,\n",
    "    batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, # Set to 1 while testing if image matches label during sanity check\n",
    "    val_split_ratio=VAL_SPLIT_RATIO, patch_size_pixels_dm=PATCH_SIZE_PIXELS, in_channels_dm=IN_CHANNELS\n",
    ")\n",
    "try:\n",
    "    data_module.prepare_data()\n",
    "    data_module.setup(stage='val')\n",
    "\n",
    "    # Visualize a sample batch from DataModule\n",
    "    train_dl = data_module.train_dataloader()\n",
    "    if train_dl and len(train_dl) > 0 :\n",
    "        print(\"Visualizing a sample batch from DataModule's train_dataloader...\")\n",
    "        images, masks = next(iter(train_dl))\n",
    "        def show_dm_batch(image_tensor, mask_tensor, num_samples=4, mean_val=None, std_val=None, in_channels=IN_CHANNELS):\n",
    "            images_np = image_tensor[:num_samples].cpu().numpy()\n",
    "            masks_np = mask_tensor[:num_samples].cpu().numpy()\n",
    "            fig, axes = plt.subplots(num_samples, 2, figsize=(8, num_samples * 4))\n",
    "            if num_samples == 1: axes = np.array([axes])\n",
    "            for i in range(num_samples):\n",
    "                img = images_np[i].transpose(1, 2, 0)\n",
    "                if mean_val and std_val:\n",
    "                    m, s = np.array(mean_val), np.array(std_val)\n",
    "                    img = s * img + m\n",
    "                img = np.clip(img, 0, 1)\n",
    "                if img.shape[-1] == 1: img = img.squeeze(-1) # For grayscale display\n",
    "\n",
    "                mask = masks_np[i].squeeze()\n",
    "                axes[i, 0].imshow(img, cmap='gray' if img.ndim==2 else None); axes[i, 0].set_title(\"Image\"); axes[i, 0].axis('off')\n",
    "                axes[i, 1].imshow(mask, cmap='gray'); axes[i, 1].set_title(\"Mask\"); axes[i, 1].axis('off')\n",
    "            plt.tight_layout(); plt.show()\n",
    "        show_dm_batch(images, masks, mean_val=data_module.imagenet_mean, std_val=data_module.imagenet_std)\n",
    "    else:\n",
    "        print(\"Train dataloader from DataModule is empty or not available.\")\n",
    "        data_module = None # Prevent training if data is not loaded\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error setting up DataModule or visualizing batch: {e}\")\n",
    "    data_module = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rrpktcXPes25"
   },
   "source": [
    "## Streamlined Training with PyTorch Lightning Trainer\n",
    "\n",
    "Once the `LightningDataModule` (for data handling) and the `LightningModule` (for the model, optimizers, and training/validation logic) are defined, PyTorch Lightning makes the actual training process remarkably straightforward using its `Trainer` class.\n",
    "\n",
    "**The `Trainer` Class ** (lightning docs excerpt):\n",
    "\n",
    "The Lightning ``Trainer`` does much more than just \"training\". *Under the hood*, it handles all loop details for you, some examples include:\n",
    "\n",
    "- Moving batches and data between correct devices (CPU <-> GPU/TPU).\n",
    "- Automatically enabling/disabling grads\n",
    "- Running the training, validation and test dataloaders (iterating over epochs and batches)\n",
    "- Calling the appropriate methods in your `LightningModule` (e.g., `training_step`, `validation_step`)\n",
    "- Calling the Callbacks at the appropriate times\n",
    "    - e.g. Performing optimizer steps and learning rate scheduler adjustments\n",
    "- Putting batches and computations on the correct devices\n",
    "\n",
    "Here's the pseudocode for what the trainer does under the hood (showing the train loop only)\n",
    "\n",
    "```python\n",
    "    # enable grads\n",
    "    torch.set_grad_enabled(True)\n",
    "\n",
    "    losses = []\n",
    "    for batch in train_dataloader:\n",
    "        # calls hooks like this one\n",
    "        on_train_batch_start()\n",
    "        # train step\n",
    "        loss = training_step(batch)\n",
    "        # clear gradients\n",
    "        optimizer.zero_grad()\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        losses.append(loss)\n",
    "```\n",
    "\n",
    "**Key `Trainer` Arguments Used in this Notebook:**\n",
    "\n",
    "*   `accelerator`: Specifies the hardware to use (e.g., \"mps\", \"gpu\", \"cpu\").\n",
    "*   `devices`: Specifies the number of accelerator devices or lists specific device IDs (e.g. gpu0, gpu3)\n",
    "*   `max_epochs`: maximum number of epochs to train\n",
    "*   `logger`: Accepts one or more logger instances (e.g., `TensorBoardLogger`, `CSVLogger`) to record metrics and hyperparameters.\n",
    "*   `callbacks`: A list of callback objects that can customize the training behavior at various points. Common callbacks include:\n",
    "    *   `ModelCheckpoint`: Saves the model periodically, often based on a monitored metric (e.g., best validation IoU).\n",
    "    *   `LearningRateMonitor`: Logs the learning rate at each epoch or step.\n",
    "    *   `EarlyStopping`: Stops training if a monitored metric stops improving for a certain number of epochs (patience).\n",
    "*   `precision`: Configures training precision (e.g., \"32\" for full precision, \"16-mixed\" for mixed-precision training).\n",
    "    *`compile`: Requires PyTorch 2.0+ and enables JIT compilation for graph optimization, which can speed up training and inference.\n",
    "\n",
    "**Initiating Training:**\n",
    "\n",
    "Training is typically started with a single line: `trainer.fit(model, datamodule=data_module)`.\n",
    "\n",
    "PyTorch Lightning's `Trainer` abstracts away the complexities of the training loop, allowing you to focus on the core components of your deep learning model and experiment more rapidly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kkxvjKR98St6"
   },
   "outputs": [],
   "source": [
    "if data_module.train_dataloader() and (data_module.val_dataloader() or VAL_SPLIT_RATIO == 0):\n",
    "    for config_name, params in MODEL_CONFIGURATIONS.items():\n",
    "        print(f\"\\n--- Training Model: {config_name} ---\")\n",
    "        lightning_model = PVSegmentationTask(\n",
    "            model_arch=params['arch'], encoder_name=params['encoder'], encoder_weights=ENCODER_WEIGHTS,\n",
    "            in_channels=IN_CHANNELS, num_classes=NUM_CLASSES, activation=TARGET_ACTIVATION,\n",
    "            learning_rate=LEARNING_RATE)\n",
    "        log_version = f\"{SELECTED_RESOLUTION_TYPE}_{time.strftime('%Y%m%d-%H%M%S')}\"\n",
    "        tb_logger = TensorBoardLogger(\"tb_logs\", name=config_name, version=log_version)\n",
    "        # wandb.ai logger - ensure you have wandb installed (pip install wandb)\n",
    "        # and WANDB_API_KEY environment variable set.\n",
    "        # WANDB_PROJECT_NAME should be set in your .env or above.\n",
    "        wandb_logger = WandbLogger(\n",
    "            name=f\"{config_name}-{log_version}\", # Run name in W&B\n",
    "            project=WANDB_PROJECT_NAME,\n",
    "            # entity=WANDB_ENTITY, # Optional: your W&B team/entity\n",
    "            log_model=\"all\" # Log model checkpoints to W&B: True, \"all\", or False\n",
    "        )\n",
    "        checkpoint_dir = CHECKPOINT_DIR / config_name / SELECTED_RESOLUTION_TYPE\n",
    "        checkpoint_cb = pl.callbacks.ModelCheckpoint(\n",
    "            dirpath=checkpoint_dir,\n",
    "            filename=\"{epoch}-{val_iou:.4f}\", monitor=\"val_iou\", mode=\"max\", save_top_k=1\n",
    "        )\n",
    "        lr_monitor_cb = pl.callbacks.LearningRateMonitor(logging_interval='epoch')\n",
    "        early_stop_cb = pl.callbacks.EarlyStopping(monitor=\"val_iou\", patience=10, mode=\"max\") # Increased patience\n",
    "\n",
    "        trainer_kwargs = {\"devices\": DEVICES, \"max_epochs\": NUM_EPOCHS,\n",
    "                          \"logger\": [tb_logger, wandb_logger], # Use WandbLogger\n",
    "                          \"callbacks\": [checkpoint_cb, lr_monitor_cb, early_stop_cb],\n",
    "                          \"precision\": PRECISION_TRAINER}\n",
    "        print(f\"Using these args for Trainer: {trainer_kwargs}\")\n",
    "\n",
    "        trainer = pl.Trainer(accumulate_grad_batches=ACCUM_GRAD_N_BATCHES, **trainer_kwargs)\n",
    "        # if USE_TORCH_COMPILE:\n",
    "        #     import torch._dynamo\n",
    "        #     torch._dynamo.config.suppress_errors = True\n",
    "        #     compile_mode = USE_TORCH_COMPILE if isinstance(USE_TORCH_COMPILE, str) else True\n",
    "        #     # lightning_model = torch.compile(lightning_model, fullgraph=True, dynamic=True, backend=\"cudagraphs\")\n",
    "        #     # look into eager mode\n",
    "        #     lightning_model = torch.compile(lightning_model, mode=compile_mode, backend=\"cudagraphs\")\n",
    "        #     print(f\"PyTorch 2.0 compilation enabled with mode: {compile_mode}\")\n",
    "        try:\n",
    "            print(f\"Starting training for {config_name} ({SELECTED_RESOLUTION_TYPE}) with {ACCELERATOR}...\")\n",
    "            # torch.set_float32_matmul_precision('medium')\n",
    "            trainer.fit(lightning_model, datamodule=data_module)\n",
    "            print(f\"Training finished for {config_name} ({SELECTED_RESOLUTION_TYPE}).\")\n",
    "            if checkpoint_cb.best_model_path: print(f\"Best model saved at: {checkpoint_cb.best_model_path}\")\n",
    "            wandb_logger.experiment.finish() # Finish W&B run\n",
    "            print(f\"Wandb run finished for {config_name} ({SELECTED_RESOLUTION_TYPE}).\")\n",
    "            # Optionally, save the model checkpoint to a specific directory\n",
    "        except Exception as e:\n",
    "            print(f\"Error training {config_name}: {e}\")\n",
    "            traceback.print_exc()\n",
    "            if wandb_logger.experiment: wandb_logger.experiment.finish(exit_code=1) # Finish run with error\n",
    "            continue\n",
    "else:\n",
    "    print(\"DataLoaders not available. Skipping training. Check DataModule setup and data paths.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EaPrV0BB8Usr"
   },
   "outputs": [],
   "source": [
    "# # setup local tensorboard logging\n",
    "%load_ext tensorboard\n",
    "# Remember to move the logs to gdrive so we save our experiments logs\n",
    "%tensorboard --logdir tb_logs --port 6006\n",
    "PRECISION_TRAINER = '16-mixed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D91nGOnres25"
   },
   "outputs": [],
   "source": [
    "torch._dynamo.list_backends()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DMOD-yQLes25"
   },
   "source": [
    "## Model Evaluation and Prediction Visualization\n",
    "\n",
    "After training, it's crucial to evaluate the model's performance on unseen data (typically the validation or a separate test set) and visualize its predictions to gain qualitative insights. This section demonstrates a basic approach to:\n",
    "\n",
    "1.  **Loading the Best Model:** PyTorch Lightning's `ModelCheckpoint` callback saves the best performing model based on a monitored metric. We load this checkpoint for evaluation.\n",
    "2.  **Making Predictions:** The loaded model is used to make predictions on a batch of data from the validation set.\n",
    "3.  **Visualizing Results:** The original images, ground truth masks, and the model's predicted masks are displayed side-by-side for comparison.\n",
    "\n",
    "This allows for a visual assessment of how well the model is segmenting the PV panels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "isCwhqCJes25"
   },
   "outputs": [],
   "source": [
    "# --- 6. Evaluation & Visualization (Example) ---\n",
    "\n",
    "if data_module and MODEL_CONFIGURATIONS:\n",
    "        print(\"test\")\n",
    "        first_config_name = list(MODEL_CONFIGURATIONS.keys())[0]\n",
    "        # Determine device for evaluation (mps, cuda, or cpu)\n",
    "        if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "            DEVICE_EVAL = torch.device(\"mps\")\n",
    "        elif torch.cuda.is_available():\n",
    "            DEVICE_EVAL = torch.device(\"cuda\")\n",
    "        else:\n",
    "            DEVICE_EVAL = torch.device(\"cpu\")\n",
    "        print(f\"Using device for evaluation: {DEVICE_EVAL}\")\n",
    "\n",
    "        eval_resolution_type = SELECTED_RESOLUTION_TYPE\n",
    "        if checkpoint_dir.exists():\n",
    "            ckpt_files = sorted(list(checkpoint_dir.glob(\"*.ckpt\")), key=os.path.getmtime, reverse=True)\n",
    "            if ckpt_files:\n",
    "                best_model_path_eval = ckpt_files[0] # Load the most recently saved best model\n",
    "                print(f\"\\n--- Evaluating and Visualizing: {first_config_name} from {best_model_path_eval} ---\")\n",
    "                try:\n",
    "                    eval_model = PVSegmentationTask.load_from_checkpoint(best_model_path_eval, map_location=DEVICE_EVAL)\n",
    "                    eval_model.to(DEVICE_EVAL) # Ensure model is on the correct device\n",
    "                    eval_model.eval()\n",
    "\n",
    "                    # current_eval_patch_size = 416 if eval_resolution_type == 'native' else 832\n",
    "                    current_eval_patch_size = PATCH_SIZE_PIXELS\n",
    "                    # Check if DataModule needs re-setup for the evaluation resolution/patch size\n",
    "                    dm_params_match = (hasattr(data_module, 'patch_size_tuple') and\n",
    "                                    data_module.patch_size_tuple == (current_eval_patch_size, current_eval_patch_size) and\n",
    "                                    Path(data_module.image_dir).name == f\"image_chips_{eval_resolution_type}\")\n",
    "\n",
    "                    if not dm_params_match:\n",
    "                        print(f\"Re-setting up DataModule for evaluation: resolution {eval_resolution_type}, patch size {current_eval_patch_size}\")\n",
    "                        # Determine correct IMAGE_PARENT_DIR and MASK_PARENT_DIR for eval if they changed\n",
    "                        eval_image_dir = str(IMAGE_PATCH_DIR)\n",
    "                        eval_mask_dir = str(MASK_DIR)\n",
    "\n",
    "                        data_module_eval = PVSegmentationDataModule(\n",
    "                            image_dir=str(eval_image_dir), mask_dir=str(eval_mask_dir),\n",
    "                            batch_size=BATCH_SIZE, num_workers=NUM_WORKERS,\n",
    "                            val_split_ratio=VAL_SPLIT_RATIO, # or 0.99 for nearly all data if just for viz\n",
    "                            patch_size_pixels_dm=current_eval_patch_size,\n",
    "                            in_channels_dm=IN_CHANNELS\n",
    "                        )\n",
    "                        data_module_eval.prepare_data()\n",
    "                        data_module_eval.setup(stage='test') # or 'test' if you have a test_dataloader\n",
    "                        val_loader_eval = data_module_eval.val_dataloader()\n",
    "                        dm_for_viz = data_module_eval\n",
    "                    else:\n",
    "                        val_loader_eval = data_module.val_dataloader()\n",
    "                        dm_for_viz = data_module\n",
    "\n",
    "                    if val_loader_eval and len(val_loader_eval) > 0:\n",
    "                        images_eval, masks_gt_eval = next(iter(val_loader_eval))\n",
    "                        images_eval = images_eval.to(DEVICE_EVAL) # Move images to device\n",
    "\n",
    "                        with torch.no_grad():\n",
    "                            masks_pred_eval = eval_model(images_eval).cpu() # Move predictions to CPU for numpy/PIL\n",
    "\n",
    "                        unnorm_transform = transforms.Compose([\n",
    "                            transforms.Normalize(mean=[0.]*IN_CHANNELS, std=[1/s for s in data_module.imagenet_std]),\n",
    "                            transforms.Normalize(mean=[-m for m in data_module.imagenet_mean], std=[1.]*IN_CHANNELS),\n",
    "                            transforms.ToPILImage()\n",
    "                        ])\n",
    "                        masks_gt_pil = [transforms.ToPILImage()(m.cpu()) for m in masks_gt_eval]\n",
    "                        if eval_model.hparams.activation is None: masks_pred_eval = torch.sigmoid(masks_pred_eval)\n",
    "                        masks_pred_binary_pil = [transforms.ToPILImage()((m > 0.5).float().cpu()) for m in masks_pred_eval]\n",
    "\n",
    "                        num_to_show = min(4, images_eval.size(0))\n",
    "                        fig, axes = plt.subplots(num_to_show, 3, figsize=(12, num_to_show * 4))\n",
    "                        if num_to_show == 1: axes = np.array([axes]) # Ensure axes is always 2D for consistent indexing\n",
    "                        for i in range(num_to_show):\n",
    "                            img_pil = unnorm_transform(images_eval[i].cpu()) # Move image to CPU before unnorm\n",
    "                            axes[i, 0].imshow(img_pil); axes[i, 0].set_title(\"Image\"); axes[i, 0].axis('off')\n",
    "                            axes[i, 1].imshow(masks_gt_pil[i], cmap='gray'); axes[i, 1].set_title(\"Ground Truth\"); axes[i, 1].axis('off')\n",
    "                            axes[i, 2].imshow(masks_pred_binary_pil[i], cmap='gray'); axes[i, 2].set_title(\"Prediction\"); axes[i, 2].axis('off')\n",
    "                        plt.tight_layout(); plt.show()\n",
    "                    else: print(\"Validation dataloader not available or empty for visualization.\")\n",
    "                except Exception as e: print(f\"Error during eval/viz of {first_config_name}: {e}\"); import traceback; traceback.print_exc()\n",
    "            else: print(f\"No checkpoint file found in {checkpoint_dir} for {first_config_name}.\")\n",
    "        else: print(f\"Checkpoint directory not found for {first_config_name}. Train first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "08znIcZ7S70_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5U5uIQUMes25"
   },
   "source": [
    "## Next Steps and Further Experimentation\n",
    "\n",
    "This notebook provides a foundational framework for PV panel segmentation. Here are some potential next steps and areas for further experimentation:\n",
    "\n",
    "*   **Full Training Runs:** Increase `NUM_EPOCHS` for more comprehensive training.\n",
    "*   **Hyperparameter Tuning:** Experiment with different learning rates, batch sizes, optimizer settings, and loss function weights.\n",
    "*   **Explore More Architectures/Encoders:** Leverage the flexibility of `segmentation-models-pytorch` to try other model configurations available in `MODEL_CONFIGURATIONS` or add new ones.\n",
    "*   **Data Augmentation:** Implement more sophisticated data augmentation techniques using `albumentations` within the `PVSegmentationDataset` or `PVSegmentationDataModule` to improve model generalization.\n",
    "*   **Advanced Loss Functions:** Explore other loss functions or combinations suitable for imbalanced segmentation tasks.\n",
    "*   **Test Set Evaluation:** Create a dedicated test set (if not already done) and evaluate the final model on it for an unbiased performance measure.\n",
    "*   **Post-processing:** Implement post-processing steps (e.g., removing small predicted regions, morphological operations) to potentially improve segmentation quality.\n",
    "*   **Larger Datasets:** Train on larger and more diverse datasets if available.\n",
    "*   **Cross-Validation:** Implement k-fold cross-validation for more robust performance estimation.\n",
    "\n",
    "Review the logs generated in the `tb_logs/` (TensorBoard) and `csv_logs/` directories, and inspect the saved model checkpoints in the `checkpoints/` directory to monitor training progress and select the best models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cFNYT_HUes25"
   },
   "outputs": [],
   "source": [
    "# %% --- 7. Next Steps ---\n",
    "print(\"\\nNotebook execution finished.\")\n",
    "print(\"Next steps: Review logs in 'tb_logs/' and 'csv_logs/'. Check 'checkpoints/' for saved models.\")\n",
    "print(\"Consider increasing NUM_EPOCHS for full training runs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FlyY9m3Tes25"
   },
   "outputs": [],
   "source": [
    "# %% --- 8. Export to CoreML and Quantization ---\n",
    "\n",
    "# Only run this cell after successful training and evaluation\n",
    "\n",
    "if 'eval_model' not in locals() or eval_model is None:\n",
    "    print(\"No model available for export. Please run the evaluation cell first.\")\n",
    "else:\n",
    "    print(\"Preparing model for export to CoreML...\")\n",
    "\n",
    "    try:\n",
    "        import coremltools as ct\n",
    "        from coremltools.models.neural_network import quantization_utils\n",
    "\n",
    "        # Move model to CPU for export\n",
    "        lightning_model.to('cpu')\n",
    "        lightning_model.eval()\n",
    "\n",
    "        # Define input shape\n",
    "        example_input = torch.rand(1, IN_CHANNELS, PATCH_SIZE_PIXELS, PATCH_SIZE_PIXELS)\n",
    "\n",
    "        # First, export to TorchScript format\n",
    "        scripted_model = torch.jit.trace(lightning_model, example_input)\n",
    "\n",
    "        # Convert to CoreML\n",
    "        print(\"Converting model to CoreML format...\")\n",
    "        mlmodel = ct.convert(\n",
    "            scripted_model,\n",
    "            inputs=[ct.TensorType(name=\"input\", shape=example_input.shape)],\n",
    "            convert_to=\"mlprogram\",  # Use the newer ML Program format for better performance\n",
    "            # compute_units=\"ALL\"  # Can be \"ALL\", \"CPU_ONLY\", \"CPU_AND_GPU\", \"CPU_AND_NE\" (Neural Engine)\n",
    "        )\n",
    "\n",
    "        # Set model metadata\n",
    "        mlmodel.short_description = \"PV Segmentation using PyTorch Lightning and SMP\"\n",
    "        mlmodel.input_description['input'] = \"Input image (RGB or multi-channel)\"\n",
    "        output_name = mlmodel._spec.description.output[0].name\n",
    "        mlmodel.output_description[output_name] = \"Segmentation mask for PV panels\"\n",
    "\n",
    "        # Save the model\n",
    "        output_path = Path(\"exported_models\") / f\"{first_config_name}_coreml.mlpackage\"\n",
    "        os.makedirs(output_path.parent, exist_ok=True)\n",
    "        mlmodel.save(str(output_path))\n",
    "        print(f\"Model successfully exported to: {output_path}\")\n",
    "\n",
    "        # ---------- QUANTIZATION OPTIONS (COMMENTED) ----------\n",
    "\n",
    "        # # FP16 Quantization\n",
    "        # print(\"\\nCreating FP16 quantized model...\")\n",
    "        # mlmodel_fp16 = quantization_utils.quantize_weights(mlmodel, nbits=16)\n",
    "        # mlmodel_fp16.save(str(output_path).replace(\".mlpackage\", \"_fp16.mlpackage\"))\n",
    "        # print(\"FP16 model saved.\")\n",
    "\n",
    "        # # INT8 Quantization (more aggressive compression, may affect accuracy)\n",
    "        # # print(\"\\nCreating INT8 quantized model...\")\n",
    "        # # mlmodel_int8 = quantization_utils.quantize_weights(mlmodel, nbits=8)\n",
    "        # # mlmodel_int8.save(str(output_path).replace(\".mlpackage\", \"_int8.mlpackage\"))\n",
    "        # # print(\"INT8 model saved.\")\n",
    "\n",
    "        # # For even more advanced quantization with calibration:\n",
    "        # # from coremltools.optimize.coreml import create_quantized_model\n",
    "        # # mlmodel_quantized = create_quantized_model(\n",
    "        # #    mlmodel,\n",
    "        # #    calibration_data=create_image_generator(calibration_images),\n",
    "        # #    multiarray_dtype='int8'\n",
    "        # # )\n",
    "\n",
    "    except ImportError as e:\n",
    "        print(f\"CoreML export failed: {e}\")\n",
    "        print(\"Please install coremltools using: pip install coremltools\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during CoreML export: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hBXDCmTbes25"
   },
   "source": [
    "## Datasets Overview\n",
    "\n",
    "The training and evaluation of PV segmentation models rely on diverse, publicly available datasets. Many of these datasets are located in [Zenodo](https://zenodo.org/), a general-purpose open-access repository developed under the European OpenAIRE program and operated by CERN. Others are hosted in figshare, a web-based platform for sharing research data and other types of content. The rest are hosted in GitHub repositories or other open-access data platforms.\n",
    "\n",
    "The dataset labels are available in a variety of formats, including CSV, GeoJSON, GeoPackage, ESRI shapefiles, raw raster masks, and GeoParquet. For this notebook, we assume that these datasets have been preprocessed into image patches and corresponding *raster* segmentation masks.\n",
    "\n",
    "Here is a list of some prominent Solar Panel dataset publications, their first authors, DOI links, and approximate number of labels, which can be sources for preparing data for this notebook:\n",
    "\n",
    "-   **\"Distributed solar photovoltaic array location and extent dataset for remote sensing object identification\"** - K. Bradbury, 2016 | [paper DOI](https://doi.org/10.1038/sdata.2016.106) | [dataset DOI](https://doi.org/10.6084/m9.figshare.3385780.v4) | polygon annotations for 19,433 PV modules in 4 cities in California, USA\n",
    "-   **\"A solar panel dataset of very high resolution satellite imagery to support the Sustainable Development Goals\"** - C. Clark et al, 2023 | [paper DOI](https://doi.org/10.1038/s41597-023-02539-8) | [dataset DOI](https://doi.org/10.6084/m9.figshare.22081091.v3) | 2,542 object labels (per spatial resolution)\n",
    "-   \"A harmonised, high-coverage, open dataset of solar photovoltaic installations in the UK\" - D. Stowell et al, 2020** | [paper DOI](https://doi.org/10.1038/s41597-020-00739-0) | [dataset DOI](https://zenodo.org/records/4059881) | 265,418 data points (over 255,000 are stand-alone installations, 1067 solar farms, and rest are subcomponents within solar farms)\n",
    "-   \"Georectified polygon database of ground-mounted large-scale solar photovoltaic sites in the United States\" - K. Sydny, 2023 | [paper DOI](https://doi.org/10.1038/s41597-023-02644-8) | [dataset DOI](https://www.sciencebase.gov/catalog/item/6671c479d34e84915adb7536) | 4186 data points\n",
    "-   \"Vectorized solar photovoltaic installation dataset across China in 2015 and 2020\" - J. Liu et al, 2024 | [paper DOI](https://doi.org/10.1038/s41597-024-04356-z) | [dataset link](https://github.com/qingfengxitu/ChinaPV) | 3,356 PV labels (inspect quality!)\n",
    "-   *\"Multi-resolution dataset for photovoltaic panel segmentation from satellite and aerial imagery\"* - H. Jiang, 2021 | [paper DOI](https://doi.org/10.5194/essd-13-5389-2021) | [dataset DOI](https://doi.org/10.5281/zenodo.5171712) | 3,716 samples of PV data points\n",
    "- **\"A crowdsourced dataset of aerial images with annotated solar photovoltaic arrays and installation metadata\"** - G. Kasmi, 2023 | [paper DOI](https://doi.org/10.1038/s41597-023-01951-4) | [dataset DOI](https://doi.org/10.5281/zenodo.6865878) | > 28K points of PV installations; 13K+ segmentation masks for PV arrays; metadata for 8K+ installations\n",
    "-   \"An Artificial Intelligence Dataset for Solar Energy Locations in India\" - A. Ortiz, 2022 | [paper DOI](https://doi.org/10.1038/s41597-022-01499-9) | [dataset link 1](https://researchlabwuopendata.blob.core.windows.net/solar-farms/solar_farms_india_2021.geojson) or [dataset link 2](https://raw.githubusercontent.com/microsoft/solar-farms-mapping/refs/heads/main/data/solar_farms_india_2021_merged_simplified.geojson) | 117 geo-referenced points of solar installations across India\n",
    "- **\"GloSoFarID: Global multispectral dataset for Solar Farm IDentification in satellite imagery\"** - Z. Yang, 2024** | [paper DOI](https://doi.org/10.48550/arXiv.2404.05180) | [vector dataset DOI](https://github.com/yzyly1992/GloSoFarID/tree/main/data_coordinates) | [zenodo rasters DOI](https://doi.org/10.5281/zenodo.10939099) | 6,793 PV samples across 3 years (double counting of samples)\n",
    "-   \"A global inventory of photovoltaic solar energy generating units\" - L. Kruitwagen et al, 2021 | [paper DOI](https://doi.org/10.1038/s41586-021-03957-7) | [dataset DOI](https://doi.org/10.5281/zenodo.5005867) | 50,426 for training, cross-validation, and testing; 68,661 predicted polygon labels\n",
    "-   \"Harmonised global datasets of wind and solar farm locations and power\" - S. Dunnett et al, 2020 | [paper DOI](https://doi.org/10.1038/s41597-020-0469-8) | [dataset DOI](https://doi.org/10.6084/m9.figshare.11310269.v6) | 35272 PV installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YR00dC_hes25"
   },
   "outputs": [],
   "source": [
    "# # %% --- 9. Interactive Visualization Slideshow ---\n",
    "\n",
    "# from IPython.display import HTML, Image, display\n",
    "# import os\n",
    "# from pathlib import Path\n",
    "# import ipywidgets as widgets\n",
    "# import matplotlib.pyplot as plt\n",
    "# import glob\n",
    "\n",
    "# def create_slideshow(image_dir=\"report/assets/visualizations\", height=500):\n",
    "#     \"\"\"Create an interactive slideshow from images in the specified directory.\n",
    "\n",
    "#     Args:\n",
    "#         image_dir: Path to directory containing the screenshots/images\n",
    "#         height: Height in pixels for the display area\n",
    "\n",
    "#     Returns:\n",
    "#         Interactive widget displaying the slideshow\n",
    "#     \"\"\"\n",
    "#     # Create directory if it doesn't exist\n",
    "#     os.makedirs(image_dir, exist_ok=True)\n",
    "\n",
    "#     # Find all image files\n",
    "#     image_extensions = [\"jpg\", \"jpeg\", \"png\", \"gif\"]\n",
    "#     image_files = []\n",
    "#     for ext in image_extensions:\n",
    "#         image_files.extend(glob.glob(os.path.join(image_dir, f\"*.{ext}\")))\n",
    "#         image_files.extend(glob.glob(os.path.join(image_dir, f\"*.{ext.upper()}\")))  # Include uppercase extensions\n",
    "\n",
    "#     image_files = sorted(image_files)  # Sort alphabetically\n",
    "\n",
    "#     if not image_files:\n",
    "#         print(f\"No images found in {image_dir}\")\n",
    "#         print(f\"Please add your screenshots to the {image_dir} directory\")\n",
    "#         print(f\"Supported formats: {', '.join(image_extensions)}\")\n",
    "#         return None\n",
    "\n",
    "#     # Create widgets\n",
    "#     slider = widgets.IntSlider(\n",
    "#         value=0,\n",
    "#         min=0,\n",
    "#         max=len(image_files)-1,\n",
    "#         step=1,\n",
    "#         description='Image:',\n",
    "#         continuous_update=False,\n",
    "#         layout=widgets.Layout(width='50%')\n",
    "#     )\n",
    "\n",
    "#     prev_button = widgets.Button(\n",
    "#         description='Previous',\n",
    "#         disabled=False,\n",
    "#         button_style='',\n",
    "#         tooltip='Previous image',\n",
    "#         icon='arrow-left'\n",
    "#     )\n",
    "\n",
    "#     next_button = widgets.Button(\n",
    "#         description='Next',\n",
    "#         disabled=False,\n",
    "#         button_style='',\n",
    "#         tooltip='Next image',\n",
    "#         icon='arrow-right'\n",
    "#     )\n",
    "\n",
    "#     image_widget = widgets.Image(\n",
    "#         layout=widgets.Layout(height=f\"{height}px\"),\n",
    "#     )\n",
    "\n",
    "#     title_widget = widgets.HTML(\n",
    "#         layout=widgets.Layout(height='auto')\n",
    "#     )\n",
    "\n",
    "#     # Define callback functions\n",
    "#     def on_slider_change(change):\n",
    "#         index = change['new']\n",
    "#         display_image(index)\n",
    "\n",
    "#     def on_prev_button_click(b):\n",
    "#         if slider.value > 0:\n",
    "#             slider.value -= 1\n",
    "\n",
    "#     def on_next_button_click(b):\n",
    "#         if slider.value < len(image_files) - 1:\n",
    "#             slider.value += 1\n",
    "\n",
    "#     def display_image(index):\n",
    "#         filename = image_files[index]\n",
    "#         with open(filename, 'rb') as f:\n",
    "#             image_widget.value = f.read()\n",
    "\n",
    "#         base_filename = os.path.basename(filename)\n",
    "#         title_widget.value = f\"<div style='text-align: center; font-weight: bold;'>{base_filename} ({index + 1}/{len(image_files)})</div>\"\n",
    "\n",
    "#     # Attach callbacks to widgets\n",
    "#     slider.observe(on_slider_change, names='value')\n",
    "#     prev_button.on_click(on_prev_button_click)\n",
    "#     next_button.on_click(on_next_button_click)\n",
    "\n",
    "#     # Display initial image\n",
    "#     display_image(0)\n",
    "\n",
    "#     # Arrange widgets\n",
    "#     button_box = widgets.HBox([prev_button, next_button])\n",
    "#     main_box = widgets.VBox([title_widget, image_widget, slider, button_box])\n",
    "\n",
    "#     return main_box\n",
    "\n",
    "# # Create and display the slideshow widget\n",
    "# slideshow = create_slideshow()\n",
    "\n",
    "# if slideshow:\n",
    "#     display(slideshow)\n",
    "# else:\n",
    "#     # Create directory structure if it doesn't exist\n",
    "#     viz_dir = \"report/assets/visualizations\"\n",
    "#     os.makedirs(viz_dir, exist_ok=True)\n",
    "\n",
    "#     # Display placeholder image with instructions\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     plt.text(0.5, 0.5, f\"Add your screenshots to:\\n{os.path.abspath(viz_dir)}\",\n",
    "#              ha='center', va='center', fontsize=16, wrap=True)\n",
    "#     plt.axis('off')\n",
    "#     plt.title(\"Interactive Slideshow - Setup Instructions\", fontsize=18)\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "#     print(\"\\nTo use the slideshow:\")\n",
    "#     print(f\"1. Add your screenshot images to: {os.path.abspath(viz_dir)}\")\n",
    "#     print(\"2. Make sure they're in jpg, jpeg, png, or gif format\")\n",
    "#     print(\"3. Re-run this cell to see the interactive slideshow\")\n",
    "#     print(\"\\nTip: You can change the images directory by modifying the argument to create_slideshow()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_QJy9q-1es26"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ux6IeOWes26"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "10b59470983649ccaf3b73103690de25": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": "initial"
     }
    },
    "140645207fdc4a74b41695991987be82": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ImageModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ImageModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ImageView",
      "format": "png",
      "height": "auto",
      "layout": "IPY_MODEL_8a47f9d00cc846cab947bc67c7f79e70",
      "width": "50%"
     }
    },
    "170c7a0f691948c29f8767d9e9b90461": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1a78d9f6339348a5aab198975ba47021": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_140645207fdc4a74b41695991987be82",
       "IPY_MODEL_f8d285a41d3d497a8b7f42f2a681e56b"
      ],
      "layout": "IPY_MODEL_574ea374e63d493b9a166e97dcb7b878"
     }
    },
    "2464335edf1c4b7ab449d35ca96fdbbe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DropdownModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DropdownModel",
      "_options_labels": [
       "False",
       "True",
       "reduce-overhead",
       "max-autotune"
      ],
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "DropdownView",
      "description": "Torch Compile Mode:",
      "description_tooltip": null,
      "disabled": false,
      "index": 2,
      "layout": "IPY_MODEL_170c7a0f691948c29f8767d9e9b90461",
      "style": "IPY_MODEL_2ab47fd22c104d0f84a7d2b12a495caf"
     }
    },
    "2ab47fd22c104d0f84a7d2b12a495caf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": "initial"
     }
    },
    "2f205a6f99d44e99ab3b0bb463a392c6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "400px"
     }
    },
    "372c299f611642829acc7af54010bd68": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DropdownModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DropdownModel",
      "_options_labels": [
       "Native 31cm Source (416px)",
       "HD 15.5cm Source (832px)",
       "HD Source downscaled (512px)",
       "HD Source downscaled (416px)",
       "Native Source (256px input - downscaled)"
      ],
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "DropdownView",
      "description": "Resolution:",
      "description_tooltip": null,
      "disabled": false,
      "index": 0,
      "layout": "IPY_MODEL_d181151dfb0a4d4ca0f6462d77f0d15d",
      "style": "IPY_MODEL_d9486b799f9946bb9db438303052643c"
     }
    },
    "4074769a62a7456aae009efd4d923db1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DropdownModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DropdownModel",
      "_options_labels": [
       "Maxar Germany VHR MSI",
       "France crowdsourced aerials, rooftop",
       "USA Cali, USGS ortho aerials"
      ],
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "DropdownView",
      "description": "Train Dataset:",
      "description_tooltip": null,
      "disabled": false,
      "index": 0,
      "layout": "IPY_MODEL_4c903bb790364f4dbe34e32c4a1c27b0",
      "style": "IPY_MODEL_9a8f3aee11ee40999b3d27cafce57643"
     }
    },
    "425a6eb80b0b46cb9709173f8998d467": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4428245d00a74eebb6cef68d6718798e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_425a6eb80b0b46cb9709173f8998d467",
      "placeholder": "​",
      "style": "IPY_MODEL_7944445abc9c4d7aa22f09498b0283a5",
      "value": "<p>PV imagery: PlanetLabs Dove vs Maxar Worldview-3</p>"
     }
    },
    "450b0332bec54d2eb515ae3b5342c2ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_dfbcb8cf4df44919974eb717221d1e6c",
       "IPY_MODEL_e29e78348af049fb9488b118e0b481eb"
      ],
      "layout": "IPY_MODEL_7f8510d346e444d6a844b1b9a4014531"
     }
    },
    "495ff8c2d24b4313b1b871a1f3e594f3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4c903bb790364f4dbe34e32c4a1c27b0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4eb294cc7ec141ff928184fbb181bdf8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1a78d9f6339348a5aab198975ba47021",
       "IPY_MODEL_4428245d00a74eebb6cef68d6718798e"
      ],
      "layout": "IPY_MODEL_7cb51c19393b4844a348063a1f704a1b"
     }
    },
    "525ce3fbb1a2450eafaed9a5e17faa50": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "574ea374e63d493b9a166e97dcb7b878": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": "center",
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6f7651f2884e4c91b87f94ee98dc18f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8c558a7dedd4400db48d56e53dc51563",
      "placeholder": "​",
      "style": "IPY_MODEL_aa230c2e5d3c47f98a25828752eef8dd",
      "value": "<p style=\"margin-bottom:10px\">\n    Select which Maxar imagery resolution to use for model training.\n    HD refers to Maxar's proprietary upscaling algorithm that simulates 15.5cm ground sample distance.\n    </p>"
     }
    },
    "790aad4163374922920a62596c14f39f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7944445abc9c4d7aa22f09498b0283a5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7cb51c19393b4844a348063a1f704a1b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": "center",
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "flex",
      "flex": null,
      "flex_flow": "column",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "7f8510d346e444d6a844b1b9a4014531": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "894adc77e2f544f9bb6ca8a6a52757f4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "IntSliderModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntSliderModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "IntSliderView",
      "continuous_update": true,
      "description": "Epochs:",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_dd700469a2dc408fb3330c99bf580510",
      "max": 100,
      "min": 30,
      "orientation": "horizontal",
      "readout": true,
      "readout_format": "d",
      "step": 5,
      "style": "IPY_MODEL_ba17dd52d2ca420d9c24de91ec0ac051",
      "value": 30
     }
    },
    "8a47f9d00cc846cab947bc67c7f79e70": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8c26d387ba7f4b0d8e478afd27dc7c2a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "IntSliderModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntSliderModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "IntSliderView",
      "continuous_update": true,
      "description": "Batch Size:",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_94089f70a98d408fa220708b908cc462",
      "max": 128,
      "min": 16,
      "orientation": "horizontal",
      "readout": true,
      "readout_format": "d",
      "step": 8,
      "style": "IPY_MODEL_95ddfb019416446e90e1ed8e63a24b3d",
      "value": 16
     }
    },
    "8c558a7dedd4400db48d56e53dc51563": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "91509d54ed9440d399c56d9784517194": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatSliderModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatSliderModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "FloatSliderView",
      "continuous_update": true,
      "description": "Val Split Ratio:",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_d361e57dbbcc48bcbfbc29b7de4a35f1",
      "max": 0.5,
      "min": 0.1,
      "orientation": "horizontal",
      "readout": true,
      "readout_format": ".2f",
      "step": 0.05,
      "style": "IPY_MODEL_d29666a9e0464d669936a4b92f540648",
      "value": 0.2
     }
    },
    "94089f70a98d408fa220708b908cc462": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "95ddfb019416446e90e1ed8e63a24b3d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "SliderStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "SliderStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": "initial",
      "handle_color": null
     }
    },
    "9a8f3aee11ee40999b3d27cafce57643": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": "initial"
     }
    },
    "a15881ff72e24f528336eecb32d02329": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "400px"
     }
    },
    "aa230c2e5d3c47f98a25828752eef8dd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ab563228053d4483930b3a055dbb7512": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ImageModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ImageModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ImageView",
      "format": "jpg",
      "height": "auto",
      "layout": "IPY_MODEL_790aad4163374922920a62596c14f39f",
      "width": "95%"
     }
    },
    "ba17dd52d2ca420d9c24de91ec0ac051": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "SliderStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "SliderStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": "initial",
      "handle_color": null
     }
    },
    "c0581a3c140148429d0c9a3206bd97f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "SliderStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "SliderStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": "initial",
      "handle_color": null
     }
    },
    "c985dbc603124f2890d54263a13820d3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DropdownModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DropdownModel",
      "_options_labels": [
       "resnet18",
       "resnet34",
       "resnet50",
       "resnet101",
       "resnet152",
       "resnext50_32x4d",
       "resnext101_32x8d",
       "resnext101_64x4d",
       "resnext101_32x8d",
       "mobilenet_v2",
       "inceptionresnetv2",
       "efficientnet-b0",
       "efficientnet-b2",
       "efficientnet-b4",
       "efficientnet-b6",
       "efficientnet-b7",
       "timm-tf_efficientnet_lite4",
       "vgg16",
       "vgg19",
       "vgg16_bn",
       "densenet121",
       "densenet169",
       "densenet201",
       "densenet161",
       "tu-fastvit_sa36.apple_dist_in1k",
       "tu-efficientformer_l3.snap_dist_in1k",
       "mit_b0",
       "mit_b1",
       "mit_b2",
       "mit_b3",
       "mit_b4",
       "mit_b5",
       "tu-swin_base_patch4_window7_224.ms_in22k_ft_in1k",
       "tu-swin_large_patch4_window7_224.ms_in22k_ft_in1k",
       "tu-swinv2_large_window12to16_192to256.ms_in22k_ft_in1k",
       "mobileone_s0",
       "mobileone_s2",
       "mobileone_s3",
       "mobileone_s5",
       "tu-vitamin_large2_224.datacomp1b_clip",
       "tu-vit_medium_patch16_clip_224.tinyclip_yfcc15m",
       "tu-xcit_large_24_p8_384.fb_dist_in1k",
       "tu-twins_svt_base.in1k"
      ],
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "DropdownView",
      "description": "Select encoder (backend):",
      "description_tooltip": null,
      "disabled": false,
      "index": 4,
      "layout": "IPY_MODEL_a15881ff72e24f528336eecb32d02329",
      "style": "IPY_MODEL_eeeaf88a3091474991df7b4b0cd10eeb"
     }
    },
    "cf573829b0f841a091dc97ddaa280561": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "IntSliderModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntSliderModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "IntSliderView",
      "continuous_update": true,
      "description": "Accum N Grad. Batches:",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_d999f1054a294585b4d8d383dde29992",
      "max": 8,
      "min": 2,
      "orientation": "horizontal",
      "readout": true,
      "readout_format": "d",
      "step": 2,
      "style": "IPY_MODEL_c0581a3c140148429d0c9a3206bd97f5",
      "value": 2
     }
    },
    "d181151dfb0a4d4ca0f6462d77f0d15d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "300px"
     }
    },
    "d29666a9e0464d669936a4b92f540648": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "SliderStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "SliderStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": "initial",
      "handle_color": null
     }
    },
    "d361e57dbbcc48bcbfbc29b7de4a35f1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d9486b799f9946bb9db438303052643c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": "initial"
     }
    },
    "d999f1054a294585b4d8d383dde29992": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dd700469a2dc408fb3330c99bf580510": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dfbcb8cf4df44919974eb717221d1e6c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ImageModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ImageModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ImageView",
      "format": "jpg",
      "height": "auto",
      "layout": "IPY_MODEL_495ff8c2d24b4313b1b871a1f3e594f3",
      "width": "90%"
     }
    },
    "e29e78348af049fb9488b118e0b481eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ImageModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ImageModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ImageView",
      "format": "jpg",
      "height": "auto",
      "layout": "IPY_MODEL_525ce3fbb1a2450eafaed9a5e17faa50",
      "width": "90%"
     }
    },
    "e625d0edca0b4d39870fe0db08aa80ac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "RadioButtonsModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "RadioButtonsModel",
      "_options_labels": [
       "Unet",
       "UnetPlusPlus",
       "FPN",
       "Linknet",
       "PSPNet",
       "DeepLabV3Plus",
       "MAnet",
       "PAN",
       "Segformer",
       "DPT"
      ],
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "RadioButtonsView",
      "description": "Select model arch:",
      "description_tooltip": null,
      "disabled": false,
      "index": 1,
      "layout": "IPY_MODEL_2f205a6f99d44e99ab3b0bb463a392c6",
      "style": "IPY_MODEL_10b59470983649ccaf3b73103690de25"
     }
    },
    "eeeaf88a3091474991df7b4b0cd10eeb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": "initial"
     }
    },
    "ef1c2c06c3704dc9adfcb8711fca8760": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f8d285a41d3d497a8b7f42f2a681e56b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ImageModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ImageModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ImageView",
      "format": "png",
      "height": "auto",
      "layout": "IPY_MODEL_ef1c2c06c3704dc9adfcb8711fca8760",
      "width": "50%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
