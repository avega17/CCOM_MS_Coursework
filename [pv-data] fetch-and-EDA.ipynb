{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73f645e0",
   "metadata": {},
   "source": [
    "# Fetch Open Datasets of PV locations\n",
    "\n",
    "Many of these datasets are located in [Zenodo](https://zenodo.org/), a general-purpose open-access repository developed under the European OpenAIRE program and operated by CERN. Others are hosted in figshare, a web-based platform for sharing research data and other types of content. The rest are hosted in GitHub repositories or other open-access platforms.The datasets are available in various formats, including CSV, GeoJSON, and shapefiles, and raster masks. We'll be using open-source Python libraries to download and process them into properly georeferenced geoparquet files\n",
    "that will serve as a base for our duckdb tables that we'll manage with dbt\n",
    "\n",
    "Here we list the dataset titles alongside their first author, DOI links, and their number of labels:\n",
    "- \"A solar panel dataset of very high resolution satellite imagery to support the Sustainable Development Goals\" - C. Clark et al, 2023 | [paper DOI](https://doi.org/10.1038/s41597-023-02539-8) | [dataset DOI](https://doi.org/10.6084/m9.figshare.22081091.v3) | 2,542 object labels (per spatial resolution)\n",
    "- \"A global inventory of photovoltaic solar energy generating units\" - L. Kruitwagen et al, 2021 | [paper DOI](https://doi.org/10.1038/s41586-021-03957-7) | [dataset DOI](https://doi.org/10.5281/zenodo.5005867) | 50,426 for training, cross-validation, and testing; 68,661 predicted polygon labels \n",
    "- \"A harmonised, high-coverage, open dataset of solar photovoltaic installations in the UK\" - D. Stowell et al, 2020 | [paper DOI](https://doi.org/10.1038/s41597-020-00739-0) | [dataset DOI](https://zenodo.org/records/4059881) | 265,418 data points (over 255,000 are stand-alone installations, 1067 solar farms, and rest are subcomponents within solar farms)\n",
    "- \"A crowdsourced dataset of aerial images with annotated solar photovoltaic arrays and installation metadata\" - G. Kasmi, 2023 | [paper DOI](https://doi.org/10.1038/s41597-023-01951-4) | [dataset DOI](https://doi.org/10.5281/zenodo.6865878) | > 28K points of PV installations; 13K+ segmentation masks for PV arrays; metadata for 8K+ installations\n",
    "- \"Georectified polygon database of ground-mounted large-scale solar photovoltaic sites in the United States\" - K. Sydny, 2023 | [paper DOI](https://doi.org/10.1038/s41597-023-02644-8) | [dataset DOI](https://www.sciencebase.gov/catalog/item/6671c479d34e84915adb7536) | 4186 data points (Note: these correspond to PV _facilities_ rather than individual panel arrays or objects and need filtering of duplicates with other datasets and further processing to extract the PV arrays in the facility)\n",
    "- \"Vectorized solar photovoltaic installation dataset across China in 2015 and 2020\" - J. Liu et al, 2024 | [paper DOI](https://doi.org/10.1038/s41597-024-04356-z) | [dataset link](https://github.com/qingfengxitu/ChinaPV) | 3,356 PV labels (inspect quality!)\n",
    "- \"Multi-resolution dataset for photovoltaic panel segmentation from satellite and aerial imagery\" - H. Jiang, 2021 | [paper DOI](https://doi.org/10.5194/essd-13-5389-2021) | [dataset DOI](https://doi.org/10.5281/zenodo.5171712) | 3,716 samples of PV data points\n",
    "- \"An Artificial Intelligence Dataset for Solar Energy Locations in India\" - A. Ortiz, 2022 | [paper DOI](https://doi.org/10.1038/s41597-022-01499-9) | [dataset link 1](https://researchlabwuopendata.blob.core.windows.net/solar-farms/solar_farms_india_2021.geojson) or [dataset link 2](https://raw.githubusercontent.com/microsoft/solar-farms-mapping/refs/heads/main/data/solar_farms_india_2021_merged_simplified.geojson) | 117 geo-referenced points of solar installations across India\n",
    "- \"GloSoFarID: Global multispectral dataset for Solar Farm IDentification in satellite imagery\" - Z. Yang, 2024 | [paper DOI](https://doi.org/10.48550/arXiv.2404.05180) | [dataset DOI](https://github.com/yzyly1992/GloSoFarID/tree/main/data_coordinates) | 6,793 PV samples across 3 years (double counting of samples)\n",
    "- \"Distributed solar photovoltaic array location and extent dataset for remote sensing object identification\" - K. Bradbury, 2016 | [paper DOI](https://doi.org/10.1038/sdata.2016.106) | [dataset DOI](https://doi.org/10.6084/m9.figshare.3385780.v4) | polygon annotations for 19,433 PV modules in 4 cities in California, USA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8564ba95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import Layout\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from branca.colormap import linear\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "import shapely\n",
    "import pygeohash\n",
    "import folium\n",
    "import lonboard\n",
    "import pydeck as pdk\n",
    "# import openeo \n",
    "# import pystac_client\n",
    "\n",
    "# import easystac\n",
    "# import cubo\n",
    "\n",
    "import duckdb as dd \n",
    "import datahugger\n",
    "import sciencebasepy\n",
    "from seedir import seedir\n",
    "\n",
    "# python libraries\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import urllib.parse\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import tempfile\n",
    "import shutil\n",
    "import pprint as pp\n",
    "import time\n",
    "import re\n",
    "from zipfile import ZipFile\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3aad185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dict of metadata for datasets\n",
    "# this will be used for interactive widget and managing downloads\n",
    "\n",
    "# for maxar dataset\n",
    "# Catalogue ID 1040050029DC8C00; use to find geospatial extent coords\n",
    "# The geocoordinates for each solar panel object may be determined using the native resolution labels (found in the labels_native directory). \n",
    "# The center and width values for each object, along with the relative location information provided by the naming convention for each label, \n",
    "# may be used to determine the pixel coordinates for each object in the full, corresponding native resolution tile. \n",
    "# The pixel coordinates may be translated to geocoordinates using the EPSG:32633 coordinate system and the following geotransform for each tile:\n",
    "\n",
    "# Tile 1: (307670.04, 0.31, 0.0, 5434427.100000001, 0.0, -0.31)\n",
    "# Tile 2: (312749.07999999996, 0.31, 0.0, 5403952.860000001, 0.0, -0.31)\n",
    "# Tile 3: (312749.07999999996, 0.31, 0.0, 5363320.540000001, 0.0, -0.31)\n",
    "# see here on gdal format geotransform: https://gdal.org/en/stable/tutorials/geotransforms_tut.html\n",
    "\n",
    "# look into adding dataset crs or projection to metadata dict\n",
    "# note that most of these details are hardcoded and difficult to parse ahead of time\n",
    "# load environment variables\n",
    "load_dotenv()\n",
    "DATASET_DIR = Path(os.getenv('DATA_PATH'))\n",
    "dataset_metadata = {\n",
    "    'deu_maxar_vhr_2023': {\n",
    "        'doi': '10.6084/m9.figshare.22081091.v3',\n",
    "        'repo': 'figshare',\n",
    "        'compression': 'zip',\n",
    "        'label_fmt': 'yolo_fmt_txt',\n",
    "        'has_imgs': False,\n",
    "        'label_count': 2542 # solar panel objects (ie not individual panels)\n",
    "    },\n",
    "    'uk_crowdsourced_pv_2020': {\n",
    "        'doi': '10.5281/zenodo.4059881',\n",
    "        'repo': 'zenodo',\n",
    "        'compression': None,\n",
    "        'label_fmt': 'geojson',\n",
    "        'geom_type': {'features': ['Point', 'Polygon', 'MultiPolygon']},\n",
    "        'crs': None, # default to WGS84 when processing\n",
    "        'has_imgs': False,\n",
    "        'label_count': 265418\n",
    "    },\n",
    "    # note for report later: Maxar Technologies (MT) was primarily used to determine the extent of solar arrays\n",
    "    'usa_eia_large_scale_pv_2023': {\n",
    "        'doi': '10.5281/zenodo.8038684',\n",
    "        'repo': 'sciencebase',\n",
    "        'compression': 'zip',\n",
    "        'label_fmt': 'shapefile',\n",
    "        'has_imgs': False,\n",
    "        'label_count': 4186\n",
    "    },\n",
    "    'chn_med_res_pv_2024': {\n",
    "        # using github files since zenodo shapefiles fail to load in QGIS\n",
    "        'doi': 'https://github.com/qingfengxitu/ChinaPV/tree/main',\n",
    "        'repo': 'github',\n",
    "        'compression': None,\n",
    "        'label_fmt': 'shapefile',\n",
    "        'has_imgs': False,\n",
    "        'label_count': 3356\n",
    "    },\n",
    "    'usa_cali_usgs_pv_2016': {\n",
    "        'doi': '10.6084/m9.figshare.3385780.v4',\n",
    "        'repo': 'figshare',\n",
    "        'compression': None,\n",
    "        'label_fmt': 'geojson',\n",
    "        'crs': 'NAD83',\n",
    "        'geom_type': {'features': 'Polygon'},\n",
    "        'has_imgs': False,\n",
    "        'label_count': 19433\n",
    "    },\n",
    "    'chn_jiangsu_vhr_pv_2021': {\n",
    "        'doi': '10.5281/zenodo.5171712',\n",
    "        'repo': 'zenodo',\n",
    "        'compression': 'zip',\n",
    "        # look into geotransform details for processing these labels\n",
    "        'label_fmt': 'pixel_mask',\n",
    "        'has_imgs': True,\n",
    "        'label_count': 3716\n",
    "    },\n",
    "    'ind_pv_solar_farms_2022': {\n",
    "        'doi': 'https://raw.githubusercontent.com/microsoft/solar-farms-mapping/refs/heads/main/data/solar_farms_india_2021_merged_simplified.geojson',\n",
    "        'repo': 'github',\n",
    "        'compression': None,\n",
    "        'label_fmt': 'geojson',\n",
    "        'geom_type': {'features': 'MultiPolygon'}, \n",
    "        'crs': 'WGS84',\n",
    "        'has_imgs': False,\n",
    "        'label_count': 117\n",
    "    },\n",
    "    'fra_west_eur_pv_installations_2023': {\n",
    "        'doi': '10.5281/zenodo.6865878',\n",
    "        'repo': 'zenodo',\n",
    "        'compression': 'zip',\n",
    "        'label_fmt': 'json',\n",
    "        'geom_type': {'Polygon': ['Point']},\n",
    "        'crs': None, \n",
    "        'has_imgs': True, \n",
    "        'label_count': (13303, 7686)\n",
    "    },\n",
    "    'global_pv_inventory_sent2_spot_2021': {\n",
    "        'doi': '10.5281/zenodo.5005867',\n",
    "        'repo': 'zenodo',\n",
    "        'compression': None,\n",
    "        'label_fmt': 'geojson',\n",
    "        'geom_type': ['Polygon'],\n",
    "        'crs': 'WGS84',\n",
    "        'has_imgs': False,\n",
    "        'label_count': 50426\n",
    "    },\n",
    "    'global_pv_inventory_sent2_2024': {\n",
    "        'doi': 'https://github.com/yzyly1992/GloSoFarID/tree/main/data_coordinates',\n",
    "        'repo': 'github',\n",
    "        'compression': None,\n",
    "        'label_fmt': 'json',\n",
    "        'crs': None, # default to WGS84 when processing\n",
    "        'geom_type': ['Point'], # normal json with no geometry attribute\n",
    "        'has_imgs': True, \n",
    "        'label_count': 6793\n",
    "    }\n",
    "\n",
    "}\n",
    "\n",
    "dataset_choices = [\n",
    "    # 'global_pv_inventory_sent2_2024',\n",
    "    'global_pv_inventory_sent2_spot_2021',\n",
    "    # 'fra_west_eur_pv_installations_2023',\n",
    "    'ind_pv_solar_farms_2022',\n",
    "    'usa_cali_usgs_pv_2016',\n",
    "    # 'chn_med_res_pv_2024',\n",
    "    # 'usa_eia_large_scale_pv_2023',\n",
    "    # 'uk_crowdsourced_pv_2020',\n",
    "    # 'deu_maxar_vhr_2023'   \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6162a17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store selected datasets\n",
    "# mostly gen by github copilot with Claude 3.7 model\n",
    "selected_datasets = dataset_choices.copy()\n",
    "\n",
    "def format_dataset_info(dataset):\n",
    "    \"\"\"Create a formatted HTML table for dataset metadata\"\"\"\n",
    "    metadata = dataset_metadata[dataset]\n",
    "    \n",
    "    # Create table with metadata\n",
    "    html = f\"\"\"\n",
    "    <style>\n",
    "    .dataset-table {{\n",
    "        border-collapse: collapse;\n",
    "        width: 30%;\n",
    "        margin: 20px auto;\n",
    "        font-family: Arial, sans-serif;\n",
    "    }}\n",
    "    .dataset-table th, .dataset-table td {{\n",
    "        border: 1px solid #ddd;\n",
    "        padding: 8px;\n",
    "        text-align: left;\n",
    "    }}\n",
    "    .dataset-table th {{\n",
    "        background-color: #f2f2f2;\n",
    "        font-weight: bold;\n",
    "    }}\n",
    "    </style>\n",
    "    <table class=\"dataset-table\">\n",
    "        <tr><th>Metadata</th><th>Value</th></tr>\n",
    "        <tr><td>DOI/URL</td><td>{metadata['doi']}</td></tr>\n",
    "        <tr><td>Repository</td><td>{metadata['repo']}</td></tr>\n",
    "        <tr><td>Compression</td><td>{metadata['compression'] or 'None'}</td></tr>\n",
    "        <tr><td>Label Format</td><td>{metadata['label_fmt']}</td></tr>\n",
    "        <tr><td>Has Images</td><td>{metadata['has_imgs']}</td></tr>\n",
    "        <tr><td>Label Count</td><td>{metadata.get('label_count', 'Unknown')}</td></tr>\n",
    "    </table>\n",
    "    \"\"\"\n",
    "    return html\n",
    "\n",
    "# Create an accordion to display selected datasets with centered layout\n",
    "dataset_accordion = widgets.Accordion(\n",
    "    children=[widgets.HTML(format_dataset_info(ds)) for ds in selected_datasets],\n",
    "    layout=Layout(width='50%', margin='0 auto')\n",
    ")\n",
    "for i, ds in enumerate(selected_datasets):\n",
    "    dataset_accordion.set_title(i, ds)\n",
    "\n",
    "# Define a function to add or remove datasets\n",
    "def manage_datasets(action, dataset=None):\n",
    "    global selected_datasets, dataset_accordion\n",
    "    \n",
    "    if action == 'add' and dataset and dataset not in selected_datasets:\n",
    "        selected_datasets.append(dataset)\n",
    "    elif action == 'remove' and dataset and dataset in selected_datasets:\n",
    "        selected_datasets.remove(dataset)\n",
    "    \n",
    "    # Update the accordion with current selections\n",
    "    dataset_accordion.children = [widgets.HTML(format_dataset_info(ds)) for ds in selected_datasets]\n",
    "    for i, ds in enumerate(selected_datasets):\n",
    "        dataset_accordion.set_title(i, ds)\n",
    "    \n",
    "    f\"Currently selected datasets: {len(selected_datasets)}\"\n",
    "\n",
    "# Create dropdown for available datasets\n",
    "dataset_dropdown = widgets.Dropdown(\n",
    "    options=list(dataset_metadata.keys()),\n",
    "    description='Dataset:',\n",
    "    disabled=False,\n",
    "    layout=Layout(width='70%', margin='20 20 auto 20 20')\n",
    ")\n",
    "\n",
    "# Create buttons for actions\n",
    "add_button = widgets.Button(description=\"Add Dataset\", button_style='success')\n",
    "remove_button = widgets.Button(description=\"Remove Dataset\", button_style='danger')\n",
    "\n",
    "# Define button click handlers\n",
    "def on_add_clicked(b):\n",
    "    manage_datasets('add', dataset_dropdown.value)\n",
    "\n",
    "def on_remove_clicked(b):\n",
    "    manage_datasets('remove', dataset_dropdown.value)\n",
    "\n",
    "# Link buttons to handlers\n",
    "add_button.on_click(on_add_clicked)\n",
    "remove_button.on_click(on_remove_clicked)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f44fde4",
   "metadata": {},
   "source": [
    "## Dataset Selection Interface\n",
    "#### Use the dropdown and buttons below to customize which solar panel datasets will be fetched and processed.\n",
    "- Select a dataset from the dropdown:\n",
    "    - Click \"Add Dataset\" to include it in processing\n",
    "    - Click \"Remove Dataset\" to exclude it\n",
    "- View metadata table in the selected dataset's dropdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb943c7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4292dd7afce848f0a35ba85277357cc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Dropdown(description='Dataset:', layout=Layout(margin='20 20 auto 20 20', width='70%'), options…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b33ee95a40a4466cb4ad3a162a84b210",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(HTML(value='\\n    <style>\\n    .dataset-table {\\n        border-collapse: collapse;\\n     …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the widgets\n",
    "display(widgets.HBox([dataset_dropdown, add_button, remove_button]))\n",
    "display(dataset_accordion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a78984",
   "metadata": {},
   "source": [
    "# Fetching and Organizing datasets for later-preprocessing\n",
    "\n",
    "We will use [datahugger](https://j535d165.github.io/datahugger/) to fetch datasets hosted in Zenodo, figshare, and GitHub. \n",
    "\n",
    "We will sciencebase for the dataset hosted in the USGS ScienceBase Catalog.\n",
    "We will pre-process and convert datasets into geojson, if not already formatted, and manage these using [geopandas](https://geopandas.org/). These will be further processed into geoparquet files for use in duckdb tables used to manage and later consolidate the datasets with dbt.  \n",
    "- The datasets will be stored in the `data/` directory\n",
    "    - the geoparquet files will be stored in the `data/geoparquet/` directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f56a89",
   "metadata": {},
   "source": [
    "#### Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b86cf8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# move to utility functions later\n",
    "# def fetch_github_repo_files(dataset_name, \n",
    "\n",
    "\n",
    "# use the metadata to fetch the dataset files using datahugger\n",
    "def fetch_dataset_files(dataset_name, max_mb=100, force=False):\n",
    "    metadata = dataset_metadata[dataset_name]\n",
    "    doi = metadata['doi']\n",
    "    repo = metadata['repo']\n",
    "    compression = metadata['compression']\n",
    "    label_fmt = metadata['label_fmt']\n",
    "    # convert to bytes\n",
    "    max_dl = max_mb * 1024 * 1024\n",
    "    dataset_dir = os.path.join(os.getenv('DATA_PATH'), 'raw', 'labels', dataset_name)\n",
    "    geofile_regex = r'^(.*\\.(geojson|json|shp|zip|csv))$'\n",
    "    dst = os.path.join(os.getcwd(), dataset_dir)\n",
    "    dst_p = Path(dst)\n",
    "\n",
    "    # prettyprint metadata and dst info\n",
    "    # pp.pprint(metadata)\n",
    "    # print(f\"Destination: {dataset_dir}\")\n",
    "    # print(f\"Max download size: {max_mb} MB\")\n",
    "    # print(f\"Force Download: {force}\")\n",
    "\n",
    "    dataset_tree = {}\n",
    "\n",
    "    # TODO: move different repo handling to separate functions\n",
    "\n",
    "    # use datahugger to fetch files from most repos\n",
    "    if repo in ['figshare', 'zenodo']:\n",
    "\n",
    "        ds_tree = datahugger.get(doi, dst, max_file_size=max_dl, force_download=force)\n",
    "        # compare files to be fetched (after filtering on max file size) with existing files  \n",
    "        files_to_fetch = [f['name'] for f in ds_tree.dataset.files if f['size'] <= max_dl]\n",
    "        ds_files = [os.path.join(root, fname) for root, dirs, files in os.walk(dst_p) for fname in files if re.match(geofile_regex, fname)]\n",
    "        # flag for avoiding extracting zip when already extracted\n",
    "        # is_unzipped = all(f in ds_files for f in files_to_fetch) and len(ds_files) > 1\n",
    "        # TODO: handle .zip files that consist of a redundant copy of the entire dataset\n",
    "        if metadata['compression'] == 'zip' and any(f.endswith('.zip') for f in ds_files):\n",
    "            print(f\"Dataset metadata for {dataset_name} indicates handling of one or more downloaded zip files.\")\n",
    "            # check if the zip file was fetched and directly extract if it's the only file in the dataset\n",
    "            extracted_files = []\n",
    "            if len(ds_files) <= 2 and ds_files[0].endswith('.zip'):\n",
    "                zip_file = dst_p / ds_files[0]\n",
    "                # print(f\"Found single zip file for dataset: {zip_file}\")\n",
    "                # extract the zip file and delete it \n",
    "                with ZipFile(zip_file, 'r') as zip_ref:\n",
    "                    extracted_files = zip_ref.namelist()\n",
    "                    zip_ref.extractall(dst)\n",
    "                \n",
    "                # remove the zip file\n",
    "                # try:\n",
    "                #     os.remove(zip_file)\n",
    "                #     print(f\"Removed {os.path.relpath(zip_file)} after extraction\")\n",
    "                # except Exception as e:\n",
    "                #     print(f\"Error removing {zip_file}: {e}\")\n",
    "                # check if zip file consisted of a single dir and move contents up one level\n",
    "                top_level_dir = dst_p / extracted_files[0]\n",
    "                if top_level_dir.is_dir():\n",
    "                    # move only first level dirs and files to our dataset dir\n",
    "                    for item in top_level_dir.iterdir():\n",
    "                        if item.name.endswith('.zip'):\n",
    "                            continue\n",
    "                        # don't copy if already exists and is non-empty\n",
    "                        # TODO: add non-empty check\n",
    "                        elif os.path.exists(dst_p / item.name):\n",
    "                            print(f\"Skipping {item} as it already exists in {os.path.relpath(dst)}\")\n",
    "                            continue\n",
    "                        elif item.parent == top_level_dir:\n",
    "                            print(f\"Moving {item} to {os.path.relpath(dst)}\")\n",
    "                            shutil.move(item, dst)\n",
    "                    # remove the top level dir\n",
    "                    shutil.rmtree(top_level_dir)\n",
    "\n",
    "                ds_files = [os.path.join(root, fname) for root, dirs, files in os.walk(dst_p) for fname in files if re.match(geofile_regex, fname)]\n",
    "                print(f\"Moved items from {os.path.relpath(top_level_dir)} to:\\n{os.path.relpath(dst_p)}\")\n",
    "                print(f\"After extraction and moving, we have {len(ds_files)} files in {os.path.relpath(dst)}:\\n{ds_files}\")\n",
    "\n",
    "            elif len(ds_files) > 2:\n",
    "                # multiple files in addition to the zip file; handle on case by case basis\n",
    "                print(f\"Multiple files found in {dst_p}:\\n{os.listdir(dst_p)}\")\n",
    "        # no further processing needed; get file list directly from datahugger\n",
    "        else: \n",
    "            ds_files = [os.path.join(root, fname) for root, dirs, files in os.walk(dst_p) for fname in files if re.match(geofile_regex, fname)]\n",
    "\n",
    "\n",
    "        dataset_tree = {\n",
    "            'dataset': dataset_name,\n",
    "            'output_dir': ds_tree.output_folder,\n",
    "            'files': ds_files,\n",
    "            'fs_tree': seedir(dst_p, depthlimit=5, printout=False, regex=True, include_files=geofile_regex)\n",
    "        }\n",
    "\n",
    "    elif repo == 'github':\n",
    "        # Handle GitHub repositories using git partial cloning of repo \n",
    "        \n",
    "        # Create destination directory if it doesn't exist\n",
    "        os.makedirs(dst, exist_ok=True)\n",
    "        # Parse the GitHub URL\n",
    "        # [user, repo, tree, branch, rest of path]\n",
    "        parts = doi.replace('https://github.com/', '').split('/')\n",
    "        repo_path = f\"{parts[0]}/{parts[1]}\"\n",
    "        \n",
    "        # Extract branch and path\n",
    "        branch = 'main'  # Default branch\n",
    "        path = ''\n",
    "        \n",
    "        # check if local path exists and contains expected files\n",
    "        if os.path.exists(dst) and any(os.path.splitext(fname)[1] in ['.geojson', '.json', '.shp', '.zip'] for fname in os.listdir(dst)) and not force:  \n",
    "            print(f\"Destination path for {dataset_name}'s repo already exists and contains expected files.\")\n",
    "            # print in bold\n",
    "            print(f\"\\033[1mSkipping Download!\\033[0m\")\n",
    "            # fetch dataset dir info from Pathlib and tree from seedir \n",
    "            tree = seedir(dst_p, depthlimit=5, printout=False, regex=True, include_files=geofile_regex)\n",
    "            # get list of files in Path object that satisfy regex\n",
    "            ds_files = [os.path.join(root, fname) for root, dirs, files in os.walk(dst_p) for fname in files if re.match(geofile_regex, fname)]\n",
    "            dataset_tree = {\n",
    "                'dataset': dataset_name,\n",
    "                'output_dir': dst,\n",
    "                'files': ds_files,\n",
    "                'fs_tree': tree\n",
    "            }\n",
    "\n",
    "        # Check if it's a folder/repository or a single file\n",
    "        elif '/blob/' not in doi and 'raw.githubusercontent.com' not in doi:\n",
    "            try:\n",
    "                if 'tree' in parts:\n",
    "                    tree_index = parts.index('tree')\n",
    "                    branch = parts[tree_index + 1]\n",
    "                    path = '/'.join(parts[tree_index + 2:]) if len(parts) > tree_index + 2 else ''\n",
    "                \n",
    "                # Create a temporary directory for the sparse checkout\n",
    "                with tempfile.TemporaryDirectory() as temp_dir:\n",
    "                    # Initialize the git repository and set up sparse checkout\n",
    "                    commands = [f\"git clone --filter=blob:limit={max_mb}m --depth 1 https://github.com/{repo_path}.git {dataset_name}\"]\n",
    "                    # print(f\"Running commands: {commands}\")\n",
    "                    # Execute git commands\n",
    "                    for cmd in commands:\n",
    "                        \n",
    "                        process = subprocess.run(cmd, shell=True, cwd=temp_dir, \n",
    "                                               capture_output=True, text=True)\n",
    "                        # show command output (debug)\n",
    "                        print(f\"Command stdout: {process.stdout}\")\n",
    "                        if process.returncode != 0:\n",
    "                            raise Exception(f\"Git command failed: {cmd}\\n{process.stderr}\")\n",
    "                    \n",
    "                    # Copy only the files in the dir specified in DOI/URL\n",
    "                    repo_ds_dir = os.path.join(temp_dir, dataset_name, path) if path else os.path.join(temp_dir, dataset_name)\n",
    "                    files_list = []\n",
    "                    #\n",
    "                    for root, _, files in os.walk(repo_ds_dir):\n",
    "                        for file in files:\n",
    "                            if file.startswith('.git'):\n",
    "                                continue\n",
    "                            src_file = os.path.join(root, file)\n",
    "                            # Create relative path\n",
    "                            rel_path = os.path.relpath(src_file, repo_ds_dir)\n",
    "                            dst_file = os.path.join(dst, rel_path)\n",
    "                            \n",
    "                            # Create destination directory if needed\n",
    "                            os.makedirs(os.path.dirname(dst_file), exist_ok=True)\n",
    "                            \n",
    "                            # Copy the file\n",
    "                            shutil.copy2(src_file, dst_file)\n",
    "                            files_list.append(dst_file)\n",
    "                            print(f\"Copied {rel_path} to ./{dataset_dir}/{rel_path}\")\n",
    "\n",
    "                dataset_tree = {\n",
    "                    'dataset': dataset_name,\n",
    "                    'output_dir': dst,\n",
    "                    'files': files_list,\n",
    "                    'fs_tree': seedir(dst_p, depthlimit=5, printout=False, regex=True, include_files=geofile_regex)\n",
    "                }\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error performing git clone: {e}\")\n",
    "                return None\n",
    "        else:\n",
    "            # It's a single file (raw URL or blob URL)\n",
    "            try:\n",
    "                # Convert blob URL to raw URL if needed\n",
    "                if '/blob/' in doi:\n",
    "                    raw_url = doi.replace('github.com', 'raw.githubusercontent.com').replace('/blob/', '/')\n",
    "                else:\n",
    "                    raw_url = doi\n",
    "                \n",
    "                # Extract filename from URL\n",
    "                filename = os.path.basename(urllib.parse.urlparse(raw_url).path)\n",
    "                local_file_path = os.path.join(dst, filename)\n",
    "                \n",
    "                # Download the file\n",
    "                response = requests.get(raw_url, stream=True)\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                # Check file size\n",
    "                file_size = int(response.headers.get('content-length', 0))\n",
    "                if file_size > max_dl:\n",
    "                    print(f\"File size ({file_size} bytes) exceeds maximum allowed size ({max_dl * 1024 * 1024} MB)\")\n",
    "                    return None\n",
    "                \n",
    "                with open(local_file_path, 'wb') as f:\n",
    "                    for chunk in tqdm(response.iter_content(chunk_size=8192), desc=f\"Downloading {filename}\", unit='KB'):\n",
    "                        f.write(chunk)\n",
    "                print(f\"Downloaded {filename} to {os.path.relpath(local_file_path)}\")\n",
    "                dataset_tree = {\n",
    "                    'dataset': dataset_name,\n",
    "                    'output_dir': dst,\n",
    "                    'files': [local_file_path],\n",
    "                    'fs_tree': ds_tree.tree()\n",
    "                }\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error downloading GitHub file: {e}\")\n",
    "\n",
    "    elif repo == 'sciencebase':\n",
    "        # Initialize ScienceBase client\n",
    "        # sb = sciencebasepy.SbSession()\n",
    "        \n",
    "        # # Extract the item ID from the DOI or URL\n",
    "        # # DOIs like 10.5281/zenodo.8038684 or URLs with item ID\n",
    "        # item_id = doi.split('/')[-1] if '/' in doi else doi\n",
    "        \n",
    "        # try:\n",
    "        #     # Get item details\n",
    "        #     item = sb.get_item(item_id)\n",
    "            \n",
    "        #     # Create destination directory\n",
    "        #     os.makedirs(dst, exist_ok=True)\n",
    "            \n",
    "        #     # Download all files associated with the item\n",
    "        #     downloaded_files = []\n",
    "            \n",
    "        #     # Get item files\n",
    "        #     files = sb.get_item_file_info(item_id)\n",
    "            \n",
    "        #     for file_info in files:\n",
    "        #         file_name = file_info['name']\n",
    "        #         file_url = file_info['url']\n",
    "                \n",
    "        #         # Check file size if available\n",
    "        #         if 'size' in file_info and file_info['size'] > max_dl:\n",
    "        #             print(f\"Skipping file {file_name} as it exceeds the maximum download size\")\n",
    "        #             continue\n",
    "                \n",
    "        #         # Download the file\n",
    "        #         local_file_path = os.path.join(dst, file_name)\n",
    "        #         sb.download_file(file_url, local_file_path)\n",
    "                \n",
    "        #         downloaded_files.append(local_file_path)\n",
    "        #         print(f\"Downloaded {file_name} to {local_file_path}\")\n",
    "        print(\"Not Implemented yet\")\n",
    "        return None\n",
    "\n",
    "    print(f\"Fetched {len(dataset_tree['files'])} dataset files for {dataset_name} in {os.path.relpath(dataset_tree['output_dir'])}:\")\n",
    "    print(dataset_tree['fs_tree'])\n",
    "\n",
    "    return dataset_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15d311f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check datahugger get arguments\n",
    "# print(datahugger.get.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "05b2c58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_ds = selected_datasets[2]\n",
    "# test_doi = dataset_metadata[test_ds]['doi']\n",
    "# max_mb = 500\n",
    "# dst_dir = os.path.join(os.getcwd(), os.getenv('DATA_PATH'), 'raw', 'labels', test_ds)\n",
    "# t = datahugger.get(test_doi, dst_dir, print_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78eece7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching 3 datasets with files of max size 300 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ff3bda1c8b54a58a073531a26f4de19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='\\n<style>\\n.jupyter-widgets-output-area pre {\\n    white-space: pre-wrap !important;       /* CSS3…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f004535a8694522bd036a1f8a4eb87d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='\\n<style>\\n.widget-html {\\n    text-align: left !important;\\n}\\n.widget-checkbox {\\n    justify-co…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# iterate through the selected datasets and fetch files\n",
    "# iterate through the selected datasets and fetch files\n",
    "ds_trees = {}\n",
    "max_mb = int(os.getenv('MAX_LABEL_MB', 100))\n",
    "print(f\"Fetching {len(selected_datasets)} datasets with files of max size {max_mb} MB\")\n",
    "\n",
    "# Create widgets for controlling the fetching process\n",
    "fetch_output = widgets.Output(\n",
    "    layout=widgets.Layout(\n",
    "        width='80%', \n",
    "        border='1px solid #ddd', \n",
    "        padding='10px',\n",
    "        overflow='auto'\n",
    "    )\n",
    ")\n",
    "# Apply direct CSS styling for text wrapping (Note: unvalidated)\n",
    "display(widgets.HTML(\"\"\"\n",
    "<style>\n",
    ".jupyter-widgets-output-area pre {\n",
    "    white-space: pre-wrap !important;       /* CSS3 */\n",
    "    word-wrap: break-word !important;        /* Internet Explorer 5.5+ */\n",
    "    overflow-wrap: break-word !important;\n",
    "    max-width: 100%;\n",
    "}\n",
    "</style>\n",
    "\"\"\"))\n",
    "control_panel = widgets.VBox(layout=widgets.Layout(width='20%', padding='10px', overflow='auto', word_wrap='break-word'))\n",
    "fetch_button = widgets.Button(description=\"Fetch Next Dataset\", button_style=\"primary\")\n",
    "progress_label = widgets.HTML(\"Waiting to start...\")\n",
    "dataset_index = 0\n",
    "\n",
    "# Function to fetch the next dataset\n",
    "def fetch_next_dataset(button=None):\n",
    "    global dataset_index\n",
    "    global dataset_metadata\n",
    "    \n",
    "    if dataset_index >= len(selected_datasets):\n",
    "        with fetch_output:\n",
    "            print(\"All datasets have been fetched!\")\n",
    "            progress_label.value = f\"<b>Completed:</b> {dataset_index}/{len(selected_datasets)} datasets\"\n",
    "        fetch_button.disabled = True\n",
    "        return\n",
    "    \n",
    "    dataset = selected_datasets[dataset_index]\n",
    "    progress_label.value = f\"<b>Fetching:</b> {dataset_index+1}/{len(selected_datasets)}<br><b>Current:</b> {dataset}\"\n",
    "    \n",
    "    with fetch_output:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Fetching dataset files for {dataset} using DOI/URL:\\n {dataset_metadata[dataset]['doi']}\")\n",
    "        ds_tree = fetch_dataset_files(dataset, max_mb=max_mb, force=force_download_checkbox.value)\n",
    "        \n",
    "        if ds_tree:\n",
    "            ds_trees[dataset] = ds_tree\n",
    "            # update metadata dict with local filesystem info\n",
    "            dataset_metadata[dataset]['output_dir'] = ds_tree['output_dir']\n",
    "            dataset_metadata[dataset]['files'] = ds_tree['files']\n",
    "            dataset_metadata[dataset]['tree'] = ds_tree['fs_tree']\n",
    "            # print the dataset file tree\n",
    "        else:\n",
    "            print(f\"Failed to fetch dataset {dataset}\")\n",
    "    \n",
    "    dataset_index += 1\n",
    "    progress_label.value = f\"<b>Completed:</b> {dataset_index}/{len(selected_datasets)}<br><b>Next:</b> {selected_datasets[dataset_index] if dataset_index < len(selected_datasets) else 'Done'}\"\n",
    "\n",
    "# Add a checkbox for force download option\n",
    "force_download_checkbox = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='Force Download',\n",
    "    tooltip='If checked, download will be forced even if files exist locally',\n",
    "    layout=widgets.Layout(width='auto')\n",
    ")\n",
    "\n",
    "# Configure the button callback\n",
    "fetch_button.on_click(fetch_next_dataset)\n",
    "\n",
    "# Create the control panel\n",
    "dataset_progress = widgets.HTML(f\"Datasets selected: {len(selected_datasets)}\")\n",
    "fetch_status = widgets.HTML(\n",
    "    f\"Status: Ready to begin\",\n",
    "    layout=widgets.Layout(margin=\"10px 0\")\n",
    ")\n",
    "\n",
    "# Create the control panel with left alignment\n",
    "control_panel.children = [\n",
    "    widgets.HTML(\"<h3 style='align:left;'>Fetch Control</h3>\"), \n",
    "    dataset_progress,\n",
    "    force_download_checkbox,\n",
    "    widgets.HTML(\"<hr style='margin:10px 0'>\"),\n",
    "    progress_label,\n",
    "    fetch_button\n",
    "]\n",
    "\n",
    "# Add custom CSS to ensure alignment\n",
    "display(widgets.HTML(\"\"\"\n",
    "<style>\n",
    ".widget-html {\n",
    "    text-align: left !important;\n",
    "}\n",
    ".widget-checkbox {\n",
    "    justify-content: flex-start !important;\n",
    "}\n",
    ".widget-button {\n",
    "    width: 100% !important;\n",
    "}\n",
    "</style>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ddc7a4",
   "metadata": {},
   "source": [
    "#### Fetching selected datasets and visualizing metadata and file structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2e46bbab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca132c41b8c740df8e48b5c60101350a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Output(layout=Layout(border='1px solid #ddd', overflow='auto', padding='10px', width='80%')), V…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the widget layout\n",
    "display(widgets.HBox([fetch_output, control_panel]))\n",
    "\n",
    "# Set up for first fetch\n",
    "if selected_datasets:\n",
    "    progress_label.value = f\"<b>Ready to start:</b><br>First dataset: {selected_datasets[0]}\"\n",
    "else:\n",
    "    progress_label.value = \"<b>No datasets selected</b>\"\n",
    "    fetch_button.disabled = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea7659bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All selected datasets have been fetched with the following file tree:\n",
      "\n",
      "labels/\n",
      "+-global_pv_inventory_sent2_spot_2021/\n",
      "| +-predicted_set.geojson\n",
      "| +-cv_polygons.geojson\n",
      "| +-cv_tiles.geojson\n",
      "| +-test_tiles.geojson\n",
      "| +-test_polygons.geojson\n",
      "| +-trn_polygons.geojson\n",
      "| +-trn_tiles.geojson\n",
      "| +-global_pv_inventory_all.zip\n",
      "+-ind_pv_solar_farms_2022/\n",
      "| +-solar_farms_india_2021_merged_simplified.geojson\n",
      "+-usa_cali_usgs_pv_2016/\n",
      "  +-SolarArrayPolygons.geojson\n",
      "  +-.DS_Store\n",
      "  +-polygonVertices_LatitudeLongitude.csv\n",
      "  +-polygonVertices_PixelCoordinates.csv\n",
      "  +-polygonDataExceptVertices.csv\n",
      "  +-SolarArrayPolygons.json\n"
     ]
    }
   ],
   "source": [
    "# keep subset of metadata dict for selected datasets\n",
    "selected_metadata = {ds: dataset_metadata[ds] for ds in selected_datasets}\n",
    "get_ds_files = lambda ds: dataset_metadata[ds]['files']\n",
    "get_ds_dir = lambda ds: dataset_metadata[ds]['output_dir']\n",
    "fra_ds_folder = 'replication'\n",
    "# make a manual selection of the set of files we'll use from each dataset\n",
    "selected_ds_files = {\n",
    "    # 'global_pv_inventory_sent2_2024':\n",
    "        # [f for f in get_ds_files('global_pv_inventory_sent2_2024') if f.endswith('.json')],\n",
    "    'global_pv_inventory_sent2_spot_2021':\n",
    "        [f for f in get_ds_files('global_pv_inventory_sent2_spot_2021') if f.endswith('polygons.geojson') or f.endswith('set.geojson')],\n",
    "    # 'fra_west_eur_pv_installations_2023':\n",
    "    #     [os.path.join(root, fname) for root, _, files in os.walk(get_ds_dir('fra_west_eur_pv_installations_2023')) for fname in files ],\n",
    "    'ind_pv_solar_farms_2022': \n",
    "        [f for f in get_ds_files('ind_pv_solar_farms_2022') if f.endswith('.geojson')],\n",
    "    'usa_cali_usgs_pv_2016':\n",
    "        # grab all except the normal json\n",
    "        [f for f in get_ds_files('usa_cali_usgs_pv_2016') if not f.endswith('.json')]\n",
    "}\n",
    "\n",
    "# build and output tree for selected datasets\n",
    "selected_ds_dirs = [get_ds_dir(ds) for ds in selected_datasets]\n",
    "print(\"All selected datasets have been fetched with the following file tree:\\n\")\n",
    "selected_ds_tree = seedir(DATASET_DIR / 'raw' / 'labels', depthlimit=10, printout=True, regex=False, include_folders=selected_datasets, style='plus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "84f76f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cali_files = selected_ds_files['usa_cali_usgs_pv_2016']\n",
    "# cali_str = \"\\n\".join([f\"{i}: {os.path.relpath(f)}\" for i, f in enumerate(cali_files)])\n",
    "# print(f\"\\n\\nSelected dataset files:\\n{cali_str}\")\n",
    "# global_files = selected_ds_files['global_pv_inventory_sent2_spot_2021']\n",
    "# global_str = \"\\n\".join([f\"{i}: {os.path.relpath(f)}\" for i, f in enumerate(global_files)])\n",
    "# print(f\"\\n\\nSelected dataset files:\\n{global_str}\")\n",
    "# india_files = selected_ds_files['ind_pv_solar_farms_2022']\n",
    "# india_str = \"\\n\".join([f\"{i}: {os.path.relpath(f)}\" for i, f in enumerate(india_files)])\n",
    "# print(f\"\\n\\nSelected dataset files:\\n{india_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218e707e",
   "metadata": {},
   "source": [
    "#### Global inventory of solar PV units (Kruitwagen et al, 2021)\n",
    "\n",
    "From Zenodo:\n",
    "```\n",
    "Repository contents:\n",
    "\n",
    "trn_tiles.geojson: 18,570 rectangular areas-of-interest used for sampling training patch data.\n",
    "\n",
    "trn_polygons.geojson: 36,882 polygons obtained from OSM in 2017 used to label training patches.\n",
    "\n",
    "cv_tiles.geojson: 560 rectangular areas-of-interest used for sampling cross-validation data seeded from WRI GPPDB\n",
    "\n",
    "cv_polygons.geojson: 6,281 polygons corresponding to all PV solar generating units present in cv_tiles.geojson at the end of 2018.\n",
    "\n",
    "test_tiles.geojson: 122 rectangular regions-of-interest used for building the test set.\n",
    "\n",
    "test_polygons.geojson: 7,263 polygons corresponding to all utility-scale (>10kW) solar generating units present in test_tiles.geojson at the end of 2018.\n",
    "\n",
    "predicted_polygons.geojson: 68,661 polygons corresponding to predicted polygons in global deployment, capturing the status of deployed photovoltaic solar energy generating capacity at the end of 2018.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f160322d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional preprocessing specific to each dataset (mostly attaching any included metadata)\n",
    "def global_pv_inventory_spot_processing(gdf, dataset_name, output_dir, subset_bbox=None, geom_type='Polygon', rm_invalid=True):\n",
    "    all_cols = [\n",
    "        'unique_id', 'area', 'confidence', 'install_date', 'iso-3166-1', 'iso-3166-2', 'gti', 'pvout', 'capacity_mw', 'match_id', 'wdpa_10km', 'LC_CLC300_1992', 'LC_CLC300_1993',\n",
    "        'LC_CLC300_1994', 'LC_CLC300_1995', 'LC_CLC300_1996', 'LC_CLC300_1997', 'LC_CLC300_1998', 'LC_CLC300_1999', 'LC_CLC300_2000', 'LC_CLC300_2001', 'LC_CLC300_2002',\n",
    "        'LC_CLC300_2003', 'LC_CLC300_2004', 'LC_CLC300_2005', 'LC_CLC300_2006', 'LC_CLC300_2007', 'LC_CLC300_2008', 'LC_CLC300_2009', 'LC_CLC300_2010', 'LC_CLC300_2011',\n",
    "        'LC_CLC300_2012', 'LC_CLC300_2013', 'LC_CLC300_2014', 'LC_CLC300_2015', 'LC_CLC300_2016', 'LC_CLC300_2017', 'LC_CLC300_2018', 'mean_ai', 'GCR', 'eff', 'ILR',\n",
    "        'area_error', 'lc_mode', 'lc_arid', 'lc_vis', 'geometry', 'aoi_idx', 'aoi', 'id', 'Country', 'Province', 'Project', 'WRI_ref', 'Polygon Source', 'Date', 'building',\n",
    "        'operator', 'generator_source', 'amenity', 'landuse', 'power_source', 'shop', 'sport', 'tourism', 'way_area', 'access', 'construction', 'denomination', 'historic',\n",
    "        'leisure', 'man_made', 'natural', 'ref', 'religion', 'surface', 'z_order', 'layer', 'name', 'barrier', 'addr_housenumber', 'office', 'power', 'osm_id', 'military'\n",
    "    ]\n",
    "    # remove unwanted columns\n",
    "    keep_cols = ['geometry', 'unique_id', 'area', 'confidence', 'install_date', 'capacity_mw', 'iso-3166-2', 'pvout', 'osm_id', 'Project', 'construction']\n",
    "    print(f\"Filtering from {len(all_cols)} columns to {len(keep_cols)} columns:\\n{keep_cols}\")\n",
    "    gdf = gdf[keep_cols]\n",
    "    return gdf\n",
    "def global_pv_inventory_sent2_processing(gdf, dataset_name, output_dir, subset_bbox=None, geom_type='Polygon', rm_invalid=True):\n",
    "    return gdf\n",
    "def india_pv_solar_farms_processing(gdf, dataset_name, output_dir, subset_bbox=None, geom_type='Polygon', rm_invalid=True):\n",
    "    return gdf\n",
    "def usa_cali_usgs_pv_processing(gdf, dataset_name, output_dir, subset_bbox=None, geom_type='Polygon', rm_invalid=True):\n",
    "    return gdf\n",
    "def usa_eia_large_scale_pv_processing(gdf, dataset_name, output_dir, subset_bbox=None, geom_type='Polygon', rm_invalid=True):\n",
    "    return gdf\n",
    "def usa_eia_large_scale_pv_processing(gdf, dataset_name, output_dir, subset_bbox=None, geom_type='Polygon', rm_invalid=True):\n",
    "    return gdf\n",
    "\n",
    "def filter_duplicates(gdf, geom_type='Polygon', overlap_thresh=0.75):\n",
    "    \"\"\"\n",
    "    Remove duplicate geometries from a GeoDataFrame based on a specified overlap threshold.\n",
    "    \n",
    "    Args:\n",
    "        gdf (GeoDataFrame): Input GeoDataFrame.\n",
    "        geom_type (str): Geometry type to filter by. Default is 'Polygon'.\n",
    "        overlap_thresh (float): Overlap threshold for removing duplicates. Default is 0.75.\n",
    "        \n",
    "    Returns:\n",
    "        gdf (GeoDataFrame): GeoDataFrame with duplicates removed.\n",
    "    \"\"\"\n",
    "    # First identify exact duplicates\n",
    "    gdf = gdf.drop_duplicates('geometry')\n",
    "    \n",
    "    # Identify geometries that overlap substantially\n",
    "    overlaps = []\n",
    "    # Use spatial index for efficiency\n",
    "    spatial_index = gdf.sindex\n",
    "    \n",
    "    for idx, geom in enumerate(gdf.geometry):\n",
    "        # Find potential overlaps using the spatial index\n",
    "        possible_matches = list(spatial_index.intersection(geom.bounds))\n",
    "        # Remove self from matches\n",
    "        if idx in possible_matches:\n",
    "            possible_matches.remove(idx)\n",
    "        \n",
    "        for other_idx in possible_matches:\n",
    "            other_geom = gdf.iloc[other_idx].geometry\n",
    "            if geom.intersects(other_geom):\n",
    "                # Calculate overlap percentage (relative to the smaller polygon)\n",
    "                intersection_area = geom.intersection(other_geom).area\n",
    "                min_area = min(geom.area, other_geom.area)\n",
    "                overlap_percentage = intersection_area / min_area\n",
    "                \n",
    "                # If overlap is significant (e.g., >75%)\n",
    "                if overlap_percentage > overlap_thresh:\n",
    "                    # Keep the geometry with the larger area\n",
    "                    if geom.area < other_geom.area:\n",
    "                        overlaps.append(idx)\n",
    "                    \n",
    "                    else:\n",
    "                        overlaps.append(other_idx)\n",
    "                        break\n",
    "    \n",
    "    # Remove overlapping geometries\n",
    "    if overlaps:\n",
    "        print(f\"Removing {len(overlaps)} geometries with >{overlap_thresh*100}% overlap\")\n",
    "        gdf = gdf.drop(gdf.index[overlaps]).reset_index(drop=True)\n",
    "    \n",
    "    return gdf\n",
    "\n",
    "# basic processing for geojson, shapefiles, and already georeferenced data\n",
    "def process_geojson(geojson_files, dataset_name, output_dir=None, subset_bbox=None, geom_type='Polygon', rm_invalid=True, overlap_thresh=0.75, out_fmt='geoparquet'):\n",
    "    \"\"\"\n",
    "    Process a GeoJSON file and return a GeoDataFrame.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the GeoJSON file.\n",
    "        dataset_name (str): Name of the dataset.\n",
    "        geom_type (str): Geometry type to filter by. Default is 'Polygon'.\n",
    "        \n",
    "    Returns:\n",
    "        gdf (GeoDataFrame): Processed GeoDataFrame.\n",
    "    \"\"\"\n",
    "    if output_dir and not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    ds_dataframes = []\n",
    "\n",
    "    for fname in geojson_files:\n",
    "        if fname.endswith('.geojson') or fname.endswith('.json'):\n",
    "            # Check if the file is a valid GeoJSON\n",
    "            try:\n",
    "                gdf = gpd.read_file(fname)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {os.path.relpath(fname)}: {e}\")\n",
    "                continue\n",
    "            ds_dataframes.append(gdf)\n",
    "    \n",
    "    if len(ds_dataframes) == 0:\n",
    "        print(f\"No valid GeoJSON files found in {dataset_name}.\")\n",
    "        print(f\"Skipping dataset {dataset_name}\")\n",
    "        return None\n",
    "        \n",
    "    # Concatenate all dataframes into a single GeoDataFrame\n",
    "    gdf = gpd.GeoDataFrame(pd.concat(ds_dataframes, ignore_index=True))\n",
    "    # make sure the geometry column is included and named correctly\n",
    "    if 'geometry' not in gdf.columns:\n",
    "        gdf['geometry'] = gdf.geometry\n",
    "\n",
    "    # Basic info about the dataset\n",
    "    print(f\"Loaded geodataframe with raw counts of {len(gdf)} PV installations\")\n",
    "    print(f\"Coordinate reference system: {gdf.crs}\")\n",
    "    print(f\"Available columns: {gdf.columns.tolist()}\")\n",
    "    \n",
    "    # Add dataset name as a new column\n",
    "    gdf['dataset'] = dataset_name\n",
    "    \n",
    "    # Convert to WGS84 if not already in that CRS\n",
    "    if gdf.crs is not None and gdf.crs.to_string() != 'EPSG:4326':\n",
    "        # convert to WGS84 in cases of other crs (eg NAD83 for Cali dataset)\n",
    "        gdf = gdf.to_crs(epsg=4326)\n",
    "    if subset_bbox is not None:\n",
    "        # Filter the GeoDataFrame by the georeferenced bounding box\n",
    "        gdf = gdf.cx[subset_bbox[0]:subset_bbox[2], subset_bbox[1]:subset_bbox[3]]\n",
    "    \n",
    "    # DQ and cleaning\n",
    "    # check for missing and invalid geometries\n",
    "    invalid_geoms = gdf[gdf.geometry.is_empty | ~gdf.geometry.is_valid]\n",
    "    if len(invalid_geoms) > 0 and rm_invalid:\n",
    "        print(f\"Warning: {len(invalid_geoms)} invalid or empty geometries found and will be removed.\")\n",
    "        # Optionally remove invalid geometries\n",
    "        gdf = gdf[~gdf.geometry.is_empty & gdf.geometry.is_valid].reset_index(drop=True)\n",
    "    # Eliminating duplicates and geometries that overlap too much\n",
    "    if geom_type == 'Polygon':\n",
    "        gdf = filter_duplicates(gdf, geom_type=geom_type, overlap_thresh=overlap_thresh)\n",
    "\n",
    "    # perform any additional processing specific to the dataset for metadata and other attributes\n",
    "    if dataset_name == 'global_pv_inventory_sent2_2024':\n",
    "        print(\"Processing global_pv_inventory_sent2_2024 metadata\")\n",
    "        gdf = global_pv_inventory_sent2_processing(gdf, dataset_name, output_dir, subset_bbox=subset_bbox, geom_type=geom_type)\n",
    "    elif dataset_name == 'global_pv_inventory_sent2_spot_2021':\n",
    "        print(\"Processing global_pv_inventory_sent2_spot_2021 metadata\")\n",
    "        gdf = global_pv_inventory_spot_processing(gdf, dataset_name, output_dir, subset_bbox=subset_bbox, geom_type=geom_type)\n",
    "    elif dataset_name == 'ind_pv_solar_farms_2022':\n",
    "        print(\"Processing ind_pv_solar_farms_2022 metadata\")\n",
    "        gdf = india_pv_solar_farms_processing(gdf, dataset_name, output_dir, subset_bbox=subset_bbox, geom_type=geom_type)\n",
    "    elif dataset_name == 'usa_cali_usgs_pv_2016':\n",
    "        print(\"Processing usa_cali_usgs_pv_2016 metadata\")\n",
    "        gdf = usa_cali_usgs_pv_processing(gdf, dataset_name, output_dir, subset_bbox=subset_bbox, geom_type=geom_type)\n",
    "    elif dataset_name == 'usa_eia_large_scale_pv_2023':\n",
    "        print(\"Processing usa_eia_large_scale_pv_2023 metadata\")\n",
    "        gdf = usa_eia_large_scale_pv_processing(gdf, dataset_name, output_dir, subset_bbox=subset_bbox, geom_type=geom_type)\n",
    "    \n",
    "    # add some basic geometry info\n",
    "    # if not gdf.crs.is_geographic:\n",
    "    #     gdf['area_m2'] = gdf.geometry.area\n",
    "    # else:\n",
    "    #     # todo: check if other crs is more appropriate\n",
    "    gdf_proj = gdf['geometry'].to_crs(epsg=4326)\n",
    "    gdf['area_m2'] = gdf_proj.geometry.area\n",
    "    \n",
    "    # gdf['centroid_lon'] = gdf.geometry.centroid.x\n",
    "    gdf['centroid_lon'] = gdf_proj.geometry.centroid.x\n",
    "    # gdf['centroid_lat'] = gdf.geometry.centroid.y\n",
    "    gdf['centroid_lat'] = gdf_proj.geometry.centroid.y\n",
    "    # use gpd conversion argument\n",
    "    # gdf['bbox'] = gdf.geometry.apply(lambda geom: geom.bounds) \n",
    "\n",
    "    print(f\"After filtering and cleaning, we have {len(gdf)} PV installations\")\n",
    "    print(f\"Coordinate reference system: {gdf.crs}\")\n",
    "    print(f\"Available columns: {gdf.columns.tolist()}\")\n",
    "\n",
    "    if output_dir:\n",
    "        out_path = os.path.join(output_dir, f\"{dataset_name}_processed.{out_fmt}\")\n",
    "\n",
    "        if out_fmt == 'geoparquet':\n",
    "            gdf.to_parquet(out_path, \n",
    "                index=None, \n",
    "                compression='snappy',\n",
    "                geometry_encoding='WKB', \n",
    "                write_covering_bbox=True,\n",
    "                schema_version='1.1.0')\n",
    "        else:\n",
    "            gdf.to_file(out_path, driver='GeoJSON', index=None)\n",
    "        print(f\"Saved processed GeoDataFrame to {os.path.relpath(out_path)}\")\n",
    "    \n",
    "    return gdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11057816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset usa_cali_usgs_pv_2016 with 4 files in datasets/raw/labels/usa_cali_usgs_pv_2016\n",
      "Loaded geodataframe with raw counts of 19433 PV installations\n",
      "Coordinate reference system: EPSG:4326\n",
      "Available columns: ['polygon_id', 'centroid_latitude', 'centroid_longitude', 'centroid_latitude_pixels', 'centroid_longitude_pixels', 'city', 'area_pixels', 'area_meters', 'image_name', 'nw_corner_of_image_latitude', 'nw_corner_of_image_longitude', 'se_corner_of_image_latitude', 'se_corner_of_image_longitude', 'datum', 'projection_zone', 'resolution', 'jaccard_index', 'polygon_vertices_pixels', 'geometry']\n",
      "Warning: 60 invalid or empty geometries found and will be removed.\n",
      "Processing usa_cali_usgs_pv_2016 metadata\n",
      "After filtering and cleaning, we have 19373 PV installations\n",
      "Coordinate reference system: EPSG:4326\n",
      "Available columns: ['polygon_id', 'centroid_latitude', 'centroid_longitude', 'centroid_latitude_pixels', 'centroid_longitude_pixels', 'city', 'area_pixels', 'area_meters', 'image_name', 'nw_corner_of_image_latitude', 'nw_corner_of_image_longitude', 'se_corner_of_image_latitude', 'se_corner_of_image_longitude', 'datum', 'projection_zone', 'resolution', 'jaccard_index', 'polygon_vertices_pixels', 'geometry', 'dataset', 'area_m2', 'centroid_lon', 'centroid_lat']\n",
      "Saved processed GeoDataFrame to datasets/raw/labels/geoparquet/usa_cali_usgs_pv_2016_processed.geoparquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alejandovega/miniconda3/envs/eo-pv-cv/lib/python3.10/site-packages/shapely/predicates.py:798: RuntimeWarning: invalid value encountered in intersects\n",
      "  return lib.intersects(a, b, **kwargs)\n",
      "/Users/alejandovega/miniconda3/envs/eo-pv-cv/lib/python3.10/site-packages/shapely/set_operations.py:133: RuntimeWarning: invalid value encountered in intersection\n",
      "  return lib.intersection(a, b, **kwargs)\n",
      "/var/folders/81/h1xdmxqn6l3_p3v3sw_s9cbw0000gp/T/ipykernel_32865/4059283130.py:168: UserWarning: Geometry is in a geographic CRS. Results from 'area' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  gdf['area_m2'] = gdf_proj.geometry.area\n",
      "/var/folders/81/h1xdmxqn6l3_p3v3sw_s9cbw0000gp/T/ipykernel_32865/4059283130.py:171: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  gdf['centroid_lon'] = gdf_proj.geometry.centroid.x\n",
      "/var/folders/81/h1xdmxqn6l3_p3v3sw_s9cbw0000gp/T/ipykernel_32865/4059283130.py:173: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  gdf['centroid_lat'] = gdf_proj.geometry.centroid.y\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "polygon_id",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "centroid_latitude",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "centroid_longitude",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "centroid_latitude_pixels",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "centroid_longitude_pixels",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "area_pixels",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "area_meters",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "nw_corner_of_image_latitude",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "nw_corner_of_image_longitude",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "se_corner_of_image_latitude",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "se_corner_of_image_longitude",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "resolution",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "jaccard_index",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "area_m2",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "centroid_lon",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "centroid_lat",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "89fb5c7c-675b-4c52-b147-c4cb5ce013d0",
       "rows": [
        [
         "count",
         "19373.0",
         "19373.0",
         "19373.0",
         "19373.0",
         "19373.0",
         "19373.0",
         "19373.0",
         "19373.0",
         "19373.0",
         "19373.0",
         "19373.0",
         "19373.0",
         "19373.0",
         "19373.0",
         "19373.0",
         "19373.0"
        ],
        [
         "mean",
         "9809.361276002684",
         "36.78491174245795",
         "-119.94715905742885",
         "2385.1559934045604",
         "2588.6545887670222",
         "445.2396675907092",
         "40.07157008271498",
         "36.79122463958705",
         "-119.95602472286171",
         "36.778193260587415",
         "-119.93860571174316",
         "0.29999999999999993",
         "0.6056667921859357",
         "4.0565190133096265e-09",
         "-119.94715906209623",
         "36.78491174461461"
        ],
        [
         "std",
         "5697.7599311517915",
         "0.8744500897635621",
         "0.5974233110956428",
         "1483.504467882348",
         "1472.8196781631798",
         "1377.8651305746505",
         "124.00786174758827",
         "0.8748671324226521",
         "0.597034090207862",
         "0.8741699594423453",
         "0.5975058523741837",
         "1.0652864865849169e-13",
         "0.38808582443998946",
         "1.2592953219155805e-08",
         "0.5974233165998064",
         "0.8744500916107775"
        ],
        [
         "min",
         "1.0",
         "34.14519004660754",
         "-121.38087510351755",
         "2.667674716",
         "2.878649346",
         "4.229101454",
         "0.3806191309",
         "34.14931944",
         "-121.3823056",
         "34.13853056",
         "-121.3655222",
         "0.3",
         "0.0",
         "3.8861391660953095e-11",
         "-121.38087510351691",
         "34.145190046609244"
        ],
        [
         "25%",
         "4889.0",
         "36.78757152535533",
         "-119.85683239746987",
         "1065.071283",
         "1355.111998",
         "131.2497145",
         "11.8124743",
         "36.79326667",
         "-119.8649056",
         "36.78015",
         "-119.8476056",
         "0.3",
         "0.0",
         "1.1948951022614048e-09",
         "-119.85683239747127",
         "36.787571525355546"
        ],
        [
         "50%",
         "9744.0",
         "36.83236247837969",
         "-119.75470659182292",
         "2261.083358",
         "2592.542739",
         "223.5700446",
         "20.12130401",
         "36.83571111",
         "-119.7650722",
         "36.82258056",
         "-119.7477778",
         "0.3",
         "0.8220041131",
         "2.0347966130375144e-09",
         "-119.75470659182398",
         "36.83236247837903"
        ],
        [
         "75%",
         "14655.0",
         "36.868902428944054",
         "-119.68566801822907",
         "3732.191838",
         "3806.929754",
         "379.3418395",
         "34.14076555",
         "36.87623056",
         "-119.6940444",
         "36.86310278",
         "-119.6767889",
         "0.3",
         "0.8883719011",
         "3.454262672214184e-09",
         "-119.68566801823019",
         "36.86890242894346"
        ],
        [
         "max",
         "19863.0",
         "38.06811821129079",
         "-119.13375593803408",
         "4997.249972",
         "5987.691893",
         "68384.5702",
         "6154.611318",
         "38.07042222",
         "-119.1510722",
         "38.05666111",
         "-119.1310889",
         "0.3",
         "1.0",
         "6.319997669089085e-07",
         "-119.13375593803445",
         "38.06811821129076"
        ]
       ],
       "shape": {
        "columns": 16,
        "rows": 8
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polygon_id</th>\n",
       "      <th>centroid_latitude</th>\n",
       "      <th>centroid_longitude</th>\n",
       "      <th>centroid_latitude_pixels</th>\n",
       "      <th>centroid_longitude_pixels</th>\n",
       "      <th>area_pixels</th>\n",
       "      <th>area_meters</th>\n",
       "      <th>nw_corner_of_image_latitude</th>\n",
       "      <th>nw_corner_of_image_longitude</th>\n",
       "      <th>se_corner_of_image_latitude</th>\n",
       "      <th>se_corner_of_image_longitude</th>\n",
       "      <th>resolution</th>\n",
       "      <th>jaccard_index</th>\n",
       "      <th>area_m2</th>\n",
       "      <th>centroid_lon</th>\n",
       "      <th>centroid_lat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>19373.000000</td>\n",
       "      <td>19373.000000</td>\n",
       "      <td>19373.000000</td>\n",
       "      <td>19373.000000</td>\n",
       "      <td>19373.000000</td>\n",
       "      <td>19373.000000</td>\n",
       "      <td>19373.000000</td>\n",
       "      <td>19373.000000</td>\n",
       "      <td>19373.000000</td>\n",
       "      <td>19373.000000</td>\n",
       "      <td>19373.000000</td>\n",
       "      <td>1.937300e+04</td>\n",
       "      <td>19373.000000</td>\n",
       "      <td>1.937300e+04</td>\n",
       "      <td>19373.000000</td>\n",
       "      <td>19373.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>9809.361276</td>\n",
       "      <td>36.784912</td>\n",
       "      <td>-119.947159</td>\n",
       "      <td>2385.155993</td>\n",
       "      <td>2588.654589</td>\n",
       "      <td>445.239668</td>\n",
       "      <td>40.071570</td>\n",
       "      <td>36.791225</td>\n",
       "      <td>-119.956025</td>\n",
       "      <td>36.778193</td>\n",
       "      <td>-119.938606</td>\n",
       "      <td>3.000000e-01</td>\n",
       "      <td>0.605667</td>\n",
       "      <td>4.056519e-09</td>\n",
       "      <td>-119.947159</td>\n",
       "      <td>36.784912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5697.759931</td>\n",
       "      <td>0.874450</td>\n",
       "      <td>0.597423</td>\n",
       "      <td>1483.504468</td>\n",
       "      <td>1472.819678</td>\n",
       "      <td>1377.865131</td>\n",
       "      <td>124.007862</td>\n",
       "      <td>0.874867</td>\n",
       "      <td>0.597034</td>\n",
       "      <td>0.874170</td>\n",
       "      <td>0.597506</td>\n",
       "      <td>1.065286e-13</td>\n",
       "      <td>0.388086</td>\n",
       "      <td>1.259295e-08</td>\n",
       "      <td>0.597423</td>\n",
       "      <td>0.874450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>34.145190</td>\n",
       "      <td>-121.380875</td>\n",
       "      <td>2.667675</td>\n",
       "      <td>2.878649</td>\n",
       "      <td>4.229101</td>\n",
       "      <td>0.380619</td>\n",
       "      <td>34.149319</td>\n",
       "      <td>-121.382306</td>\n",
       "      <td>34.138531</td>\n",
       "      <td>-121.365522</td>\n",
       "      <td>3.000000e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.886139e-11</td>\n",
       "      <td>-121.380875</td>\n",
       "      <td>34.145190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4889.000000</td>\n",
       "      <td>36.787572</td>\n",
       "      <td>-119.856832</td>\n",
       "      <td>1065.071283</td>\n",
       "      <td>1355.111998</td>\n",
       "      <td>131.249715</td>\n",
       "      <td>11.812474</td>\n",
       "      <td>36.793267</td>\n",
       "      <td>-119.864906</td>\n",
       "      <td>36.780150</td>\n",
       "      <td>-119.847606</td>\n",
       "      <td>3.000000e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.194895e-09</td>\n",
       "      <td>-119.856832</td>\n",
       "      <td>36.787572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9744.000000</td>\n",
       "      <td>36.832362</td>\n",
       "      <td>-119.754707</td>\n",
       "      <td>2261.083358</td>\n",
       "      <td>2592.542739</td>\n",
       "      <td>223.570045</td>\n",
       "      <td>20.121304</td>\n",
       "      <td>36.835711</td>\n",
       "      <td>-119.765072</td>\n",
       "      <td>36.822581</td>\n",
       "      <td>-119.747778</td>\n",
       "      <td>3.000000e-01</td>\n",
       "      <td>0.822004</td>\n",
       "      <td>2.034797e-09</td>\n",
       "      <td>-119.754707</td>\n",
       "      <td>36.832362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>14655.000000</td>\n",
       "      <td>36.868902</td>\n",
       "      <td>-119.685668</td>\n",
       "      <td>3732.191838</td>\n",
       "      <td>3806.929754</td>\n",
       "      <td>379.341839</td>\n",
       "      <td>34.140766</td>\n",
       "      <td>36.876231</td>\n",
       "      <td>-119.694044</td>\n",
       "      <td>36.863103</td>\n",
       "      <td>-119.676789</td>\n",
       "      <td>3.000000e-01</td>\n",
       "      <td>0.888372</td>\n",
       "      <td>3.454263e-09</td>\n",
       "      <td>-119.685668</td>\n",
       "      <td>36.868902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>19863.000000</td>\n",
       "      <td>38.068118</td>\n",
       "      <td>-119.133756</td>\n",
       "      <td>4997.249972</td>\n",
       "      <td>5987.691893</td>\n",
       "      <td>68384.570200</td>\n",
       "      <td>6154.611318</td>\n",
       "      <td>38.070422</td>\n",
       "      <td>-119.151072</td>\n",
       "      <td>38.056661</td>\n",
       "      <td>-119.131089</td>\n",
       "      <td>3.000000e-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.319998e-07</td>\n",
       "      <td>-119.133756</td>\n",
       "      <td>38.068118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         polygon_id  centroid_latitude  centroid_longitude  \\\n",
       "count  19373.000000       19373.000000        19373.000000   \n",
       "mean    9809.361276          36.784912         -119.947159   \n",
       "std     5697.759931           0.874450            0.597423   \n",
       "min        1.000000          34.145190         -121.380875   \n",
       "25%     4889.000000          36.787572         -119.856832   \n",
       "50%     9744.000000          36.832362         -119.754707   \n",
       "75%    14655.000000          36.868902         -119.685668   \n",
       "max    19863.000000          38.068118         -119.133756   \n",
       "\n",
       "       centroid_latitude_pixels  centroid_longitude_pixels   area_pixels  \\\n",
       "count              19373.000000               19373.000000  19373.000000   \n",
       "mean                2385.155993                2588.654589    445.239668   \n",
       "std                 1483.504468                1472.819678   1377.865131   \n",
       "min                    2.667675                   2.878649      4.229101   \n",
       "25%                 1065.071283                1355.111998    131.249715   \n",
       "50%                 2261.083358                2592.542739    223.570045   \n",
       "75%                 3732.191838                3806.929754    379.341839   \n",
       "max                 4997.249972                5987.691893  68384.570200   \n",
       "\n",
       "        area_meters  nw_corner_of_image_latitude  \\\n",
       "count  19373.000000                 19373.000000   \n",
       "mean      40.071570                    36.791225   \n",
       "std      124.007862                     0.874867   \n",
       "min        0.380619                    34.149319   \n",
       "25%       11.812474                    36.793267   \n",
       "50%       20.121304                    36.835711   \n",
       "75%       34.140766                    36.876231   \n",
       "max     6154.611318                    38.070422   \n",
       "\n",
       "       nw_corner_of_image_longitude  se_corner_of_image_latitude  \\\n",
       "count                  19373.000000                 19373.000000   \n",
       "mean                    -119.956025                    36.778193   \n",
       "std                        0.597034                     0.874170   \n",
       "min                     -121.382306                    34.138531   \n",
       "25%                     -119.864906                    36.780150   \n",
       "50%                     -119.765072                    36.822581   \n",
       "75%                     -119.694044                    36.863103   \n",
       "max                     -119.151072                    38.056661   \n",
       "\n",
       "       se_corner_of_image_longitude    resolution  jaccard_index  \\\n",
       "count                  19373.000000  1.937300e+04   19373.000000   \n",
       "mean                    -119.938606  3.000000e-01       0.605667   \n",
       "std                        0.597506  1.065286e-13       0.388086   \n",
       "min                     -121.365522  3.000000e-01       0.000000   \n",
       "25%                     -119.847606  3.000000e-01       0.000000   \n",
       "50%                     -119.747778  3.000000e-01       0.822004   \n",
       "75%                     -119.676789  3.000000e-01       0.888372   \n",
       "max                     -119.131089  3.000000e-01       1.000000   \n",
       "\n",
       "            area_m2  centroid_lon  centroid_lat  \n",
       "count  1.937300e+04  19373.000000  19373.000000  \n",
       "mean   4.056519e-09   -119.947159     36.784912  \n",
       "std    1.259295e-08      0.597423      0.874450  \n",
       "min    3.886139e-11   -121.380875     34.145190  \n",
       "25%    1.194895e-09   -119.856832     36.787572  \n",
       "50%    2.034797e-09   -119.754707     36.832362  \n",
       "75%    3.454263e-09   -119.685668     36.868902  \n",
       "max    6.319998e-07   -119.133756     38.068118  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polygon_id</th>\n",
       "      <th>centroid_latitude</th>\n",
       "      <th>centroid_longitude</th>\n",
       "      <th>centroid_latitude_pixels</th>\n",
       "      <th>centroid_longitude_pixels</th>\n",
       "      <th>city</th>\n",
       "      <th>area_pixels</th>\n",
       "      <th>area_meters</th>\n",
       "      <th>image_name</th>\n",
       "      <th>nw_corner_of_image_latitude</th>\n",
       "      <th>...</th>\n",
       "      <th>datum</th>\n",
       "      <th>projection_zone</th>\n",
       "      <th>resolution</th>\n",
       "      <th>jaccard_index</th>\n",
       "      <th>polygon_vertices_pixels</th>\n",
       "      <th>geometry</th>\n",
       "      <th>dataset</th>\n",
       "      <th>area_m2</th>\n",
       "      <th>centroid_lon</th>\n",
       "      <th>centroid_lat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>36.926310</td>\n",
       "      <td>-119.840555</td>\n",
       "      <td>107.618458</td>\n",
       "      <td>3286.151487</td>\n",
       "      <td>Fresno</td>\n",
       "      <td>1513.254134</td>\n",
       "      <td>136.192872</td>\n",
       "      <td>11ska460890</td>\n",
       "      <td>36.926336</td>\n",
       "      <td>...</td>\n",
       "      <td>NAD83</td>\n",
       "      <td>11</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.914020</td>\n",
       "      <td>[ [ 3360.4950690000001, 131.63116400000001 ], ...</td>\n",
       "      <td>POLYGON ((-119.8403 36.92625, -119.84068 36.92...</td>\n",
       "      <td>usa_cali_usgs_pv_2016</td>\n",
       "      <td>1.376423e-08</td>\n",
       "      <td>-119.840555</td>\n",
       "      <td>36.926310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>36.926477</td>\n",
       "      <td>-119.840561</td>\n",
       "      <td>45.977659</td>\n",
       "      <td>3286.352946</td>\n",
       "      <td>Fresno</td>\n",
       "      <td>1727.907934</td>\n",
       "      <td>155.511714</td>\n",
       "      <td>11ska460890</td>\n",
       "      <td>36.926336</td>\n",
       "      <td>...</td>\n",
       "      <td>NAD83</td>\n",
       "      <td>11</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.829071</td>\n",
       "      <td>[ [ 3361.1538460000002, 69.615385000000003 ], ...</td>\n",
       "      <td>POLYGON ((-119.84031 36.92642, -119.8408 36.92...</td>\n",
       "      <td>usa_cali_usgs_pv_2016</td>\n",
       "      <td>1.571668e-08</td>\n",
       "      <td>-119.840561</td>\n",
       "      <td>36.926477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>36.926542</td>\n",
       "      <td>-119.840506</td>\n",
       "      <td>22.280851</td>\n",
       "      <td>3303.465657</td>\n",
       "      <td>Fresno</td>\n",
       "      <td>1242.184349</td>\n",
       "      <td>111.796591</td>\n",
       "      <td>11ska460890</td>\n",
       "      <td>36.926336</td>\n",
       "      <td>...</td>\n",
       "      <td>NAD83</td>\n",
       "      <td>11</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.937961</td>\n",
       "      <td>[ [ 3358.0157260000001, 48.136862999999998 ], ...</td>\n",
       "      <td>POLYGON ((-119.84032 36.92648, -119.84032 36.9...</td>\n",
       "      <td>usa_cali_usgs_pv_2016</td>\n",
       "      <td>1.129864e-08</td>\n",
       "      <td>-119.840506</td>\n",
       "      <td>36.926542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>36.921008</td>\n",
       "      <td>-119.842847</td>\n",
       "      <td>2048.362567</td>\n",
       "      <td>2547.366116</td>\n",
       "      <td>Fresno</td>\n",
       "      <td>688.933420</td>\n",
       "      <td>62.004008</td>\n",
       "      <td>11ska460890</td>\n",
       "      <td>36.926336</td>\n",
       "      <td>...</td>\n",
       "      <td>NAD83</td>\n",
       "      <td>11</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.842634</td>\n",
       "      <td>[ [ 2571.5917159999999, 2068.0493099999999 ], ...</td>\n",
       "      <td>POLYGON ((-119.84276 36.92096, -119.84277 36.9...</td>\n",
       "      <td>usa_cali_usgs_pv_2016</td>\n",
       "      <td>6.266390e-09</td>\n",
       "      <td>-119.842847</td>\n",
       "      <td>36.921008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>36.920976</td>\n",
       "      <td>-119.842906</td>\n",
       "      <td>2060.014890</td>\n",
       "      <td>2529.504997</td>\n",
       "      <td>Fresno</td>\n",
       "      <td>1060.890554</td>\n",
       "      <td>95.480150</td>\n",
       "      <td>11ska460890</td>\n",
       "      <td>36.926336</td>\n",
       "      <td>...</td>\n",
       "      <td>NAD83</td>\n",
       "      <td>11</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.890998</td>\n",
       "      <td>[ [ 2563.7810650000001, 2091.3984220000002 ], ...</td>\n",
       "      <td>POLYGON ((-119.84279 36.92089, -119.84299 36.9...</td>\n",
       "      <td>usa_cali_usgs_pv_2016</td>\n",
       "      <td>9.649632e-09</td>\n",
       "      <td>-119.842906</td>\n",
       "      <td>36.920976</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   polygon_id  centroid_latitude  centroid_longitude  \\\n",
       "0           1          36.926310         -119.840555   \n",
       "1           2          36.926477         -119.840561   \n",
       "2           3          36.926542         -119.840506   \n",
       "3           4          36.921008         -119.842847   \n",
       "4           5          36.920976         -119.842906   \n",
       "\n",
       "   centroid_latitude_pixels  centroid_longitude_pixels    city  area_pixels  \\\n",
       "0                107.618458                3286.151487  Fresno  1513.254134   \n",
       "1                 45.977659                3286.352946  Fresno  1727.907934   \n",
       "2                 22.280851                3303.465657  Fresno  1242.184349   \n",
       "3               2048.362567                2547.366116  Fresno   688.933420   \n",
       "4               2060.014890                2529.504997  Fresno  1060.890554   \n",
       "\n",
       "   area_meters   image_name  nw_corner_of_image_latitude  ...  datum  \\\n",
       "0   136.192872  11ska460890                    36.926336  ...  NAD83   \n",
       "1   155.511714  11ska460890                    36.926336  ...  NAD83   \n",
       "2   111.796591  11ska460890                    36.926336  ...  NAD83   \n",
       "3    62.004008  11ska460890                    36.926336  ...  NAD83   \n",
       "4    95.480150  11ska460890                    36.926336  ...  NAD83   \n",
       "\n",
       "   projection_zone  resolution jaccard_index  \\\n",
       "0               11         0.3      0.914020   \n",
       "1               11         0.3      0.829071   \n",
       "2               11         0.3      0.937961   \n",
       "3               11         0.3      0.842634   \n",
       "4               11         0.3      0.890998   \n",
       "\n",
       "                             polygon_vertices_pixels  \\\n",
       "0  [ [ 3360.4950690000001, 131.63116400000001 ], ...   \n",
       "1  [ [ 3361.1538460000002, 69.615385000000003 ], ...   \n",
       "2  [ [ 3358.0157260000001, 48.136862999999998 ], ...   \n",
       "3  [ [ 2571.5917159999999, 2068.0493099999999 ], ...   \n",
       "4  [ [ 2563.7810650000001, 2091.3984220000002 ], ...   \n",
       "\n",
       "                                            geometry                dataset  \\\n",
       "0  POLYGON ((-119.8403 36.92625, -119.84068 36.92...  usa_cali_usgs_pv_2016   \n",
       "1  POLYGON ((-119.84031 36.92642, -119.8408 36.92...  usa_cali_usgs_pv_2016   \n",
       "2  POLYGON ((-119.84032 36.92648, -119.84032 36.9...  usa_cali_usgs_pv_2016   \n",
       "3  POLYGON ((-119.84276 36.92096, -119.84277 36.9...  usa_cali_usgs_pv_2016   \n",
       "4  POLYGON ((-119.84279 36.92089, -119.84299 36.9...  usa_cali_usgs_pv_2016   \n",
       "\n",
       "        area_m2 centroid_lon centroid_lat  \n",
       "0  1.376423e-08  -119.840555    36.926310  \n",
       "1  1.571668e-08  -119.840561    36.926477  \n",
       "2  1.129864e-08  -119.840506    36.926542  \n",
       "3  6.266390e-09  -119.842847    36.921008  \n",
       "4  9.649632e-09  -119.842906    36.920976  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset ind_pv_solar_farms_2022 with 1 files in datasets/raw/labels/ind_pv_solar_farms_2022\n",
      "Loaded geodataframe with raw counts of 1363 PV installations\n",
      "Coordinate reference system: EPSG:4326\n",
      "Available columns: ['State', 'Area', 'Latitude', 'Longitude', 'fid', 'geometry']\n",
      "Warning: 78 invalid or empty geometries found and will be removed.\n",
      "Processing ind_pv_solar_farms_2022 metadata\n",
      "After filtering and cleaning, we have 1285 PV installations\n",
      "Coordinate reference system: EPSG:4326\n",
      "Available columns: ['State', 'Area', 'Latitude', 'Longitude', 'fid', 'geometry', 'dataset', 'area_m2', 'centroid_lon', 'centroid_lat']\n",
      "Saved processed GeoDataFrame to datasets/raw/labels/geoparquet/ind_pv_solar_farms_2022_processed.geoparquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alejandovega/miniconda3/envs/eo-pv-cv/lib/python3.10/site-packages/shapely/predicates.py:798: RuntimeWarning: invalid value encountered in intersects\n",
      "  return lib.intersects(a, b, **kwargs)\n",
      "/Users/alejandovega/miniconda3/envs/eo-pv-cv/lib/python3.10/site-packages/shapely/set_operations.py:133: RuntimeWarning: invalid value encountered in intersection\n",
      "  return lib.intersection(a, b, **kwargs)\n",
      "/var/folders/81/h1xdmxqn6l3_p3v3sw_s9cbw0000gp/T/ipykernel_32865/4059283130.py:168: UserWarning: Geometry is in a geographic CRS. Results from 'area' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  gdf['area_m2'] = gdf_proj.geometry.area\n",
      "/var/folders/81/h1xdmxqn6l3_p3v3sw_s9cbw0000gp/T/ipykernel_32865/4059283130.py:171: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  gdf['centroid_lon'] = gdf_proj.geometry.centroid.x\n",
      "/var/folders/81/h1xdmxqn6l3_p3v3sw_s9cbw0000gp/T/ipykernel_32865/4059283130.py:173: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  gdf['centroid_lat'] = gdf_proj.geometry.centroid.y\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Area",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Latitude",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Longitude",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "fid",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "area_m2",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "centroid_lon",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "centroid_lat",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "5fea59be-6618-4565-b42f-0d3a45bae311",
       "rows": [
        [
         "count",
         "1285.0",
         "1285.0",
         "1285.0",
         "1285.0",
         "1285.0",
         "1285.0",
         "1285.0"
        ],
        [
         "mean",
         "849996.5443835544",
         "18.544575981748682",
         "77.2103450269258",
         "713.7455252918288",
         "1.770170825251617e-05",
         "77.20873572205036",
         "18.540253332569552"
        ],
        [
         "std",
         "1908735.660470913",
         "5.722105535610118",
         "2.7921983338646945",
         "426.8921082757054",
         "9.321062614123379e-05",
         "2.790055988779209",
         "5.7256511741118326"
        ],
        [
         "min",
         "813.5934196922979",
         "8.535580204468703",
         "69.02573350783643",
         "1.0",
         "2.698065149585962e-08",
         "69.0257047824574",
         "8.535347643890422"
        ],
        [
         "25%",
         "119013.12355516835",
         "14.169227941915556",
         "75.87575079396218",
         "355.0",
         "1.8753898632180712e-06",
         "75.87580858428949",
         "14.169181008629536"
        ],
        [
         "50%",
         "305394.7492281041",
         "17.556943109673682",
         "77.39203353595325",
         "706.0",
         "5.318158893145182e-06",
         "77.39204578068481",
         "17.55628398058761"
        ],
        [
         "75%",
         "765914.8167132069",
         "23.0388537857541",
         "78.35039686864273",
         "1063.0",
         "1.646251571950983e-05",
         "78.3485230492988",
         "23.038629339011564"
        ],
        [
         "max",
         "30744917.459943466",
         "31.962736344360415",
         "91.27673686428876",
         "4421.0",
         "0.0024513875467852067",
         "91.27672875671833",
         "31.961922772173548"
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 8
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Area</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>fid</th>\n",
       "      <th>area_m2</th>\n",
       "      <th>centroid_lon</th>\n",
       "      <th>centroid_lat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.285000e+03</td>\n",
       "      <td>1285.000000</td>\n",
       "      <td>1285.000000</td>\n",
       "      <td>1285.000000</td>\n",
       "      <td>1.285000e+03</td>\n",
       "      <td>1285.000000</td>\n",
       "      <td>1285.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8.499965e+05</td>\n",
       "      <td>18.544576</td>\n",
       "      <td>77.210345</td>\n",
       "      <td>713.745525</td>\n",
       "      <td>1.770171e-05</td>\n",
       "      <td>77.208736</td>\n",
       "      <td>18.540253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.908736e+06</td>\n",
       "      <td>5.722106</td>\n",
       "      <td>2.792198</td>\n",
       "      <td>426.892108</td>\n",
       "      <td>9.321063e-05</td>\n",
       "      <td>2.790056</td>\n",
       "      <td>5.725651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>8.135934e+02</td>\n",
       "      <td>8.535580</td>\n",
       "      <td>69.025734</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.698065e-08</td>\n",
       "      <td>69.025705</td>\n",
       "      <td>8.535348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.190131e+05</td>\n",
       "      <td>14.169228</td>\n",
       "      <td>75.875751</td>\n",
       "      <td>355.000000</td>\n",
       "      <td>1.875390e-06</td>\n",
       "      <td>75.875809</td>\n",
       "      <td>14.169181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.053947e+05</td>\n",
       "      <td>17.556943</td>\n",
       "      <td>77.392034</td>\n",
       "      <td>706.000000</td>\n",
       "      <td>5.318159e-06</td>\n",
       "      <td>77.392046</td>\n",
       "      <td>17.556284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.659148e+05</td>\n",
       "      <td>23.038854</td>\n",
       "      <td>78.350397</td>\n",
       "      <td>1063.000000</td>\n",
       "      <td>1.646252e-05</td>\n",
       "      <td>78.348523</td>\n",
       "      <td>23.038629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.074492e+07</td>\n",
       "      <td>31.962736</td>\n",
       "      <td>91.276737</td>\n",
       "      <td>4421.000000</td>\n",
       "      <td>2.451388e-03</td>\n",
       "      <td>91.276729</td>\n",
       "      <td>31.961923</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Area     Latitude    Longitude          fid       area_m2  \\\n",
       "count  1.285000e+03  1285.000000  1285.000000  1285.000000  1.285000e+03   \n",
       "mean   8.499965e+05    18.544576    77.210345   713.745525  1.770171e-05   \n",
       "std    1.908736e+06     5.722106     2.792198   426.892108  9.321063e-05   \n",
       "min    8.135934e+02     8.535580    69.025734     1.000000  2.698065e-08   \n",
       "25%    1.190131e+05    14.169228    75.875751   355.000000  1.875390e-06   \n",
       "50%    3.053947e+05    17.556943    77.392034   706.000000  5.318159e-06   \n",
       "75%    7.659148e+05    23.038854    78.350397  1063.000000  1.646252e-05   \n",
       "max    3.074492e+07    31.962736    91.276737  4421.000000  2.451388e-03   \n",
       "\n",
       "       centroid_lon  centroid_lat  \n",
       "count   1285.000000   1285.000000  \n",
       "mean      77.208736     18.540253  \n",
       "std        2.790056      5.725651  \n",
       "min       69.025705      8.535348  \n",
       "25%       75.875809     14.169181  \n",
       "50%       77.392046     17.556284  \n",
       "75%       78.348523     23.038629  \n",
       "max       91.276729     31.961923  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>Area</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>fid</th>\n",
       "      <th>geometry</th>\n",
       "      <th>dataset</th>\n",
       "      <th>area_m2</th>\n",
       "      <th>centroid_lon</th>\n",
       "      <th>centroid_lat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Karnataka</td>\n",
       "      <td>3.072701e+05</td>\n",
       "      <td>13.094437</td>\n",
       "      <td>78.284459</td>\n",
       "      <td>677</td>\n",
       "      <td>MULTIPOLYGON (((78.28741 13.09153, 78.2881 13....</td>\n",
       "      <td>ind_pv_solar_farms_2022</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>78.286072</td>\n",
       "      <td>13.092826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Karnataka</td>\n",
       "      <td>1.394675e+05</td>\n",
       "      <td>13.083870</td>\n",
       "      <td>78.292082</td>\n",
       "      <td>678</td>\n",
       "      <td>MULTIPOLYGON (((78.29433 13.07967, 78.29461 13...</td>\n",
       "      <td>ind_pv_solar_farms_2022</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>78.291715</td>\n",
       "      <td>13.082543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Karnataka</td>\n",
       "      <td>5.497562e+05</td>\n",
       "      <td>13.121554</td>\n",
       "      <td>78.343837</td>\n",
       "      <td>675</td>\n",
       "      <td>MULTIPOLYGON (((78.3433 13.12249, 78.34427 13....</td>\n",
       "      <td>ind_pv_solar_farms_2022</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>78.343961</td>\n",
       "      <td>13.121596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Karnataka</td>\n",
       "      <td>5.240948e+05</td>\n",
       "      <td>13.747866</td>\n",
       "      <td>77.552639</td>\n",
       "      <td>676</td>\n",
       "      <td>MULTIPOLYGON (((77.55266 13.74717, 77.55364 13...</td>\n",
       "      <td>ind_pv_solar_farms_2022</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>77.552326</td>\n",
       "      <td>13.747899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Karnataka</td>\n",
       "      <td>1.701728e+06</td>\n",
       "      <td>13.551716</td>\n",
       "      <td>77.514227</td>\n",
       "      <td>665</td>\n",
       "      <td>MULTIPOLYGON (((77.50913 13.55098, 77.51162 13...</td>\n",
       "      <td>ind_pv_solar_farms_2022</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>77.512698</td>\n",
       "      <td>13.550867</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       State          Area   Latitude  Longitude  fid  \\\n",
       "0  Karnataka  3.072701e+05  13.094437  78.284459  677   \n",
       "1  Karnataka  1.394675e+05  13.083870  78.292082  678   \n",
       "2  Karnataka  5.497562e+05  13.121554  78.343837  675   \n",
       "3  Karnataka  5.240948e+05  13.747866  77.552639  676   \n",
       "4  Karnataka  1.701728e+06  13.551716  77.514227  665   \n",
       "\n",
       "                                            geometry                  dataset  \\\n",
       "0  MULTIPOLYGON (((78.28741 13.09153, 78.2881 13....  ind_pv_solar_farms_2022   \n",
       "1  MULTIPOLYGON (((78.29433 13.07967, 78.29461 13...  ind_pv_solar_farms_2022   \n",
       "2  MULTIPOLYGON (((78.3433 13.12249, 78.34427 13....  ind_pv_solar_farms_2022   \n",
       "3  MULTIPOLYGON (((77.55266 13.74717, 77.55364 13...  ind_pv_solar_farms_2022   \n",
       "4  MULTIPOLYGON (((77.50913 13.55098, 77.51162 13...  ind_pv_solar_farms_2022   \n",
       "\n",
       "    area_m2  centroid_lon  centroid_lat  \n",
       "0  0.000004     78.286072     13.092826  \n",
       "1  0.000007     78.291715     13.082543  \n",
       "2  0.000002     78.343961     13.121596  \n",
       "3  0.000002     77.552326     13.747899  \n",
       "4  0.000016     77.512698     13.550867  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset global_pv_inventory_sent2_spot_2021 with 4 files in datasets/raw/labels/global_pv_inventory_sent2_spot_2021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pyogrio._io:Skipping field ind_comm_10km: unsupported OGR type: 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded geodataframe with raw counts of 119087 PV installations\n",
      "Coordinate reference system: EPSG:4326\n",
      "Available columns: ['unique_id', 'area', 'confidence', 'install_date', 'iso-3166-1', 'iso-3166-2', 'gti', 'pvout', 'capacity_mw', 'match_id', 'wdpa_10km', 'LC_CLC300_1992', 'LC_CLC300_1993', 'LC_CLC300_1994', 'LC_CLC300_1995', 'LC_CLC300_1996', 'LC_CLC300_1997', 'LC_CLC300_1998', 'LC_CLC300_1999', 'LC_CLC300_2000', 'LC_CLC300_2001', 'LC_CLC300_2002', 'LC_CLC300_2003', 'LC_CLC300_2004', 'LC_CLC300_2005', 'LC_CLC300_2006', 'LC_CLC300_2007', 'LC_CLC300_2008', 'LC_CLC300_2009', 'LC_CLC300_2010', 'LC_CLC300_2011', 'LC_CLC300_2012', 'LC_CLC300_2013', 'LC_CLC300_2014', 'LC_CLC300_2015', 'LC_CLC300_2016', 'LC_CLC300_2017', 'LC_CLC300_2018', 'mean_ai', 'GCR', 'eff', 'ILR', 'area_error', 'lc_mode', 'lc_arid', 'lc_vis', 'geometry', 'aoi_idx', 'aoi', 'id', 'Country', 'Province', 'Project', 'WRI_ref', 'Polygon Source', 'Date', 'building', 'operator', 'generator_source', 'amenity', 'landuse', 'power_source', 'shop', 'sport', 'tourism', 'way_area', 'access', 'construction', 'denomination', 'historic', 'leisure', 'man_made', 'natural', 'ref', 'religion', 'surface', 'z_order', 'layer', 'name', 'barrier', 'addr_housenumber', 'office', 'power', 'osm_id', 'military']\n",
      "Warning: 30 invalid or empty geometries found and will be removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alejandovega/miniconda3/envs/eo-pv-cv/lib/python3.10/site-packages/shapely/predicates.py:798: RuntimeWarning: invalid value encountered in intersects\n",
      "  return lib.intersects(a, b, **kwargs)\n",
      "/Users/alejandovega/miniconda3/envs/eo-pv-cv/lib/python3.10/site-packages/shapely/set_operations.py:133: RuntimeWarning: invalid value encountered in intersection\n",
      "  return lib.intersection(a, b, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 41613 geometries with >75.0% overlap\n",
      "Processing global_pv_inventory_sent2_spot_2021 metadata\n",
      "Filtering from 85 columns to 11 columns:\n",
      "['geometry', 'unique_id', 'area', 'confidence', 'install_date', 'capacity_mw', 'iso-3166-2', 'pvout', 'osm_id', 'Project', 'construction']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/81/h1xdmxqn6l3_p3v3sw_s9cbw0000gp/T/ipykernel_32865/4059283130.py:168: UserWarning: Geometry is in a geographic CRS. Results from 'area' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  gdf['area_m2'] = gdf_proj.geometry.area\n",
      "/var/folders/81/h1xdmxqn6l3_p3v3sw_s9cbw0000gp/T/ipykernel_32865/4059283130.py:171: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  gdf['centroid_lon'] = gdf_proj.geometry.centroid.x\n",
      "/var/folders/81/h1xdmxqn6l3_p3v3sw_s9cbw0000gp/T/ipykernel_32865/4059283130.py:173: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  gdf['centroid_lat'] = gdf_proj.geometry.centroid.y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After filtering and cleaning, we have 85375 PV installations\n",
      "Coordinate reference system: EPSG:4326\n",
      "Available columns: ['geometry', 'unique_id', 'area', 'confidence', 'install_date', 'capacity_mw', 'iso-3166-2', 'pvout', 'osm_id', 'Project', 'construction', 'area_m2', 'centroid_lon', 'centroid_lat']\n"
     ]
    },
    {
     "ename": "ArrowInvalid",
     "evalue": "(\"Could not convert 'yes' with type str: tried to convert to double\", 'Conversion failed for column area with type object')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m out_dir \u001b[38;5;241m=\u001b[39m DATASET_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeoparquet\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing dataset \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mds\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(ds_files)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m files in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrelpath(ds_dir)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m ds_gdf \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_geojson\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgeojson_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mds_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout_dir\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ds_gdf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     15\u001b[0m     display(ds_gdf\u001b[38;5;241m.\u001b[39mdescribe())\n",
      "Cell \u001b[0;32mIn[48], line 185\u001b[0m, in \u001b[0;36mprocess_geojson\u001b[0;34m(geojson_files, dataset_name, output_dir, subset_bbox, geom_type, rm_invalid, overlap_thresh, out_fmt)\u001b[0m\n\u001b[1;32m    182\u001b[0m out_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_processed.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_fmt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out_fmt \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeoparquet\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 185\u001b[0m     \u001b[43mgdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msnappy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeometry_encoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mWKB\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwrite_covering_bbox\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mschema_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m1.1.0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    192\u001b[0m     gdf\u001b[38;5;241m.\u001b[39mto_file(out_path, driver\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGeoJSON\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/eo-pv-cv/lib/python3.10/site-packages/geopandas/geodataframe.py:1377\u001b[0m, in \u001b[0;36mGeoDataFrame.to_parquet\u001b[0;34m(self, path, index, compression, geometry_encoding, write_covering_bbox, schema_version, **kwargs)\u001b[0m\n\u001b[1;32m   1370\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1371\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGeoPandas only supports using pyarrow as the engine for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1372\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_parquet: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mengine\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m passed instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1373\u001b[0m     )\n\u001b[1;32m   1375\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgeopandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _to_parquet\n\u001b[0;32m-> 1377\u001b[0m \u001b[43m_to_parquet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1378\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1379\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1380\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeometry_encoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeometry_encoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1382\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mschema_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1384\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwrite_covering_bbox\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrite_covering_bbox\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1385\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1386\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/eo-pv-cv/lib/python3.10/site-packages/geopandas/io/arrow.py:432\u001b[0m, in \u001b[0;36m_to_parquet\u001b[0;34m(df, path, index, compression, geometry_encoding, schema_version, write_covering_bbox, **kwargs)\u001b[0m\n\u001b[1;32m    427\u001b[0m parquet \u001b[38;5;241m=\u001b[39m import_optional_dependency(\n\u001b[1;32m    428\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m, extra\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow is required for Parquet support.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    429\u001b[0m )\n\u001b[1;32m    431\u001b[0m path \u001b[38;5;241m=\u001b[39m _expand_user(path)\n\u001b[0;32m--> 432\u001b[0m table \u001b[38;5;241m=\u001b[39m \u001b[43m_geopandas_to_arrow\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeometry_encoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeometry_encoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m    \u001b[49m\u001b[43mschema_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwrite_covering_bbox\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrite_covering_bbox\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m parquet\u001b[38;5;241m.\u001b[39mwrite_table(table, path, compression\u001b[38;5;241m=\u001b[39mcompression, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/eo-pv-cv/lib/python3.10/site-packages/geopandas/io/arrow.py:348\u001b[0m, in \u001b[0;36m_geopandas_to_arrow\u001b[0;34m(df, index, geometry_encoding, schema_version, write_covering_bbox)\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m geometry_encoding \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWKB\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m schema_version \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.1.0\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    344\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    345\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeoarrow\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m encoding is only supported with schema version >= 1.1.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    346\u001b[0m         )\n\u001b[0;32m--> 348\u001b[0m table, geometry_encoding_dict \u001b[38;5;241m=\u001b[39m \u001b[43mgeopandas_to_arrow\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeometry_encoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeometry_encoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterleaved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m    350\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    351\u001b[0m geo_metadata \u001b[38;5;241m=\u001b[39m _create_metadata(\n\u001b[1;32m    352\u001b[0m     df,\n\u001b[1;32m    353\u001b[0m     schema_version\u001b[38;5;241m=\u001b[39mschema_version,\n\u001b[1;32m    354\u001b[0m     geometry_encoding\u001b[38;5;241m=\u001b[39mgeometry_encoding_dict,\n\u001b[1;32m    355\u001b[0m     write_covering_bbox\u001b[38;5;241m=\u001b[39mwrite_covering_bbox,\n\u001b[1;32m    356\u001b[0m )\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m write_covering_bbox:\n",
      "File \u001b[0;32m~/miniconda3/envs/eo-pv-cv/lib/python3.10/site-packages/geopandas/io/_geoarrow.py:136\u001b[0m, in \u001b[0;36mgeopandas_to_arrow\u001b[0;34m(df, index, geometry_encoding, interleaved, include_z)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m geometry_columns:\n\u001b[1;32m    134\u001b[0m     df_attr[col] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 136\u001b[0m table \u001b[38;5;241m=\u001b[39m \u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_attr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreserve_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m geometry_encoding_dict \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m geometry_encoding\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeoarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/eo-pv-cv/lib/python3.10/site-packages/pyarrow/table.pxi:4751\u001b[0m, in \u001b[0;36mpyarrow.lib.Table.from_pandas\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/eo-pv-cv/lib/python3.10/site-packages/pyarrow/pandas_compat.py:652\u001b[0m, in \u001b[0;36mdataframe_to_arrays\u001b[0;34m(df, schema, preserve_index, nthreads, columns, safe)\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, maybe_fut \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(arrays):\n\u001b[1;32m    651\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_fut, futures\u001b[38;5;241m.\u001b[39mFuture):\n\u001b[0;32m--> 652\u001b[0m             arrays[i] \u001b[38;5;241m=\u001b[39m \u001b[43mmaybe_fut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    654\u001b[0m types \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/eo-pv-cv/lib/python3.10/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m~/miniconda3/envs/eo-pv-cv/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/eo-pv-cv/lib/python3.10/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m~/miniconda3/envs/eo-pv-cv/lib/python3.10/site-packages/pyarrow/pandas_compat.py:626\u001b[0m, in \u001b[0;36mdataframe_to_arrays.<locals>.convert_column\u001b[0;34m(col, field)\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (pa\u001b[38;5;241m.\u001b[39mArrowInvalid,\n\u001b[1;32m    622\u001b[0m         pa\u001b[38;5;241m.\u001b[39mArrowNotImplementedError,\n\u001b[1;32m    623\u001b[0m         pa\u001b[38;5;241m.\u001b[39mArrowTypeError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    624\u001b[0m     e\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConversion failed for column \u001b[39m\u001b[38;5;132;01m{!s}\u001b[39;00m\u001b[38;5;124m with type \u001b[39m\u001b[38;5;132;01m{!s}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    625\u001b[0m                \u001b[38;5;241m.\u001b[39mformat(col\u001b[38;5;241m.\u001b[39mname, col\u001b[38;5;241m.\u001b[39mdtype),)\n\u001b[0;32m--> 626\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m field_nullable \u001b[38;5;129;01mand\u001b[39;00m result\u001b[38;5;241m.\u001b[39mnull_count \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mField \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m was non-nullable but pandas column \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    629\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhad \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m null values\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mstr\u001b[39m(field),\n\u001b[1;32m    630\u001b[0m                                                  result\u001b[38;5;241m.\u001b[39mnull_count))\n",
      "File \u001b[0;32m~/miniconda3/envs/eo-pv-cv/lib/python3.10/site-packages/pyarrow/pandas_compat.py:620\u001b[0m, in \u001b[0;36mdataframe_to_arrays.<locals>.convert_column\u001b[0;34m(col, field)\u001b[0m\n\u001b[1;32m    617\u001b[0m     type_ \u001b[38;5;241m=\u001b[39m field\u001b[38;5;241m.\u001b[39mtype\n\u001b[1;32m    619\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 620\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtype_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_pandas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (pa\u001b[38;5;241m.\u001b[39mArrowInvalid,\n\u001b[1;32m    622\u001b[0m         pa\u001b[38;5;241m.\u001b[39mArrowNotImplementedError,\n\u001b[1;32m    623\u001b[0m         pa\u001b[38;5;241m.\u001b[39mArrowTypeError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    624\u001b[0m     e\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConversion failed for column \u001b[39m\u001b[38;5;132;01m{!s}\u001b[39;00m\u001b[38;5;124m with type \u001b[39m\u001b[38;5;132;01m{!s}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    625\u001b[0m                \u001b[38;5;241m.\u001b[39mformat(col\u001b[38;5;241m.\u001b[39mname, col\u001b[38;5;241m.\u001b[39mdtype),)\n",
      "File \u001b[0;32m~/miniconda3/envs/eo-pv-cv/lib/python3.10/site-packages/pyarrow/array.pxi:362\u001b[0m, in \u001b[0;36mpyarrow.lib.array\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/eo-pv-cv/lib/python3.10/site-packages/pyarrow/array.pxi:87\u001b[0m, in \u001b[0;36mpyarrow.lib._ndarray_to_array\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/eo-pv-cv/lib/python3.10/site-packages/pyarrow/error.pxi:92\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mArrowInvalid\u001b[0m: (\"Could not convert 'yes' with type str: tried to convert to double\", 'Conversion failed for column area with type object')"
     ]
    }
   ],
   "source": [
    "random.shuffle(selected_datasets)\n",
    "# go through the selected datasets and process them\n",
    "for ds in selected_datasets:\n",
    "    ds_files = selected_ds_files[ds]\n",
    "    ds_dir = get_ds_dir(ds)\n",
    "    out_dir = DATASET_DIR / 'raw' / 'labels' / 'geoparquet'\n",
    "    print(f\"Processing dataset {ds} with {len(ds_files)} files in {os.path.relpath(ds_dir)}\")\n",
    "    ds_gdf = process_geojson(\n",
    "                geojson_files=ds_files,\n",
    "                dataset_name=ds,\n",
    "                output_dir=out_dir\n",
    "    )\n",
    "    if ds_gdf is not None:\n",
    "        \n",
    "        display(ds_gdf.describe())\n",
    "        print(ds_gdf.info)\n",
    "        display(ds_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2aaccda2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'global_pv_inventory_sent2_spot_2021'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_datasets[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "286c2d25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Shuffle list x in place, and return None.\\n\\n        Optional argument random is a 0-argument function returning a\\n        random float in [0.0, 1.0); if it is the default None, the\\n        standard random.random will be used.\\n\\n        '"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.shuffle.__doc__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e917882",
   "metadata": {},
   "source": [
    "#### France West Europe PV Installations 2023\n",
    "\n",
    "From [research publication](https://doi.org/10.1038/s41597-023-01951-4): \n",
    "```\n",
    "The Git repository contains the raw crowdsourcing data and all the material necessary to re-generate our training dataset and technical validation.  \n",
    "It is structured as follows: the raw subfolder contains the raw annotation data from the two annotation campaigns and the raw PV installations’ metadata.  \n",
    "The replication subfolder contains the compiled data used to generate our segmentation masks.  \n",
    "The validation subfolder contains the compiled data necessary to replicate the analyses presented in the technical validation section.\n",
    "```\n",
    "\n",
    "We will be using the `replication` subfolder to generate our PV polygons geojson file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0867b736",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(selected_metadata['fra_west_eur_pv_installations_2023'].keys())\n",
    "print(selected_metadata['fra_west_eur_pv_installations_2023']['files'])\n",
    "data_out = Path(os.getenv('DATA_PATH'))\n",
    "fra_files = selected_metadata['fra_west_eur_pv_installations_2023']['files']\n",
    "fra_out = selected_metadata['fra_west_eur_pv_installations_2023']['output_dir']\n",
    "ds_sub = os.path.join(fra_out, 'replication')\n",
    "fra_sub_files = '\\n'.join([os.path.relpath(f, data_out) for f in fra_files if f.startswith(ds_sub)])\n",
    "print(f\"Subdir files:\\n{fra_sub_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5ecb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bespoke pre-processing for datsets not directly available in geojson or shapefile format\n",
    "# parse the point or polygon json files with geopandas, transform raw polygons or points features into proper geometry for geojson conversion\n",
    "from shapely.geometry import Polygon, Point, MultiPolygon\n",
    "import json\n",
    "\n",
    "# TODO: make function for processing of france json geometries\n",
    "\n",
    "def france_eur_pv_preprocess(ds_metadata, ds_subdir, metadata_dir='raw', crs=None, geom_type='Polygon'):\n",
    "    ds_dir = Path(ds_metadata['output_dir'])\n",
    "    data_dir = ds_dir / ds_subdir\n",
    "    metadata_file = 'raw-metadata_df.csv' if metadata_dir == 'raw' else 'metadata_df.csv'\n",
    "    metadata_file = ds_dir / metadata_dir / metadata_file\n",
    "    coords_file = \"polygon-analysis.json\" if geom_type == 'Polygon' else \"point-analysis.json\"\n",
    "    # keep files that are in the specified subdir and have the above filename\n",
    "    geom_files = [fpath for fpath in ds_metadata['files'] if fpath.startswith(data_dir) and fpath.endswith(coords_file)]\n",
    "    crs = crs or 'EPSG:4326' # default to WGS84\n",
    "\n",
    "    # load the metadata file\n",
    "    metadata_df = pd.read_csv(metadata_file)\n",
    "    print(f\"Loaded '{metadata_file.split('/')[-1]}' with {len(metadata_df)} rows\")\n",
    "\n",
    "    # load into geopandas, inspect the data, and add metadata_df to separate pd dataframe\n",
    "    raw_features = []\n",
    "    for geom_file_path in geom_files:\n",
    "        campaign_name = Path(geom_file_path).parent.name\n",
    "        print(f\"Processing {campaign_name} campaign...\")\n",
    "        \n",
    "        with open(geom_file_path, 'r') as f:\n",
    "            geom_data = json.load(f)\n",
    "        feat_types = set([f['type'] for f in geom_data])\n",
    "        print(f\"Feature types: {feat_types}\")\n",
    "    \n",
    "        for idx, feature_dict in enumerate(geom_data):\n",
    "            # Skip empty dictionaries\n",
    "            if not feature_dict:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                feature_id = feature_dict.get('id', idx) # Use index if ID is not present\n",
    "\n",
    "                # extract geometry and coordinates\n",
    "                if geom_type == 'Polygon':\n",
    "                    # feat_dict = [{'polygons': [{'points': {'x': <px_coord>, 'y': <px_coord>}, ...}]}, ...]\n",
    "                    coords = feature_dict['polygons']\n",
    "                    if isinstance(coords, list) and len(coords) > 0:\n",
    "                            # Handle multiple polygons\n",
    "                            polygons = []\n",
    "                            for poly_coords in coords:\n",
    "                                if len(poly_coords) >= 3:  # Need at least 3 points for a polygon\n",
    "                                    polygons.append(Polygon(poly_coords))\n",
    "                            \n",
    "                            if len(polygons) == 1:\n",
    "                                geometry = polygons[0]\n",
    "                            else:\n",
    "                                geometry = MultiPolygon(polygons)\n",
    "                                \n",
    "                            # Create feature dictionary with properties\n",
    "                            feature = {\n",
    "                                'id': feature_id,\n",
    "                                'campaign': campaign_name,\n",
    "                                'geometry': geometry\n",
    "                            }\n",
    "                    raw_features.append(feature)\n",
    "                elif geom_type == 'Point':\n",
    "                    # feat_dict = [{'clicks': [{'@type': 'Point', 'x': <px_coord>, 'y': <px_coord>}, ...]}, ...]\n",
    "                    coords = feature_dict['clicks']\n",
    "                    if isinstance(coords, list) and len(coords) > 0:\n",
    "                        points = []\n",
    "                        for point_coords in coords:\n",
    "                            if 'x' not in point_coords or 'y' not in point_coords:\n",
    "                                continue\n",
    "                            else:\n",
    "                                points.append(Point(point_coords['x'], point_coords['y']))\n",
    "                    raw_features.extend(points)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing feature {feature_dict}: {e}\")\n",
    "                continue\n",
    "\n",
    "    if raw_features:\n",
    "        # Convert to GeoDataFrame\n",
    "        gdf = gpd.GeoDataFrame(raw_features, crs=crs)\n",
    "        # add metadata to the gdf\n",
    "        if 'id' in gdf.columns:\n",
    "            gdf['id'] = gdf['id'].astype(str)\n",
    "        # Ensure CRS is set\n",
    "        if gdf.crs is None:\n",
    "            gdf.set_crs(crs, inplace=True)\n",
    "        elif str(gdf.crs) != crs:\n",
    "            gdf = gdf.to_crs(crs)\n",
    "        # need to add geotransform if available to convert pixel coords to lat/lon\n",
    "\n",
    "        # gdf['source_dataset'] = add in calling function\n",
    "    \n",
    "    return gdf, metadata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e74007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the different datasets and convert to geoparquet with geopandas\n",
    "def gdp_load_and_gpq_convert(dataset_name, label_fmt='geojson', crs=None, geom_type='Polygon'):\n",
    "\n",
    "    crs = crs or 'EPSG:4326' # default to WGS84 if crs is None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b7a3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.tree.__doc__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f01ce6",
   "metadata": {},
   "source": [
    "# Visualization Functions for PV Data\n",
    "\n",
    "After processing the datasets into standardized geoparquet format, we'll use the following visualization libraries to explore and present the data:\n",
    "\n",
    "- **Folium**: For interactive web maps with various basemaps and markers\n",
    "- **Pydeck**: For high-performance 3D and large-scale visualizations\n",
    "- **Lonboard**: For GPU-accelerated geospatial visualization of large datasets\n",
    "\n",
    "Each library has specific strengths that we'll leverage for different visualization needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf64feb",
   "metadata": {},
   "source": [
    "## Folium Visualization Functions\n",
    "\n",
    "Folium is excellent for creating interactive web maps with various basemaps and markers. It's particularly useful for visualizing geographic distributions and creating choropleth maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3987442e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folium_cluster_map(gdf, zoom_start=3, title=\"PV Installation Clusters\"):\n",
    "    \"\"\"\n",
    "    Create a cluster map of PV installations using Folium.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gdf : GeoDataFrame\n",
    "        GeoDataFrame containing PV installation data with geometry column\n",
    "    zoom_start : int\n",
    "        Initial zoom level for the map\n",
    "    title : str\n",
    "        Title for the map\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    folium.Map\n",
    "        Interactive Folium map with clustered markers\n",
    "    \"\"\"\n",
    "    # Ensure the GeoDataFrame is in WGS84 (EPSG:4326) for Folium compatibility\n",
    "    if gdf.crs != \"EPSG:4326\":\n",
    "        gdf = gdf.to_crs(\"EPSG:4326\")\n",
    "    \n",
    "    # Get centroid of all points to center the map\n",
    "    center_lat = gdf.geometry.centroid.y.mean()\n",
    "    center_lon = gdf.geometry.centroid.x.mean()\n",
    "    \n",
    "    # Create base map\n",
    "    m = folium.Map(location=[center_lat, center_lon], zoom_start=zoom_start,\n",
    "                  tiles='CartoDB positron')\n",
    "    \n",
    "    # Add title\n",
    "    title_html = f'''\n",
    "             <h3 align=\"center\" style=\"font-size:16px\"><b>{title}</b></h3>\n",
    "             '''\n",
    "    m.get_root().html.add_child(folium.Element(title_html))\n",
    "    \n",
    "    # Add marker cluster\n",
    "    marker_cluster = folium.plugins.MarkerCluster().add_to(m)\n",
    "    \n",
    "    # Add markers for each PV installation\n",
    "    for idx, row in gdf.iterrows():\n",
    "        # Get the centroid if the geometry is a polygon\n",
    "        if row.geometry.geom_type in ['Polygon', 'MultiPolygon']:\n",
    "            centroid = row.geometry.centroid\n",
    "            popup_text = f\"ID: {idx}\"\n",
    "            \n",
    "            # Add additional information if available in the dataframe\n",
    "            for col in ['capacity_mw', 'area_sqm', 'installation_date', 'source_dataset']:\n",
    "                if col in gdf.columns:\n",
    "                    popup_text += f\"<br>{col}: {row[col]}\"\n",
    "            \n",
    "            folium.Marker(\n",
    "                location=[centroid.y, centroid.x],\n",
    "                popup=folium.Popup(popup_text, max_width=300),\n",
    "                icon=folium.Icon(color='green', icon='solar-panel', prefix='fa')\n",
    "            ).add_to(marker_cluster)\n",
    "    \n",
    "    return m\n",
    "\n",
    "def create_folium_choropleth(gdf, column, bins=8, cmap='YlOrRd', \n",
    "                             title=\"PV Installation Density\"):\n",
    "    \"\"\"\n",
    "    Create a choropleth map of PV installations using Folium.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gdf : GeoDataFrame\n",
    "        GeoDataFrame containing PV installation data with geometry column\n",
    "    column : str\n",
    "        Column name to use for choropleth coloring\n",
    "    bins : int\n",
    "        Number of bins for choropleth map\n",
    "    cmap : str\n",
    "        Matplotlib colormap name\n",
    "    title : str\n",
    "        Title for the map\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    folium.Map\n",
    "        Interactive Folium choropleth map\n",
    "    \"\"\"\n",
    "    # Ensure the GeoDataFrame is in WGS84 (EPSG:4326)\n",
    "    if gdf.crs != \"EPSG:4326\":\n",
    "        gdf = gdf.to_crs(\"EPSG:4326\")\n",
    "    \n",
    "    # Get centroid of all points to center the map\n",
    "    center_lat = gdf.geometry.centroid.y.mean()\n",
    "    center_lon = gdf.geometry.centroid.x.mean()\n",
    "    \n",
    "    # Create base map\n",
    "    m = folium.Map(location=[center_lat, center_lon], zoom_start=3,\n",
    "                  tiles='CartoDB positron')\n",
    "    \n",
    "    # Add title\n",
    "    title_html = f'''\n",
    "             <h3 align=\"center\" style=\"font-size:16px\"><b>{title}</b></h3>\n",
    "             '''\n",
    "    m.get_root().html.add_child(folium.Element(title_html))\n",
    "    \n",
    "    # Create choropleth layer\n",
    "    folium.Choropleth(\n",
    "        geo_data=gdf,\n",
    "        name='choropleth',\n",
    "        data=gdf,\n",
    "        columns=[gdf.index.name if gdf.index.name else 'index', column],\n",
    "        key_on='feature.id',\n",
    "        fill_color=cmap,\n",
    "        fill_opacity=0.7,\n",
    "        line_opacity=0.2,\n",
    "        legend_name=column,\n",
    "        bins=bins\n",
    "    ).add_to(m)\n",
    "    \n",
    "    # Add hover functionality\n",
    "    style_function = lambda x: {'fillColor': '#ffffff', \n",
    "                                'color': '#000000', \n",
    "                                'fillOpacity': 0.1, \n",
    "                                'weight': 0.1}\n",
    "    highlight_function = lambda x: {'fillColor': '#000000', \n",
    "                                    'color': '#000000', \n",
    "                                    'fillOpacity': 0.5, \n",
    "                                    'weight': 0.1}\n",
    "    \n",
    "    # Add tooltips\n",
    "    folium.GeoJson(\n",
    "        gdf,\n",
    "        style_function=style_function,\n",
    "        highlight_function=highlight_function,\n",
    "        tooltip=folium.GeoJsonTooltip(\n",
    "            fields=[column],\n",
    "            aliases=[column.replace('_', ' ').title()],\n",
    "            style=(\"background-color: white; color: #333333; font-family: arial; font-size: 12px; padding: 10px;\")\n",
    "        )\n",
    "    ).add_to(m)\n",
    "    \n",
    "    # Add layer control\n",
    "    folium.LayerControl().add_to(m)\n",
    "    \n",
    "    return m\n",
    "\n",
    "def create_folium_heatmap(gdf, intensity_column=None, radius=15, \n",
    "                          title=\"PV Installation Heatmap\"):\n",
    "    \"\"\"\n",
    "    Create a heatmap of PV installations using Folium.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gdf : GeoDataFrame\n",
    "        GeoDataFrame containing PV installation data with geometry column\n",
    "    intensity_column : str, optional\n",
    "        Column name to use for heatmap intensity; if None, all points have equal weight\n",
    "    radius : int\n",
    "        Radius for heatmap points (in pixels)\n",
    "    title : str\n",
    "        Title for the map\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    folium.Map\n",
    "        Interactive Folium heatmap\n",
    "    \"\"\"\n",
    "    # Ensure the GeoDataFrame is in WGS84 (EPSG:4326)\n",
    "    if gdf.crs != \"EPSG:4326\":\n",
    "        gdf = gdf.to_crs(\"EPSG:4326\")\n",
    "    \n",
    "    # Get centroids for all geometries\n",
    "    if any(gdf.geometry.geom_type.isin(['Polygon', 'MultiPolygon'])):\n",
    "        centroids = gdf.geometry.centroid\n",
    "    else:\n",
    "        centroids = gdf.geometry\n",
    "    \n",
    "    # Get coordinates for heatmap\n",
    "    heat_data = [[point.y, point.x] for point in centroids]\n",
    "    \n",
    "    # Add intensity if specified\n",
    "    if intensity_column and intensity_column in gdf.columns:\n",
    "        heat_data = [[point.y, point.x, float(intensity)] \n",
    "                    for point, intensity in zip(centroids, gdf[intensity_column])]\n",
    "    \n",
    "    # Get centroid of all points to center the map\n",
    "    center_lat = sum(point[0] for point in heat_data) / len(heat_data)\n",
    "    center_lon = sum(point[1] for point in heat_data) / len(heat_data)\n",
    "    \n",
    "    # Create base map\n",
    "    m = folium.Map(location=[center_lat, center_lon], zoom_start=4,\n",
    "                  tiles='CartoDB positron')\n",
    "    \n",
    "    # Add title\n",
    "    title_html = f'''\n",
    "             <h3 align=\"center\" style=\"font-size:16px\"><b>{title}</b></h3>\n",
    "             '''\n",
    "    m.get_root().html.add_child(folium.Element(title_html))\n",
    "    \n",
    "    # Add heatmap layer\n",
    "    folium.plugins.HeatMap(\n",
    "        heat_data,\n",
    "        radius=radius,\n",
    "        blur=10,\n",
    "        gradient={0.4: 'blue', 0.65: 'lime', 1: 'red'}\n",
    "    ).add_to(m)\n",
    "    \n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d04e252",
   "metadata": {},
   "source": [
    "## PyDeck Visualization Functions\n",
    "\n",
    "PyDeck is excellent for high-performance 3D visualizations and handling large datasets. It's particularly useful for creating layered maps with multiple types of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaac7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pydeck_scatterplot(gdf, color_column=None, size_scale=100, \n",
    "                             title=\"PV Installation Map\"):\n",
    "    \"\"\"\n",
    "    Create a scatterplot of PV installations using PyDeck.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gdf : GeoDataFrame\n",
    "        GeoDataFrame containing PV installation data with geometry column\n",
    "    color_column : str, optional\n",
    "        Column name to use for point coloring\n",
    "    size_scale : float\n",
    "        Scaling factor for point size\n",
    "    title : str\n",
    "        Title for the map\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pydeck.Deck\n",
    "        Interactive PyDeck map with scatterplot layer\n",
    "    \"\"\"\n",
    "    # Ensure the GeoDataFrame is in WGS84 (EPSG:4326)\n",
    "    if gdf.crs != \"EPSG:4326\":\n",
    "        gdf = gdf.to_crs(\"EPSG:4326\")\n",
    "    \n",
    "    # Convert to DataFrame with lat/lon columns\n",
    "    df = pd.DataFrame({\n",
    "        'lat': gdf.geometry.centroid.y,\n",
    "        'lon': gdf.geometry.centroid.x\n",
    "    })\n",
    "    \n",
    "    # Add additional columns from the original GeoDataFrame\n",
    "    for col in gdf.columns:\n",
    "        if col != 'geometry':\n",
    "            df[col] = gdf[col]\n",
    "    \n",
    "    # Handle color mapping\n",
    "    if color_column and color_column in df.columns:\n",
    "        # Check if the column is numeric\n",
    "        if pd.api.types.is_numeric_dtype(df[color_column]):\n",
    "            color_scale = [\n",
    "                [0, [65, 182, 196]],\n",
    "                [0.33, [127, 205, 187]],\n",
    "                [0.66, [199, 233, 180]],\n",
    "                [1, [237, 248, 177]]\n",
    "            ]\n",
    "            \n",
    "            # Normalize the values\n",
    "            df['color_value'] = (df[color_column] - df[color_column].min()) / (df[color_column].max() - df[color_column].min())\n",
    "            get_color = f\"[r, g, b]\"\n",
    "            \n",
    "            # Create a calculated color column using the scale\n",
    "            df['r'] = df['color_value'].apply(lambda x: int(\n",
    "                np.interp(x, [scale[0] for scale in color_scale], [scale[1][0] for scale in color_scale])\n",
    "            ))\n",
    "            df['g'] = df['color_value'].apply(lambda x: int(\n",
    "                np.interp(x, [scale[0] for scale in color_scale], [scale[1][1] for scale in color_scale])\n",
    "            ))\n",
    "            df['b'] = df['color_value'].apply(lambda x: int(\n",
    "                np.interp(x, [scale[0] for scale in color_scale], [scale[1][2] for scale in color_scale])\n",
    "            ))\n",
    "            \n",
    "        else:\n",
    "            # For categorical data, use hash of category for color\n",
    "            unique_cats = df[color_column].unique()\n",
    "            color_map = {cat: [int(h) % 256 for h in str(hash(cat))[:3]] for cat in unique_cats}\n",
    "            df['r'] = df[color_column].map(lambda x: color_map[x][0])\n",
    "            df['g'] = df[color_column].map(lambda x: color_map[x][1])\n",
    "            df['b'] = df[color_column].map(lambda x: color_map[x][2])\n",
    "            \n",
    "        get_color = \"[r, g, b]\"\n",
    "    else:\n",
    "        # Default color: green for solar panels\n",
    "        get_color = \"[0, 128, 0]\"  # Green\n",
    "    \n",
    "    # Calculate point size - use area if available\n",
    "    if 'area_sqm' in df.columns:\n",
    "        get_size = f\"Math.sqrt(area_sqm) * {size_scale/100}\"\n",
    "    elif 'capacity_mw' in df.columns:\n",
    "        get_size = f\"Math.sqrt(capacity_mw) * {size_scale/10}\"\n",
    "    else:\n",
    "        get_size = str(size_scale)\n",
    "    \n",
    "    # Create ScatterplotLayer\n",
    "    layer = pdk.Layer(\n",
    "        'ScatterplotLayer',\n",
    "        df,\n",
    "        get_position=['lon', 'lat'],\n",
    "        get_radius=get_size,\n",
    "        get_fill_color=get_color,\n",
    "        pickable=True,\n",
    "        opacity=0.8,\n",
    "        stroked=True,\n",
    "        filled=True\n",
    "    )\n",
    "    \n",
    "    # Set initial view state to center on data\n",
    "    view_state = pdk.ViewState(\n",
    "        longitude=df['lon'].mean(),\n",
    "        latitude=df['lat'].mean(),\n",
    "        zoom=3,\n",
    "        pitch=0\n",
    "    )\n",
    "    \n",
    "    # Create tooltip\n",
    "    tooltip = {\n",
    "        \"html\": \"<b>ID:</b> {index}<br>\"\n",
    "    }\n",
    "    \n",
    "    # Add additional fields to tooltip if available\n",
    "    for col in ['capacity_mw', 'area_sqm', 'installation_date', 'source_dataset']:\n",
    "        if col in df.columns:\n",
    "            tooltip[\"html\"] += f\"<b>{col.replace('_', ' ').title()}:</b> {{{col}}}<br>\"\n",
    "    \n",
    "    if color_column:\n",
    "        tooltip[\"html\"] += f\"<b>{color_column.replace('_', ' ').title()}:</b> {{{color_column}}}\"\n",
    "    \n",
    "    # Create deck\n",
    "    deck = pdk.Deck(\n",
    "        layers=[layer],\n",
    "        initial_view_state=view_state,\n",
    "        tooltip=tooltip,\n",
    "        map_style='light'\n",
    "    )\n",
    "    \n",
    "    return deck\n",
    "\n",
    "def create_pydeck_polygons(gdf, color_column=None, extrusion_column=None, \n",
    "                          extrusion_scale=100, title=\"PV Installation Map\"):\n",
    "    \"\"\"\n",
    "    Create a 3D polygon map of PV installations using PyDeck.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gdf : GeoDataFrame\n",
    "        GeoDataFrame containing PV installation data with Polygon geometry\n",
    "    color_column : str, optional\n",
    "        Column name to use for polygon coloring\n",
    "    extrusion_column : str, optional\n",
    "        Column name to use for polygon height extrusion\n",
    "    extrusion_scale : float\n",
    "        Scaling factor for extrusion height\n",
    "    title : str\n",
    "        Title for the map\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pydeck.Deck\n",
    "        Interactive PyDeck map with polygon layer\n",
    "    \"\"\"\n",
    "    # Ensure the GeoDataFrame is in WGS84 (EPSG:4326)\n",
    "    if gdf.crs != \"EPSG:4326\":\n",
    "        gdf = gdf.to_crs(\"EPSG:4326\")\n",
    "    \n",
    "    # Filter to only include polygons\n",
    "    poly_gdf = gdf[gdf.geometry.geom_type.isin(['Polygon', 'MultiPolygon'])]\n",
    "    \n",
    "    if len(poly_gdf) == 0:\n",
    "        return \"Error: No polygon geometries found in the GeoDataFrame.\"\n",
    "    \n",
    "    # Convert to a format PyDeck can use\n",
    "    polygon_data = []\n",
    "    \n",
    "    for idx, row in poly_gdf.iterrows():\n",
    "        geom = row.geometry\n",
    "        \n",
    "        # Handle both Polygon and MultiPolygon\n",
    "        polygons = [geom] if geom.geom_type == 'Polygon' else list(geom.geoms)\n",
    "        \n",
    "        for poly in polygons:\n",
    "            # Extract exterior coordinates\n",
    "            exterior_coords = list(poly.exterior.coords)\n",
    "            coords = [[point[0], point[1]] for point in exterior_coords]\n",
    "            \n",
    "            # Create a feature for each polygon\n",
    "            feature = {\n",
    "                'polygon': coords,\n",
    "                'index': idx\n",
    "            }\n",
    "            \n",
    "            # Add additional properties\n",
    "            for col in poly_gdf.columns:\n",
    "                if col != 'geometry':\n",
    "                    feature[col] = row[col] if not pd.isna(row[col]) else None\n",
    "            \n",
    "            polygon_data.append(feature)\n",
    "    \n",
    "    # Create DataFrame from polygon data\n",
    "    df = pd.DataFrame(polygon_data)\n",
    "    \n",
    "    # Handle color mapping\n",
    "    if color_column and color_column in df.columns:\n",
    "        # Check if the column is numeric\n",
    "        if pd.api.types.is_numeric_dtype(df[color_column]):\n",
    "            color_scale = [\n",
    "                [0, [65, 182, 196]],\n",
    "                [0.33, [127, 205, 187]],\n",
    "                [0.66, [199, 233, 180]],\n",
    "                [1, [237, 248, 177]]\n",
    "            ]\n",
    "            \n",
    "            # Normalize the values\n",
    "            df['color_value'] = (df[color_column] - df[color_column].min()) / (df[color_column].max() - df[color_column].min())\n",
    "            \n",
    "            # Create a calculated color column using the scale\n",
    "            df['r'] = df['color_value'].apply(lambda x: int(\n",
    "                np.interp(x, [scale[0] for scale in color_scale], [scale[1][0] for scale in color_scale])\n",
    "            ))\n",
    "            df['g'] = df['color_value'].apply(lambda x: int(\n",
    "                np.interp(x, [scale[0] for scale in color_scale], [scale[1][1] for scale in color_scale])\n",
    "            ))\n",
    "            df['b'] = df['color_value'].apply(lambda x: int(\n",
    "                np.interp(x, [scale[0] for scale in color_scale], [scale[1][2] for scale in color_scale])\n",
    "            ))\n",
    "            \n",
    "            get_color = \"[r, g, b]\"\n",
    "        else:\n",
    "            # For categorical data, use hash of category for color\n",
    "            unique_cats = df[color_column].unique()\n",
    "            color_map = {cat: [int(h) % 256 for h in str(hash(str(cat)))[:3]] for cat in unique_cats}\n",
    "            df['r'] = df[color_column].map(lambda x: color_map[x][0])\n",
    "            df['g'] = df[color_column].map(lambda x: color_map[x][1])\n",
    "            df['b'] = df[color_column].map(lambda x: color_map[x][2])\n",
    "            \n",
    "            get_color = \"[r, g, b]\"\n",
    "    else:\n",
    "        # Default color: green for solar panels\n",
    "        get_color = \"[0, 128, 0]\"  # Green\n",
    "    \n",
    "    # Handle extrusion\n",
    "    if extrusion_column and extrusion_column in df.columns:\n",
    "        get_elevation = f\"{extrusion_column} * {extrusion_scale}\"\n",
    "    else:\n",
    "        get_elevation = str(extrusion_scale)\n",
    "    \n",
    "    # Create PolygonLayer\n",
    "    layer = pdk.Layer(\n",
    "        'PolygonLayer',\n",
    "        df,\n",
    "        get_polygon='polygon',\n",
    "        get_fill_color=get_color,\n",
    "        get_elevation=get_elevation,\n",
    "        elevation_scale=1,\n",
    "        extruded=True,\n",
    "        filled=True,\n",
    "        wireframe=True,\n",
    "        pickable=True,\n",
    "        opacity=0.6,\n",
    "        auto_highlight=True\n",
    "    )\n",
    "    \n",
    "    # Find center of polygons for the view\n",
    "    all_coords = []\n",
    "    for poly in df['polygon']:\n",
    "        all_coords.extend(poly)\n",
    "    \n",
    "    center_lon = np.mean([coord[0] for coord in all_coords])\n",
    "    center_lat = np.mean([coord[1] for coord in all_coords])\n",
    "    \n",
    "    # Set initial view state\n",
    "    view_state = pdk.ViewState(\n",
    "        longitude=center_lon,\n",
    "        latitude=center_lat,\n",
    "        zoom=10,\n",
    "        pitch=45,\n",
    "        bearing=0\n",
    "    )\n",
    "    \n",
    "    # Create tooltip\n",
    "    tooltip = {\n",
    "        \"html\": \"<b>ID:</b> {index}<br>\"\n",
    "    }\n",
    "    \n",
    "    # Add additional fields to tooltip if available\n",
    "    for col in ['capacity_mw', 'area_sqm', 'installation_date', 'source_dataset']:\n",
    "        if col in df.columns:\n",
    "            tooltip[\"html\"] += f\"<b>{col.replace('_', ' ').title()}:</b> {{{col}}}<br>\"\n",
    "    \n",
    "    if color_column:\n",
    "        tooltip[\"html\"] += f\"<b>{color_column.replace('_', ' ').title()}:</b> {{{color_column}}}<br>\"\n",
    "    \n",
    "    if extrusion_column:\n",
    "        tooltip[\"html\"] += f\"<b>{extrusion_column.replace('_', ' ').title()}:</b> {{{extrusion_column}}}\"\n",
    "    \n",
    "    # Create deck\n",
    "    deck = pdk.Deck(\n",
    "        layers=[layer],\n",
    "        initial_view_state=view_state,\n",
    "        tooltip=tooltip,\n",
    "        map_style='light'\n",
    "    )\n",
    "    \n",
    "    return deck\n",
    "\n",
    "def create_pydeck_heatmap(gdf, weight_column=None, intensity=1, radius=1000,\n",
    "                         title=\"PV Installation Heatmap\"):\n",
    "    \"\"\"\n",
    "    Create a heatmap of PV installations using PyDeck.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gdf : GeoDataFrame\n",
    "        GeoDataFrame containing PV installation data with geometry column\n",
    "    weight_column : str, optional\n",
    "        Column name to use for heatmap weighting\n",
    "    intensity : float\n",
    "        Intensity of the heatmap\n",
    "    radius : float\n",
    "        Radius of influence for each point (in meters)\n",
    "    title : str\n",
    "        Title for the map\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pydeck.Deck\n",
    "        Interactive PyDeck heatmap\n",
    "    \"\"\"\n",
    "    # Ensure the GeoDataFrame is in WGS84 (EPSG:4326)\n",
    "    if gdf.crs != \"EPSG:4326\":\n",
    "        gdf = gdf.to_crs(\"EPSG:4326\")\n",
    "    \n",
    "    # Convert to DataFrame with lat/lon columns\n",
    "    df = pd.DataFrame({\n",
    "        'lat': gdf.geometry.centroid.y,\n",
    "        'lon': gdf.geometry.centroid.x\n",
    "    })\n",
    "    \n",
    "    # Add weight column if specified\n",
    "    if weight_column and weight_column in gdf.columns:\n",
    "        df['weight'] = gdf[weight_column]\n",
    "        get_weight = 'weight'\n",
    "    else:\n",
    "        get_weight = 1\n",
    "    \n",
    "    # Create HeatmapLayer\n",
    "    layer = pdk.Layer(\n",
    "        'HeatmapLayer',\n",
    "        df,\n",
    "        get_position=['lon', 'lat'],\n",
    "        get_weight=get_weight,\n",
    "        pickable=False,\n",
    "        opacity=0.8,\n",
    "        radius_pixels=radius/100,  # Convert meters to pixels roughly\n",
    "        intensity=intensity,\n",
    "        threshold=0.05,\n",
    "        color_range=[\n",
    "            [1, 152, 189],\n",
    "            [73, 227, 206],\n",
    "            [216, 254, 181],\n",
    "            [254, 237, 177],\n",
    "            [254, 173, 84],\n",
    "            [209, 55, 78]\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Set initial view state to center on data\n",
    "    view_state = pdk.ViewState(\n",
    "        longitude=df['lon'].mean(),\n",
    "        latitude=df['lat'].mean(),\n",
    "        zoom=3,\n",
    "        pitch=0\n",
    "    )\n",
    "    \n",
    "    # Create deck\n",
    "    deck = pdk.Deck(\n",
    "        layers=[layer],\n",
    "        initial_view_state=view_state,\n",
    "        map_style='light'\n",
    "    )\n",
    "    \n",
    "    return deck"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f917a060",
   "metadata": {},
   "source": [
    "## Lonboard Visualization Functions\n",
    "\n",
    "Lonboard is a GPU-accelerated geospatial visualization library that's excellent for handling very large datasets. It's particularly useful for creating high-performance interactive visualizations of millions of data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1e5c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lonboard_map(gdf, color_column=None, size_column=None, size_scale=1,\n",
    "                       title=\"PV Installation Map\"):\n",
    "    \"\"\"\n",
    "    Create an interactive map of PV installations using Lonboard.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gdf : GeoDataFrame\n",
    "        GeoDataFrame containing PV installation data with geometry column\n",
    "    color_column : str, optional\n",
    "        Column name to use for point coloring\n",
    "    size_column : str, optional\n",
    "        Column name to use for point sizing\n",
    "    size_scale : float\n",
    "        Scaling factor for point size\n",
    "    title : str\n",
    "        Title for the map\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    lonboard.Map\n",
    "        Interactive Lonboard map\n",
    "    \"\"\"\n",
    "    # Ensure the GeoDataFrame is in WGS84 (EPSG:4326)\n",
    "    if gdf.crs != \"EPSG:4326\":\n",
    "        gdf = gdf.to_crs(\"EPSG:4326\")\n",
    "    \n",
    "    # Handle color mapping if specified\n",
    "    if color_column and color_column in gdf.columns:\n",
    "        color = gdf[color_column]\n",
    "    else:\n",
    "        color = None\n",
    "    \n",
    "    # Handle size mapping if specified\n",
    "    if size_column and size_column in gdf.columns:\n",
    "        size = gdf[size_column] * size_scale\n",
    "    else:\n",
    "        size = size_scale\n",
    "    \n",
    "    # Create the map\n",
    "    m = lonboard.Map()\n",
    "    \n",
    "    # Handle different geometry types\n",
    "    if all(gdf.geometry.geom_type.isin(['Point'])):\n",
    "        # For point geometries\n",
    "        m.add_layer(\n",
    "            lonboard.ScatterplotLayer(\n",
    "                gdf,\n",
    "                get_color=color,\n",
    "                get_radius=size,\n",
    "                opacity=0.8,\n",
    "                pickable=True,\n",
    "                auto_highlight=True\n",
    "            )\n",
    "        )\n",
    "    elif all(gdf.geometry.geom_type.isin(['Polygon', 'MultiPolygon'])):\n",
    "        # For polygon geometries\n",
    "        m.add_layer(\n",
    "            lonboard.GeoJsonLayer(\n",
    "                gdf,\n",
    "                get_fill_color=color,\n",
    "                get_line_color=[0, 0, 0, 200],\n",
    "                get_line_width=2,\n",
    "                opacity=0.8,\n",
    "                pickable=True,\n",
    "                auto_highlight=True\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        # For mixed geometries, convert to points (centroids) for simplicity\n",
    "        gdf_centroids = gdf.copy()\n",
    "        gdf_centroids['geometry'] = gdf_centroids.geometry.centroid\n",
    "        \n",
    "        m.add_layer(\n",
    "            lonboard.ScatterplotLayer(\n",
    "                gdf_centroids,\n",
    "                get_color=color,\n",
    "                get_radius=size,\n",
    "                opacity=0.8,\n",
    "                pickable=True,\n",
    "                auto_highlight=True\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    return m\n",
    "\n",
    "def create_lonboard_heatmap(gdf, weight_column=None, radius=1000,\n",
    "                          intensity=1, title=\"PV Installation Heatmap\"):\n",
    "    \"\"\"\n",
    "    Create a heatmap of PV installations using Lonboard.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gdf : GeoDataFrame\n",
    "        GeoDataFrame containing PV installation data with geometry column\n",
    "    weight_column : str, optional\n",
    "        Column name to use for heatmap weighting\n",
    "    radius : float\n",
    "        Radius of influence for each point (in meters)\n",
    "    intensity : float\n",
    "        Intensity of the heatmap\n",
    "    title : str\n",
    "        Title for the map\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    lonboard.Map\n",
    "        Interactive Lonboard heatmap\n",
    "    \"\"\"\n",
    "    # Ensure the GeoDataFrame is in WGS84 (EPSG:4326)\n",
    "    if gdf.crs != \"EPSG:4326\":\n",
    "        gdf = gdf.to_crs(\"EPSG:4326\")\n",
    "    \n",
    "    # Handle weight mapping if specified\n",
    "    if weight_column and weight_column in gdf.columns:\n",
    "        weight = gdf[weight_column]\n",
    "    else:\n",
    "        weight = 1\n",
    "    \n",
    "    # Get centroids for all geometries\n",
    "    gdf_centroids = gdf.copy()\n",
    "    if not all(gdf.geometry.geom_type.isin(['Point'])):\n",
    "        gdf_centroids['geometry'] = gdf_centroids.geometry.centroid\n",
    "    \n",
    "    # Create the map\n",
    "    m = lonboard.Map()\n",
    "    \n",
    "    # Add heatmap layer\n",
    "    m.add_layer(\n",
    "        lonboard.HeatmapLayer(\n",
    "            gdf_centroids,\n",
    "            get_weight=weight,\n",
    "            radius_pixels=int(radius/100),  # Convert meters to pixels roughly\n",
    "            intensity=intensity,\n",
    "            threshold=0.05,\n",
    "            color_range=[\n",
    "                [1, 152, 189, 255],\n",
    "                [73, 227, 206, 255],\n",
    "                [216, 254, 181, 255],\n",
    "                [254, 237, 177, 255],\n",
    "                [254, 173, 84, 255],\n",
    "                [209, 55, 78, 255]\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return m\n",
    "\n",
    "def create_lonboard_aggregation(gdf, resolution=8, color_scale='viridis',\n",
    "                              title=\"PV Installation Density\"):\n",
    "    \"\"\"\n",
    "    Create a hexbin aggregation map of PV installations using Lonboard.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gdf : GeoDataFrame\n",
    "        GeoDataFrame containing PV installation data with geometry column\n",
    "    resolution : int\n",
    "        Resolution of hexbins (higher = more detailed)\n",
    "    color_scale : str\n",
    "        Matplotlib colormap name for coloring\n",
    "    title : str\n",
    "        Title for the map\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    lonboard.Map\n",
    "        Interactive Lonboard hexbin aggregation map\n",
    "    \"\"\"\n",
    "    # Ensure the GeoDataFrame is in WGS84 (EPSG:4326)\n",
    "    if gdf.crs != \"EPSG:4326\":\n",
    "        gdf = gdf.to_crs(\"EPSG:4326\")\n",
    "    \n",
    "    # Get centroids for all geometries\n",
    "    gdf_centroids = gdf.copy()\n",
    "    if not all(gdf.geometry.geom_type.isin(['Point'])):\n",
    "        gdf_centroids['geometry'] = gdf_centroids.geometry.centroid\n",
    "    \n",
    "    # Create the map\n",
    "    m = lonboard.Map()\n",
    "    \n",
    "    # Add hexbin layer\n",
    "    m.add_layer(\n",
    "        lonboard.H3HexagonLayer(\n",
    "            gdf_centroids,\n",
    "            get_hex_id=lambda row: h3.geo_to_h3(row.geometry.y, row.geometry.x, resolution),\n",
    "            get_fill_color=\"colorScale\",\n",
    "            color_scale=color_scale,\n",
    "            opacity=0.8,\n",
    "            pickable=True,\n",
    "            auto_highlight=True\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa0ad17",
   "metadata": {},
   "source": [
    "## Example Usage\n",
    "\n",
    "Here are some examples of how to use these visualization functions with your processed PV datasets. After loading your geoparquet files into GeoDataFrames, you can use these functions to create interactive visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96e51af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage (commented out until you have processed your datasets)\n",
    "\"\"\"\n",
    "# Load a processed geoparquet file\n",
    "gdf = gpd.read_parquet('data/geoparquet/combined_pv_dataset.parquet')\n",
    "\n",
    "# Basic visualizations with each library\n",
    "# 1. Create a Folium cluster map\n",
    "folium_map = create_folium_cluster_map(gdf, zoom_start=2, title=\"Global PV Installations\")\n",
    "display(folium_map)\n",
    "\n",
    "# 2. Create a PyDeck 3D visualization\n",
    "if 'capacity_mw' in gdf.columns:\n",
    "    pydeck_map = create_pydeck_polygons(\n",
    "        gdf, \n",
    "        color_column='source_dataset',\n",
    "        extrusion_column='capacity_mw',\n",
    "        extrusion_scale=100,\n",
    "        title=\"3D PV Installation Capacity\"\n",
    "    )\n",
    "    display(pydeck_map)\n",
    "\n",
    "# 3. Create a Lonboard heatmap for large datasets\n",
    "lonboard_map = create_lonboard_heatmap(\n",
    "    gdf,\n",
    "    weight_column='area_sqm' if 'area_sqm' in gdf.columns else None,\n",
    "    radius=2000,\n",
    "    intensity=2,\n",
    "    title=\"Global PV Installation Density\"\n",
    ")\n",
    "display(lonboard_map)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8431be",
   "metadata": {},
   "source": [
    "## Advanced Visualization: Multi-layer Comparison\n",
    "\n",
    "For more sophisticated analysis, you might want to compare multiple datasets or visualize multiple attributes simultaneously. Here's an example of how to create a multi-layer visualization using PyDeck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fb1d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of multi-layer visualization (commented out until datasets are processed)\n",
    "\"\"\"\n",
    "def create_multi_dataset_comparison(gdfs_dict, base_color_scale=None):\n",
    "    '''\n",
    "    Create a multi-layer comparison of different PV datasets\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gdfs_dict : dict\n",
    "        Dictionary of {dataset_name: gdf} pairs\n",
    "    base_color_scale : list, optional\n",
    "        Base color scale to use for differentiation\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pydeck.Deck\n",
    "        Interactive PyDeck map with multiple layers\n",
    "    '''\n",
    "    if base_color_scale is None:\n",
    "        base_color_scale = [\n",
    "            [255, 0, 0],  # Red\n",
    "            [0, 255, 0],  # Green\n",
    "            [0, 0, 255],  # Blue\n",
    "            [255, 255, 0],  # Yellow\n",
    "            [255, 0, 255],  # Magenta\n",
    "            [0, 255, 255],  # Cyan\n",
    "        ]\n",
    "    \n",
    "    # Create layers list\n",
    "    layers = []\n",
    "    \n",
    "    # Track all coordinates to determine view center\n",
    "    all_lats = []\n",
    "    all_lons = []\n",
    "    \n",
    "    # Create a layer for each dataset with a unique color\n",
    "    for i, (name, gdf) in enumerate(gdfs_dict.items()):\n",
    "        # Ensure the GeoDataFrame is in WGS84 (EPSG:4326)\n",
    "        if gdf.crs != \"EPSG:4326\":\n",
    "            gdf = gdf.to_crs(\"EPSG:4326\")\n",
    "        \n",
    "        # Get color for this dataset\n",
    "        color_idx = i % len(base_color_scale)\n",
    "        color = base_color_scale[color_idx]\n",
    "        \n",
    "        # Convert to DataFrame with lat/lon columns\n",
    "        df = pd.DataFrame({\n",
    "            'lat': gdf.geometry.centroid.y,\n",
    "            'lon': gdf.geometry.centroid.x,\n",
    "            'dataset': name\n",
    "        })\n",
    "        \n",
    "        # Add additional columns from the original GeoDataFrame\n",
    "        for col in gdf.columns:\n",
    "            if col != 'geometry':\n",
    "                df[col] = gdf[col]\n",
    "        \n",
    "        # Create ScatterplotLayer for this dataset\n",
    "        layer = pdk.Layer(\n",
    "            'ScatterplotLayer',\n",
    "            df,\n",
    "            get_position=['lon', 'lat'],\n",
    "            get_radius=100,\n",
    "            get_fill_color=color + [180],  # Add alpha value\n",
    "            pickable=True,\n",
    "            opacity=0.8,\n",
    "            stroked=True,\n",
    "            filled=True,\n",
    "            id=f\"scatter-{name}\"  # Add ID for legend\n",
    "        )\n",
    "        \n",
    "        layers.append(layer)\n",
    "        \n",
    "        # Track coordinates\n",
    "        all_lats.extend(df['lat'].tolist())\n",
    "        all_lons.extend(df['lon'].tolist())\n",
    "    \n",
    "    # Set initial view state to center on all data\n",
    "    view_state = pdk.ViewState(\n",
    "        longitude=np.mean(all_lons),\n",
    "        latitude=np.mean(all_lats),\n",
    "        zoom=3,\n",
    "        pitch=0\n",
    "    )\n",
    "    \n",
    "    # Create tooltip\n",
    "    tooltip = {\n",
    "        \"html\": \"<b>Dataset:</b> {dataset}<br>\"\n",
    "    }\n",
    "    \n",
    "    # Create deck\n",
    "    deck = pdk.Deck(\n",
    "        layers=layers,\n",
    "        initial_view_state=view_state,\n",
    "        tooltip=tooltip,\n",
    "        map_style='light'\n",
    "    )\n",
    "    \n",
    "    return deck\n",
    "\n",
    "# After processing your datasets:\n",
    "# gdfs = {\n",
    "#    'Global PV Inventory': gpd.read_parquet('data/geoparquet/global_pv_inventory.parquet'),\n",
    "#    'USA PV Data': gpd.read_parquet('data/geoparquet/usa_pv_data.parquet'),\n",
    "#    'UK PV Data': gpd.read_parquet('data/geoparquet/uk_pv_data.parquet')\n",
    "# }\n",
    "# multi_comparison = create_multi_dataset_comparison(gdfs)\n",
    "# display(multi_comparison)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1326633e",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "These visualization functions provide a comprehensive toolkit for exploring and presenting your PV installation data. Each library has its strengths:\n",
    "\n",
    "- **Folium**: Best for quick interactive web maps with various basemaps and standard visualization types\n",
    "- **PyDeck**: Excellent for 3D visualizations and handling larger datasets with complex visualizations\n",
    "- **Lonboard**: Best performance for very large datasets with GPU acceleration\n",
    "\n",
    "You can customize these functions further based on your specific analysis needs and the attributes available in your processed datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eo-pv-cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
