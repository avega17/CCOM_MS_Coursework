{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73f645e0",
   "metadata": {},
   "source": [
    "# Fetch Open Datasets of PV locations\n",
    "\n",
    "Many of these datasets are located in [Zenodo](https://zenodo.org/), a general-purpose open-access repository developed under the European OpenAIRE program and operated by CERN. Others are hosted in figshare, a web-based platform for sharing research data and other types of content. The rest are hosted in GitHub repositories or other open-access platforms.The datasets are available in various formats, including CSV, GeoJSON, and shapefiles, and raster masks. We'll be using open-source Python libraries to download and process them into properly georeferenced geoparquet files\n",
    "that will serve as a base for our duckdb tables that we'll manage with dbt\n",
    "\n",
    "Here we list the dataset titles alongside their first author, DOI links, and their number of labels:\n",
    "- \"A solar panel dataset of very high resolution satellite imagery to support the Sustainable Development Goals\" - C. Clark et al, 2023 | [paper DOI](https://doi.org/10.1038/s41597-023-02539-8) | [dataset DOI](https://doi.org/10.6084/m9.figshare.22081091.v3) | 2,542 object labels (per spatial resolution)\n",
    "- \"A global inventory of photovoltaic solar energy generating units\" - L. Kruitwagen et al, 2021 | [paper DOI](https://doi.org/10.1038/s41586-021-03957-7) | [dataset DOI](https://doi.org/10.5281/zenodo.5005867) | 50,426 for training, cross-validation, and testing; 68,661 predicted polygon labels \n",
    "- \"A harmonised, high-coverage, open dataset of solar photovoltaic installations in the UK\" - D. Stowell et al, 2020 | [paper DOI](https://doi.org/10.1038/s41597-020-00739-0) | [dataset DOI](https://zenodo.org/records/4059881) | 265,418 data points (over 255,000 are stand-alone installations, 1067 solar farms, and rest are subcomponents within solar farms)\n",
    "- \"A crowdsourced dataset of aerial images with annotated solar photovoltaic arrays and installation metadata\" - G. Kasmi, 2023 | [paper DOI](https://doi.org/10.1038/s41597-023-01951-4) | [dataset DOI](https://doi.org/10.5281/zenodo.6865878) | > 28K points of PV installations; 13K+ segmentation masks for PV arrays; metadata for 8K+ installations\n",
    "- \"Georectified polygon database of ground-mounted large-scale solar photovoltaic sites in the United States\" - K. Sydny, 2023 | [paper DOI](https://doi.org/10.1038/s41597-023-02644-8) | [dataset DOI](https://www.sciencebase.gov/catalog/item/6671c479d34e84915adb7536) | 4186 data points (Note: these correspond to PV _facilities_ rather than individual panel arrays or objects and need filtering of duplicates with other datasets and further processing to extract the PV arrays in the facility)\n",
    "- \"Vectorized solar photovoltaic installation dataset across China in 2015 and 2020\" - J. Liu et al, 2024 | [paper DOI](https://doi.org/10.1038/s41597-024-04356-z) | [dataset link](https://github.com/qingfengxitu/ChinaPV) | 3,356 PV labels (inspect quality!)\n",
    "- \"Multi-resolution dataset for photovoltaic panel segmentation from satellite and aerial imagery\" - H. Jiang, 2021 | [paper DOI](https://doi.org/10.5194/essd-13-5389-2021) | [dataset DOI](https://doi.org/10.5281/zenodo.5171712) | 3,716 samples of PV data points\n",
    "- \"An Artificial Intelligence Dataset for Solar Energy Locations in India\" - A. Ortiz, 2022 | [paper DOI](https://doi.org/10.1038/s41597-022-01499-9) | [dataset link 1](https://researchlabwuopendata.blob.core.windows.net/solar-farms/solar_farms_india_2021.geojson) or [dataset link 2](https://raw.githubusercontent.com/microsoft/solar-farms-mapping/refs/heads/main/data/solar_farms_india_2021_merged_simplified.geojson) | 117 geo-referenced points of solar installations across India\n",
    "- \"GloSoFarID: Global multispectral dataset for Solar Farm IDentification in satellite imagery\" - Z. Yang, 2024 | [paper DOI](https://doi.org/10.48550/arXiv.2404.05180) | [dataset DOI](https://github.com/yzyly1992/GloSoFarID/tree/main/data_coordinates) | 6,793 PV samples across 3 years (double counting of samples)\n",
    "- \"Distributed solar photovoltaic array location and extent dataset for remote sensing object identification\" - K. Bradbury, 2016 | [paper DOI](https://doi.org/10.1038/sdata.2016.106) | [dataset DOI](https://doi.org/10.6084/m9.figshare.3385780.v4) | polygon annotations for 19,433 PV modules in 4 cities in California, USA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8564ba95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import Layout\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from branca.colormap import linear\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "import shapely\n",
    "import pygeohash\n",
    "import folium\n",
    "import lonboard\n",
    "import pydeck as pdk\n",
    "# import openeo \n",
    "# import pystac_client\n",
    "\n",
    "# import easystac\n",
    "# import cubo\n",
    "\n",
    "import duckdb as dd \n",
    "import datahugger\n",
    "import sciencebasepy\n",
    "from seedir import seedir\n",
    "\n",
    "# python libraries\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import urllib.parse\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import tempfile\n",
    "import shutil\n",
    "import pprint as pp\n",
    "import time\n",
    "import re\n",
    "from zipfile import ZipFile\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04686807",
   "metadata": {},
   "source": [
    "# Dataset Metadata and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3aad185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dict of metadata for datasets\n",
    "# this will be used for interactive widget and managing downloads\n",
    "\n",
    "# for maxar dataset\n",
    "# Catalogue ID 1040050029DC8C00; use to find geospatial extent coords\n",
    "# The geocoordinates for each solar panel object may be determined using the native resolution labels (found in the labels_native directory). \n",
    "# The center and width values for each object, along with the relative location information provided by the naming convention for each label, \n",
    "# may be used to determine the pixel coordinates for each object in the full, corresponding native resolution tile. \n",
    "# The pixel coordinates may be translated to geocoordinates using the EPSG:32633 coordinate system and the following geotransform for each tile:\n",
    "\n",
    "# Tile 1: (307670.04, 0.31, 0.0, 5434427.100000001, 0.0, -0.31)\n",
    "# Tile 2: (312749.07999999996, 0.31, 0.0, 5403952.860000001, 0.0, -0.31)\n",
    "# Tile 3: (312749.07999999996, 0.31, 0.0, 5363320.540000001, 0.0, -0.31)\n",
    "# see here on gdal format geotransform: https://gdal.org/en/stable/tutorials/geotransforms_tut.html\n",
    "\n",
    "# look into adding dataset crs or projection to metadata dict\n",
    "# note that most of these details are hardcoded and difficult to parse ahead of time\n",
    "# load environment variables\n",
    "load_dotenv()\n",
    "DATASET_DIR = Path(os.getenv('DATA_PATH'))\n",
    "dataset_metadata = {\n",
    "    'deu_maxar_vhr_2023': {\n",
    "        'doi': '10.6084/m9.figshare.22081091.v3',\n",
    "        'repo': 'figshare',\n",
    "        'compression': 'zip',\n",
    "        'label_fmt': 'yolo_fmt_txt',\n",
    "        'has_imgs': False,\n",
    "        'label_count': 2542 # solar panel objects (ie not individual panels)\n",
    "    },\n",
    "    'uk_crowdsourced_pv_2020': {\n",
    "        'doi': '10.5281/zenodo.4059881',\n",
    "        'repo': 'zenodo',\n",
    "        'compression': None,\n",
    "        'label_fmt': 'geojson',\n",
    "        'geom_type': {'features': ['Point', 'Polygon', 'MultiPolygon']},\n",
    "        'crs': None, # default to WGS84 when processing\n",
    "        'has_imgs': False,\n",
    "        'label_count': 265418\n",
    "    },\n",
    "    # note for report later: Maxar Technologies (MT) was primarily used to determine the extent of solar arrays\n",
    "    'usa_eia_large_scale_pv_2023': {\n",
    "        'doi': '10.5281/zenodo.8038684',\n",
    "        'repo': 'sciencebase',\n",
    "        'compression': 'zip',\n",
    "        'label_fmt': 'shapefile',\n",
    "        'has_imgs': False,\n",
    "        'label_count': 4186\n",
    "    },\n",
    "    'chn_med_res_pv_2024': {\n",
    "        # using github files since zenodo shapefiles fail to load in QGIS\n",
    "        'doi': 'https://github.com/qingfengxitu/ChinaPV/tree/main',\n",
    "        'repo': 'github',\n",
    "        'compression': None,\n",
    "        'label_fmt': 'shapefile',\n",
    "        'has_imgs': False,\n",
    "        'label_count': 3356\n",
    "    },\n",
    "    'usa_cali_usgs_pv_2016': {\n",
    "        'doi': '10.6084/m9.figshare.3385780.v4',\n",
    "        'repo': 'figshare',\n",
    "        'compression': None,\n",
    "        'label_fmt': 'geojson',\n",
    "        'crs': 'NAD83',\n",
    "        'geom_type': {'features': 'Polygon'},\n",
    "        'has_imgs': False,\n",
    "        'label_count': 19433\n",
    "    },\n",
    "    'chn_jiangsu_vhr_pv_2021': {\n",
    "        'doi': '10.5281/zenodo.5171712',\n",
    "        'repo': 'zenodo',\n",
    "        'compression': 'zip',\n",
    "        # look into geotransform details for processing these labels\n",
    "        'label_fmt': 'pixel_mask',\n",
    "        'has_imgs': True,\n",
    "        'label_count': 3716\n",
    "    },\n",
    "    'ind_pv_solar_farms_2022': {\n",
    "        'doi': 'https://raw.githubusercontent.com/microsoft/solar-farms-mapping/refs/heads/main/data/solar_farms_india_2021_merged_simplified.geojson',\n",
    "        'repo': 'github',\n",
    "        'compression': None,\n",
    "        'label_fmt': 'geojson',\n",
    "        'geom_type': {'features': 'MultiPolygon'}, \n",
    "        'crs': 'WGS84',\n",
    "        'has_imgs': False,\n",
    "        'label_count': 117\n",
    "    },\n",
    "    'fra_west_eur_pv_installations_2023': {\n",
    "        'doi': '10.5281/zenodo.6865878',\n",
    "        'repo': 'zenodo',\n",
    "        'compression': 'zip',\n",
    "        'label_fmt': 'json',\n",
    "        'geom_type': {'Polygon': ['Point']},\n",
    "        'crs': None, \n",
    "        'has_imgs': True, \n",
    "        'label_count': (13303, 7686)\n",
    "    },\n",
    "    'global_pv_inventory_sent2_spot_2021': {\n",
    "        'doi': '10.5281/zenodo.5005867',\n",
    "        'repo': 'zenodo',\n",
    "        'compression': None,\n",
    "        'label_fmt': 'geojson',\n",
    "        'geom_type': ['Polygon'],\n",
    "        'crs': 'WGS84',\n",
    "        'has_imgs': False,\n",
    "        'label_count': 50426\n",
    "    },\n",
    "    'global_pv_inventory_sent2_2024': {\n",
    "        'doi': 'https://github.com/yzyly1992/GloSoFarID/tree/main/data_coordinates',\n",
    "        'repo': 'github',\n",
    "        'compression': None,\n",
    "        'label_fmt': 'json',\n",
    "        'crs': None, # default to WGS84 when processing\n",
    "        'geom_type': ['Point'], # normal json with no geometry attribute\n",
    "        'has_imgs': True, \n",
    "        'label_count': 6793\n",
    "    }\n",
    "\n",
    "}\n",
    "\n",
    "dataset_choices = [\n",
    "    # 'global_pv_inventory_sent2_2024',\n",
    "    'global_pv_inventory_sent2_spot_2021',\n",
    "    # 'fra_west_eur_pv_installations_2023',\n",
    "    'ind_pv_solar_farms_2022',\n",
    "    'usa_cali_usgs_pv_2016',\n",
    "    # 'chn_med_res_pv_2024',\n",
    "    # 'usa_eia_large_scale_pv_2023',\n",
    "    # 'uk_crowdsourced_pv_2020',\n",
    "    # 'deu_maxar_vhr_2023'   \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6162a17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store selected datasets\n",
    "# mostly gen by github copilot with Claude 3.7 model\n",
    "selected_datasets = dataset_choices.copy()\n",
    "\n",
    "def format_dataset_info(dataset):\n",
    "    \"\"\"Create a formatted HTML table for dataset metadata\"\"\"\n",
    "    metadata = dataset_metadata[dataset]\n",
    "    \n",
    "    # Create table with metadata\n",
    "    html = f\"\"\"\n",
    "    <style>\n",
    "    .dataset-table {{\n",
    "        border-collapse: collapse;\n",
    "        width: 30%;\n",
    "        margin: 20px auto;\n",
    "        font-family: Arial, sans-serif;\n",
    "    }}\n",
    "    .dataset-table th, .dataset-table td {{\n",
    "        border: 1px solid #ddd;\n",
    "        padding: 8px;\n",
    "        text-align: left;\n",
    "    }}\n",
    "    .dataset-table th {{\n",
    "        background-color: #f2f2f2;\n",
    "        font-weight: bold;\n",
    "    }}\n",
    "    </style>\n",
    "    <table class=\"dataset-table\">\n",
    "        <tr><th>Metadata</th><th>Value</th></tr>\n",
    "        <tr><td>DOI/URL</td><td>{metadata['doi']}</td></tr>\n",
    "        <tr><td>Repository</td><td>{metadata['repo']}</td></tr>\n",
    "        <tr><td>Compression</td><td>{metadata['compression'] or 'None'}</td></tr>\n",
    "        <tr><td>Label Format</td><td>{metadata['label_fmt']}</td></tr>\n",
    "        <tr><td>Has Images</td><td>{metadata['has_imgs']}</td></tr>\n",
    "        <tr><td>Label Count</td><td>{metadata.get('label_count', 'Unknown')}</td></tr>\n",
    "    </table>\n",
    "    \"\"\"\n",
    "    return html\n",
    "\n",
    "# Create an accordion to display selected datasets with centered layout\n",
    "dataset_accordion = widgets.Accordion(\n",
    "    children=[widgets.HTML(format_dataset_info(ds)) for ds in selected_datasets],\n",
    "    layout=Layout(width='50%', margin='0 auto')\n",
    ")\n",
    "for i, ds in enumerate(selected_datasets):\n",
    "    dataset_accordion.set_title(i, ds)\n",
    "\n",
    "# Define a function to add or remove datasets\n",
    "def manage_datasets(action, dataset=None):\n",
    "    global selected_datasets, dataset_accordion\n",
    "    \n",
    "    if action == 'add' and dataset and dataset not in selected_datasets:\n",
    "        selected_datasets.append(dataset)\n",
    "    elif action == 'remove' and dataset and dataset in selected_datasets:\n",
    "        selected_datasets.remove(dataset)\n",
    "    \n",
    "    # Update the accordion with current selections\n",
    "    dataset_accordion.children = [widgets.HTML(format_dataset_info(ds)) for ds in selected_datasets]\n",
    "    for i, ds in enumerate(selected_datasets):\n",
    "        dataset_accordion.set_title(i, ds)\n",
    "    \n",
    "    f\"Currently selected datasets: {len(selected_datasets)}\"\n",
    "\n",
    "# Create dropdown for available datasets\n",
    "dataset_dropdown = widgets.Dropdown(\n",
    "    options=list(dataset_metadata.keys()),\n",
    "    description='Dataset:',\n",
    "    disabled=False,\n",
    "    layout=Layout(width='70%', margin='20 20 auto 20 20')\n",
    ")\n",
    "\n",
    "# Create buttons for actions\n",
    "add_button = widgets.Button(description=\"Add Dataset\", button_style='success')\n",
    "remove_button = widgets.Button(description=\"Remove Dataset\", button_style='danger')\n",
    "\n",
    "# Define button click handlers\n",
    "def on_add_clicked(b):\n",
    "    manage_datasets('add', dataset_dropdown.value)\n",
    "\n",
    "def on_remove_clicked(b):\n",
    "    manage_datasets('remove', dataset_dropdown.value)\n",
    "\n",
    "# Link buttons to handlers\n",
    "add_button.on_click(on_add_clicked)\n",
    "remove_button.on_click(on_remove_clicked)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f44fde4",
   "metadata": {},
   "source": [
    "## Dataset Selection Interface\n",
    "#### Use the dropdown and buttons below to customize which solar panel datasets will be fetched and processed.\n",
    "- Select a dataset from the dropdown:\n",
    "    - Click \"Add Dataset\" to include it in processing\n",
    "    - Click \"Remove Dataset\" to exclude it\n",
    "- View metadata table in the selected dataset's dropdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb943c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the widgets\n",
    "display(widgets.HBox([dataset_dropdown, add_button, remove_button]))\n",
    "display(dataset_accordion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a78984",
   "metadata": {},
   "source": [
    "# Fetching and Organizing datasets for later-preprocessing\n",
    "\n",
    "We will use [datahugger](https://j535d165.github.io/datahugger/) to fetch datasets hosted in Zenodo, figshare, and GitHub. \n",
    "\n",
    "We will sciencebase for the dataset hosted in the USGS ScienceBase Catalog.\n",
    "We will pre-process and convert datasets into geojson, if not already formatted, and manage these using [geopandas](https://geopandas.org/). These will be further processed into geoparquet files for use in duckdb tables used to manage and later consolidate the datasets with dbt.  \n",
    "- The datasets will be stored in the `data/` directory\n",
    "    - the geoparquet files will be stored in the `data/geoparquet/` directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f56a89",
   "metadata": {},
   "source": [
    "#### Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86cf8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# move to utility functions later\n",
    "# def fetch_github_repo_files(dataset_name, \n",
    "\n",
    "\n",
    "# use the metadata to fetch the dataset files using datahugger\n",
    "def fetch_dataset_files(dataset_name, max_mb=100, force=False):\n",
    "    metadata = dataset_metadata[dataset_name]\n",
    "    doi = metadata['doi']\n",
    "    repo = metadata['repo']\n",
    "    compression = metadata['compression']\n",
    "    label_fmt = metadata['label_fmt']\n",
    "    # convert to bytes\n",
    "    max_dl = max_mb * 1024 * 1024\n",
    "    dataset_dir = os.path.join(os.getenv('DATA_PATH'), 'raw', 'labels', dataset_name)\n",
    "    geofile_regex = r'^(.*\\.(geojson|json|shp|zip|csv))$'\n",
    "    dst = os.path.join(os.getcwd(), dataset_dir)\n",
    "    dst_p = Path(dst)\n",
    "\n",
    "    # prettyprint metadata and dst info\n",
    "    # pp.pprint(metadata)\n",
    "    # print(f\"Destination: {dataset_dir}\")\n",
    "    # print(f\"Max download size: {max_mb} MB\")\n",
    "    # print(f\"Force Download: {force}\")\n",
    "\n",
    "    dataset_tree = {}\n",
    "\n",
    "    # TODO: move different repo handling to separate functions\n",
    "\n",
    "    # use datahugger to fetch files from most repos\n",
    "    if repo in ['figshare', 'zenodo']:\n",
    "\n",
    "        ds_tree = datahugger.get(doi, dst, max_file_size=max_dl, force_download=force)\n",
    "        # compare files to be fetched (after filtering on max file size) with existing files  \n",
    "        files_to_fetch = [f['name'] for f in ds_tree.dataset.files if f['size'] <= max_dl]\n",
    "        ds_files = [os.path.join(root, fname) for root, dirs, files in os.walk(dst_p) for fname in files if re.match(geofile_regex, fname)]\n",
    "        # flag for avoiding extracting zip when already extracted\n",
    "        # is_unzipped = all(f in ds_files for f in files_to_fetch) and len(ds_files) > 1\n",
    "        # TODO: handle .zip files that consist of a redundant copy of the entire dataset\n",
    "        if metadata['compression'] == 'zip' and any(f.endswith('.zip') for f in ds_files):\n",
    "            print(f\"Dataset metadata for {dataset_name} indicates handling of one or more downloaded zip files.\")\n",
    "            # check if the zip file was fetched and directly extract if it's the only file in the dataset\n",
    "            extracted_files = []\n",
    "            if len(ds_files) <= 2 and ds_files[0].endswith('.zip'):\n",
    "                zip_file = dst_p / ds_files[0]\n",
    "                # print(f\"Found single zip file for dataset: {zip_file}\")\n",
    "                # extract the zip file and delete it \n",
    "                with ZipFile(zip_file, 'r') as zip_ref:\n",
    "                    extracted_files = zip_ref.namelist()\n",
    "                    zip_ref.extractall(dst)\n",
    "                \n",
    "                # remove the zip file\n",
    "                # try:\n",
    "                #     os.remove(zip_file)\n",
    "                #     print(f\"Removed {os.path.relpath(zip_file)} after extraction\")\n",
    "                # except Exception as e:\n",
    "                #     print(f\"Error removing {zip_file}: {e}\")\n",
    "                # check if zip file consisted of a single dir and move contents up one level\n",
    "                top_level_dir = dst_p / extracted_files[0]\n",
    "                if top_level_dir.is_dir():\n",
    "                    # move only first level dirs and files to our dataset dir\n",
    "                    for item in top_level_dir.iterdir():\n",
    "                        if item.name.endswith('.zip'):\n",
    "                            continue\n",
    "                        # don't copy if already exists and is non-empty\n",
    "                        # TODO: add non-empty check\n",
    "                        elif os.path.exists(dst_p / item.name):\n",
    "                            print(f\"Skipping {item} as it already exists in {os.path.relpath(dst)}\")\n",
    "                            continue\n",
    "                        elif item.parent == top_level_dir:\n",
    "                            print(f\"Moving {item} to {os.path.relpath(dst)}\")\n",
    "                            shutil.move(item, dst)\n",
    "                    # remove the top level dir\n",
    "                    shutil.rmtree(top_level_dir)\n",
    "\n",
    "                ds_files = [os.path.join(root, fname) for root, dirs, files in os.walk(dst_p) for fname in files if re.match(geofile_regex, fname)]\n",
    "                print(f\"Moved items from {os.path.relpath(top_level_dir)} to:\\n{os.path.relpath(dst_p)}\")\n",
    "                print(f\"After extraction and moving, we have {len(ds_files)} files in {os.path.relpath(dst)}:\\n{ds_files}\")\n",
    "\n",
    "            elif len(ds_files) > 2:\n",
    "                # multiple files in addition to the zip file; handle on case by case basis\n",
    "                print(f\"Multiple files found in {dst_p}:\\n{os.listdir(dst_p)}\")\n",
    "        # no further processing needed; get file list directly from datahugger\n",
    "        else: \n",
    "            ds_files = [os.path.join(root, fname) for root, dirs, files in os.walk(dst_p) for fname in files if re.match(geofile_regex, fname)]\n",
    "\n",
    "\n",
    "        dataset_tree = {\n",
    "            'dataset': dataset_name,\n",
    "            'output_dir': ds_tree.output_folder,\n",
    "            'files': ds_files,\n",
    "            'fs_tree': seedir(dst_p, depthlimit=5, printout=False, regex=True, include_files=geofile_regex)\n",
    "        }\n",
    "\n",
    "    elif repo == 'github':\n",
    "        # Handle GitHub repositories using git partial cloning of repo \n",
    "        \n",
    "        # Create destination directory if it doesn't exist\n",
    "        os.makedirs(dst, exist_ok=True)\n",
    "        # Parse the GitHub URL\n",
    "        # [user, repo, tree, branch, rest of path]\n",
    "        parts = doi.replace('https://github.com/', '').split('/')\n",
    "        repo_path = f\"{parts[0]}/{parts[1]}\"\n",
    "        \n",
    "        # Extract branch and path\n",
    "        branch = 'main'  # Default branch\n",
    "        path = ''\n",
    "        \n",
    "        # check if local path exists and contains expected files\n",
    "        if os.path.exists(dst) and any(os.path.splitext(fname)[1] in ['.geojson', '.json', '.shp', '.zip'] for fname in os.listdir(dst)) and not force:  \n",
    "            print(f\"Destination path for {dataset_name}'s repo already exists and contains expected files.\")\n",
    "            # print in bold\n",
    "            print(f\"\\033[1mSkipping Download!\\033[0m\")\n",
    "            # fetch dataset dir info from Pathlib and tree from seedir \n",
    "            tree = seedir(dst_p, depthlimit=5, printout=False, regex=True, include_files=geofile_regex)\n",
    "            # get list of files in Path object that satisfy regex\n",
    "            ds_files = [os.path.join(root, fname) for root, dirs, files in os.walk(dst_p) for fname in files if re.match(geofile_regex, fname)]\n",
    "            dataset_tree = {\n",
    "                'dataset': dataset_name,\n",
    "                'output_dir': dst,\n",
    "                'files': ds_files,\n",
    "                'fs_tree': tree\n",
    "            }\n",
    "\n",
    "        # Check if it's a folder/repository or a single file\n",
    "        elif '/blob/' not in doi and 'raw.githubusercontent.com' not in doi:\n",
    "            try:\n",
    "                if 'tree' in parts:\n",
    "                    tree_index = parts.index('tree')\n",
    "                    branch = parts[tree_index + 1]\n",
    "                    path = '/'.join(parts[tree_index + 2:]) if len(parts) > tree_index + 2 else ''\n",
    "                \n",
    "                # Create a temporary directory for the sparse checkout\n",
    "                with tempfile.TemporaryDirectory() as temp_dir:\n",
    "                    # Initialize the git repository and set up sparse checkout\n",
    "                    commands = [f\"git clone --filter=blob:limit={max_mb}m --depth 1 https://github.com/{repo_path}.git {dataset_name}\"]\n",
    "                    # print(f\"Running commands: {commands}\")\n",
    "                    # Execute git commands\n",
    "                    for cmd in commands:\n",
    "                        \n",
    "                        process = subprocess.run(cmd, shell=True, cwd=temp_dir, \n",
    "                                               capture_output=True, text=True)\n",
    "                        # show command output (debug)\n",
    "                        print(f\"Command stdout: {process.stdout}\")\n",
    "                        if process.returncode != 0:\n",
    "                            raise Exception(f\"Git command failed: {cmd}\\n{process.stderr}\")\n",
    "                    \n",
    "                    # Copy only the files in the dir specified in DOI/URL\n",
    "                    repo_ds_dir = os.path.join(temp_dir, dataset_name, path) if path else os.path.join(temp_dir, dataset_name)\n",
    "                    files_list = []\n",
    "                    #\n",
    "                    for root, _, files in os.walk(repo_ds_dir):\n",
    "                        for file in files:\n",
    "                            if file.startswith('.git'):\n",
    "                                continue\n",
    "                            src_file = os.path.join(root, file)\n",
    "                            # Create relative path\n",
    "                            rel_path = os.path.relpath(src_file, repo_ds_dir)\n",
    "                            dst_file = os.path.join(dst, rel_path)\n",
    "                            \n",
    "                            # Create destination directory if needed\n",
    "                            os.makedirs(os.path.dirname(dst_file), exist_ok=True)\n",
    "                            \n",
    "                            # Copy the file\n",
    "                            shutil.copy2(src_file, dst_file)\n",
    "                            files_list.append(dst_file)\n",
    "                            print(f\"Copied {rel_path} to ./{dataset_dir}/{rel_path}\")\n",
    "\n",
    "                dataset_tree = {\n",
    "                    'dataset': dataset_name,\n",
    "                    'output_dir': dst,\n",
    "                    'files': files_list,\n",
    "                    'fs_tree': seedir(dst_p, depthlimit=5, printout=False, regex=True, include_files=geofile_regex)\n",
    "                }\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error performing git clone: {e}\")\n",
    "                return None\n",
    "        else:\n",
    "            # It's a single file (raw URL or blob URL)\n",
    "            try:\n",
    "                # Convert blob URL to raw URL if needed\n",
    "                if '/blob/' in doi:\n",
    "                    raw_url = doi.replace('github.com', 'raw.githubusercontent.com').replace('/blob/', '/')\n",
    "                else:\n",
    "                    raw_url = doi\n",
    "                \n",
    "                # Extract filename from URL\n",
    "                filename = os.path.basename(urllib.parse.urlparse(raw_url).path)\n",
    "                local_file_path = os.path.join(dst, filename)\n",
    "                \n",
    "                # Download the file\n",
    "                response = requests.get(raw_url, stream=True)\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                # Check file size\n",
    "                file_size = int(response.headers.get('content-length', 0))\n",
    "                if file_size > max_dl:\n",
    "                    print(f\"File size ({file_size} bytes) exceeds maximum allowed size ({max_dl * 1024 * 1024} MB)\")\n",
    "                    return None\n",
    "                \n",
    "                with open(local_file_path, 'wb') as f:\n",
    "                    for chunk in tqdm(response.iter_content(chunk_size=8192), desc=f\"Downloading {filename}\", unit='KB'):\n",
    "                        f.write(chunk)\n",
    "                print(f\"Downloaded {filename} to {os.path.relpath(local_file_path)}\")\n",
    "                dataset_tree = {\n",
    "                    'dataset': dataset_name,\n",
    "                    'output_dir': dst,\n",
    "                    'files': [local_file_path],\n",
    "                    'fs_tree': ds_tree.tree()\n",
    "                }\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error downloading GitHub file: {e}\")\n",
    "\n",
    "    elif repo == 'sciencebase':\n",
    "        # Initialize ScienceBase client\n",
    "        # sb = sciencebasepy.SbSession()\n",
    "        \n",
    "        # # Extract the item ID from the DOI or URL\n",
    "        # # DOIs like 10.5281/zenodo.8038684 or URLs with item ID\n",
    "        # item_id = doi.split('/')[-1] if '/' in doi else doi\n",
    "        \n",
    "        # try:\n",
    "        #     # Get item details\n",
    "        #     item = sb.get_item(item_id)\n",
    "            \n",
    "        #     # Create destination directory\n",
    "        #     os.makedirs(dst, exist_ok=True)\n",
    "            \n",
    "        #     # Download all files associated with the item\n",
    "        #     downloaded_files = []\n",
    "            \n",
    "        #     # Get item files\n",
    "        #     files = sb.get_item_file_info(item_id)\n",
    "            \n",
    "        #     for file_info in files:\n",
    "        #         file_name = file_info['name']\n",
    "        #         file_url = file_info['url']\n",
    "                \n",
    "        #         # Check file size if available\n",
    "        #         if 'size' in file_info and file_info['size'] > max_dl:\n",
    "        #             print(f\"Skipping file {file_name} as it exceeds the maximum download size\")\n",
    "        #             continue\n",
    "                \n",
    "        #         # Download the file\n",
    "        #         local_file_path = os.path.join(dst, file_name)\n",
    "        #         sb.download_file(file_url, local_file_path)\n",
    "                \n",
    "        #         downloaded_files.append(local_file_path)\n",
    "        #         print(f\"Downloaded {file_name} to {local_file_path}\")\n",
    "        print(\"Not Implemented yet\")\n",
    "        return None\n",
    "\n",
    "    print(f\"Fetched {len(dataset_tree['files'])} dataset files for {dataset_name} in {os.path.relpath(dataset_tree['output_dir'])}:\")\n",
    "    print(dataset_tree['fs_tree'])\n",
    "\n",
    "    return dataset_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d311f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check datahugger get arguments\n",
    "# print(datahugger.get.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b2c58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_ds = selected_datasets[2]\n",
    "# test_doi = dataset_metadata[test_ds]['doi']\n",
    "# max_mb = 500\n",
    "# dst_dir = os.path.join(os.getcwd(), os.getenv('DATA_PATH'), 'raw', 'labels', test_ds)\n",
    "# t = datahugger.get(test_doi, dst_dir, print_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78eece7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through the selected datasets and fetch files\n",
    "# iterate through the selected datasets and fetch files\n",
    "ds_trees = {}\n",
    "max_mb = int(os.getenv('MAX_LABEL_MB', 100))\n",
    "print(f\"Fetching {len(selected_datasets)} datasets with files of max size {max_mb} MB\")\n",
    "\n",
    "# Create widgets for controlling the fetching process\n",
    "fetch_output = widgets.Output(\n",
    "    layout=widgets.Layout(\n",
    "        width='80%', \n",
    "        border='1px solid #ddd', \n",
    "        padding='10px',\n",
    "        overflow='auto'\n",
    "    )\n",
    ")\n",
    "# Apply direct CSS styling for text wrapping (Note: unvalidated)\n",
    "display(widgets.HTML(\"\"\"\n",
    "<style>\n",
    ".jupyter-widgets-output-area pre {\n",
    "    white-space: pre-wrap !important;       /* CSS3 */\n",
    "    word-wrap: break-word !important;        /* Internet Explorer 5.5+ */\n",
    "    overflow-wrap: break-word !important;\n",
    "    max-width: 100%;\n",
    "}\n",
    "</style>\n",
    "\"\"\"))\n",
    "control_panel = widgets.VBox(layout=widgets.Layout(width='20%', padding='10px', overflow='auto', word_wrap='break-word'))\n",
    "fetch_button = widgets.Button(description=\"Fetch Next Dataset\", button_style=\"primary\")\n",
    "progress_label = widgets.HTML(\"Waiting to start...\")\n",
    "dataset_index = 0\n",
    "\n",
    "# Function to fetch the next dataset\n",
    "def fetch_next_dataset(button=None):\n",
    "    global dataset_index\n",
    "    global dataset_metadata\n",
    "    \n",
    "    if dataset_index >= len(selected_datasets):\n",
    "        with fetch_output:\n",
    "            print(\"All datasets have been fetched!\")\n",
    "            progress_label.value = f\"<b>Completed:</b> {dataset_index}/{len(selected_datasets)} datasets\"\n",
    "        fetch_button.disabled = True\n",
    "        return\n",
    "    \n",
    "    dataset = selected_datasets[dataset_index]\n",
    "    progress_label.value = f\"<b>Fetching:</b> {dataset_index+1}/{len(selected_datasets)}<br><b>Current:</b> {dataset}\"\n",
    "    \n",
    "    with fetch_output:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Fetching dataset files for {dataset} using DOI/URL:\\n {dataset_metadata[dataset]['doi']}\")\n",
    "        ds_tree = fetch_dataset_files(dataset, max_mb=max_mb, force=force_download_checkbox.value)\n",
    "        \n",
    "        if ds_tree:\n",
    "            ds_trees[dataset] = ds_tree\n",
    "            # update metadata dict with local filesystem info\n",
    "            dataset_metadata[dataset]['output_dir'] = ds_tree['output_dir']\n",
    "            dataset_metadata[dataset]['files'] = ds_tree['files']\n",
    "            dataset_metadata[dataset]['tree'] = ds_tree['fs_tree']\n",
    "            # print the dataset file tree\n",
    "        else:\n",
    "            print(f\"Failed to fetch dataset {dataset}\")\n",
    "    \n",
    "    dataset_index += 1\n",
    "    progress_label.value = f\"<b>Completed:</b> {dataset_index}/{len(selected_datasets)}<br><b>Next:</b> {selected_datasets[dataset_index] if dataset_index < len(selected_datasets) else 'Done'}\"\n",
    "\n",
    "# Add a checkbox for force download option\n",
    "force_download_checkbox = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='Force Download',\n",
    "    tooltip='If checked, download will be forced even if files exist locally',\n",
    "    layout=widgets.Layout(width='auto')\n",
    ")\n",
    "\n",
    "# Configure the button callback\n",
    "fetch_button.on_click(fetch_next_dataset)\n",
    "\n",
    "# Create the control panel\n",
    "dataset_progress = widgets.HTML(f\"Datasets selected: {len(selected_datasets)}\")\n",
    "fetch_status = widgets.HTML(\n",
    "    f\"Status: Ready to begin\",\n",
    "    layout=widgets.Layout(margin=\"10px 0\")\n",
    ")\n",
    "\n",
    "# Create the control panel with left alignment\n",
    "control_panel.children = [\n",
    "    widgets.HTML(\"<h3 style='align:left;'>Fetch Control</h3>\"), \n",
    "    dataset_progress,\n",
    "    force_download_checkbox,\n",
    "    widgets.HTML(\"<hr style='margin:10px 0'>\"),\n",
    "    progress_label,\n",
    "    fetch_button\n",
    "]\n",
    "\n",
    "# Add custom CSS to ensure alignment\n",
    "display(widgets.HTML(\"\"\"\n",
    "<style>\n",
    ".widget-html {\n",
    "    text-align: left !important;\n",
    "}\n",
    ".widget-checkbox {\n",
    "    justify-content: flex-start !important;\n",
    "}\n",
    ".widget-button {\n",
    "    width: 100% !important;\n",
    "}\n",
    "</style>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ddc7a4",
   "metadata": {},
   "source": [
    "#### Fetching selected datasets and visualizing metadata and file structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e46bbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the widget layout\n",
    "display(widgets.HBox([fetch_output, control_panel]))\n",
    "\n",
    "# Set up for first fetch\n",
    "if selected_datasets:\n",
    "    progress_label.value = f\"<b>Ready to start:</b><br>First dataset: {selected_datasets[0]}\"\n",
    "else:\n",
    "    progress_label.value = \"<b>No datasets selected</b>\"\n",
    "    fetch_button.disabled = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7659bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep subset of metadata dict for selected datasets\n",
    "selected_metadata = {ds: dataset_metadata[ds] for ds in selected_datasets}\n",
    "get_ds_files = lambda ds: dataset_metadata[ds]['files']\n",
    "get_ds_dir = lambda ds: dataset_metadata[ds]['output_dir']\n",
    "fra_ds_folder = 'replication'\n",
    "# make a manual selection of the set of files we'll use from each dataset\n",
    "selected_ds_files = {\n",
    "    # 'global_pv_inventory_sent2_2024':\n",
    "        # [f for f in get_ds_files('global_pv_inventory_sent2_2024') if f.endswith('.json')],\n",
    "    'global_pv_inventory_sent2_spot_2021':\n",
    "        [f for f in get_ds_files('global_pv_inventory_sent2_spot_2021') if f.endswith('polygons.geojson') or f.endswith('set.geojson')],\n",
    "    # 'fra_west_eur_pv_installations_2023':\n",
    "    #     [os.path.join(root, fname) for root, _, files in os.walk(get_ds_dir('fra_west_eur_pv_installations_2023')) for fname in files ],\n",
    "    'ind_pv_solar_farms_2022': \n",
    "        [f for f in get_ds_files('ind_pv_solar_farms_2022') if f.endswith('.geojson')],\n",
    "    'usa_cali_usgs_pv_2016':\n",
    "        # grab all except the normal json\n",
    "        [f for f in get_ds_files('usa_cali_usgs_pv_2016') if not f.endswith('.json')]\n",
    "}\n",
    "\n",
    "# build and output tree for selected datasets\n",
    "selected_ds_dirs = [get_ds_dir(ds) for ds in selected_datasets]\n",
    "print(\"All selected datasets have been fetched with the following file tree:\\n\")\n",
    "selected_ds_tree = seedir(DATASET_DIR / 'raw' / 'labels', depthlimit=10, printout=True, regex=False, include_folders=selected_datasets, style='plus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f76f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cali_files = selected_ds_files['usa_cali_usgs_pv_2016']\n",
    "# cali_str = \"\\n\".join([f\"{i}: {os.path.relpath(f)}\" for i, f in enumerate(cali_files)])\n",
    "# print(f\"\\n\\nSelected dataset files:\\n{cali_str}\")\n",
    "# global_files = selected_ds_files['global_pv_inventory_sent2_spot_2021']\n",
    "# global_str = \"\\n\".join([f\"{i}: {os.path.relpath(f)}\" for i, f in enumerate(global_files)])\n",
    "# print(f\"\\n\\nSelected dataset files:\\n{global_str}\")\n",
    "# india_files = selected_ds_files['ind_pv_solar_farms_2022']\n",
    "# india_str = \"\\n\".join([f\"{i}: {os.path.relpath(f)}\" for i, f in enumerate(india_files)])\n",
    "# print(f\"\\n\\nSelected dataset files:\\n{india_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218e707e",
   "metadata": {},
   "source": [
    "#### Global inventory of solar PV units (Kruitwagen et al, 2021)\n",
    "\n",
    "From Zenodo:\n",
    "```\n",
    "Repository contents:\n",
    "\n",
    "trn_tiles.geojson: 18,570 rectangular areas-of-interest used for sampling training patch data.\n",
    "\n",
    "trn_polygons.geojson: 36,882 polygons obtained from OSM in 2017 used to label training patches.\n",
    "\n",
    "cv_tiles.geojson: 560 rectangular areas-of-interest used for sampling cross-validation data seeded from WRI GPPDB\n",
    "\n",
    "cv_polygons.geojson: 6,281 polygons corresponding to all PV solar generating units present in cv_tiles.geojson at the end of 2018.\n",
    "\n",
    "test_tiles.geojson: 122 rectangular regions-of-interest used for building the test set.\n",
    "\n",
    "test_polygons.geojson: 7,263 polygons corresponding to all utility-scale (>10kW) solar generating units present in test_tiles.geojson at the end of 2018.\n",
    "\n",
    "predicted_polygons.geojson: 68,661 polygons corresponding to predicted polygons in global deployment, capturing the status of deployed photovoltaic solar energy generating capacity at the end of 2018.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f160322d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional preprocessing specific to each dataset (mostly attaching any included metadata)\n",
    "def global_pv_inventory_spot_processing(gdf, dataset_name, output_dir, subset_bbox=None, geom_type='Polygon', rm_invalid=True):\n",
    "    all_cols = [\n",
    "        'unique_id', 'area', 'confidence', 'install_date', 'iso-3166-1', 'iso-3166-2', 'gti', 'pvout', 'capacity_mw', 'match_id', 'wdpa_10km', 'LC_CLC300_1992', 'LC_CLC300_1993',\n",
    "        'LC_CLC300_1994', 'LC_CLC300_1995', 'LC_CLC300_1996', 'LC_CLC300_1997', 'LC_CLC300_1998', 'LC_CLC300_1999', 'LC_CLC300_2000', 'LC_CLC300_2001', 'LC_CLC300_2002',\n",
    "        'LC_CLC300_2003', 'LC_CLC300_2004', 'LC_CLC300_2005', 'LC_CLC300_2006', 'LC_CLC300_2007', 'LC_CLC300_2008', 'LC_CLC300_2009', 'LC_CLC300_2010', 'LC_CLC300_2011',\n",
    "        'LC_CLC300_2012', 'LC_CLC300_2013', 'LC_CLC300_2014', 'LC_CLC300_2015', 'LC_CLC300_2016', 'LC_CLC300_2017', 'LC_CLC300_2018', 'mean_ai', 'GCR', 'eff', 'ILR',\n",
    "        'area_error', 'lc_mode', 'lc_arid', 'lc_vis', 'geometry', 'aoi_idx', 'aoi', 'id', 'Country', 'Province', 'Project', 'WRI_ref', 'Polygon Source', 'Date', 'building',\n",
    "        'operator', 'generator_source', 'amenity', 'landuse', 'power_source', 'shop', 'sport', 'tourism', 'way_area', 'access', 'construction', 'denomination', 'historic',\n",
    "        'leisure', 'man_made', 'natural', 'ref', 'religion', 'surface', 'z_order', 'layer', 'name', 'barrier', 'addr_housenumber', 'office', 'power', 'osm_id', 'military'\n",
    "    ]\n",
    "    # remove unwanted columns\n",
    "    keep_cols = ['geometry', 'unique_id', 'confidence', 'install_date', 'capacity_mw', 'iso-3166-2', 'pvout', 'osm_id', 'Project', 'construction']\n",
    "    print(f\"Filtering from {len(all_cols)} columns to {len(keep_cols)} columns:\\n{keep_cols}\")\n",
    "    gdf = gdf[keep_cols]\n",
    "    return gdf\n",
    "def global_pv_inventory_sent2_processing(gdf, dataset_name, output_dir, subset_bbox=None, geom_type='Polygon', rm_invalid=True):\n",
    "    return gdf\n",
    "def india_pv_solar_farms_processing(gdf, dataset_name, output_dir, subset_bbox=None, geom_type='Polygon', rm_invalid=True):\n",
    "    return gdf\n",
    "def usa_cali_usgs_pv_processing(gdf, dataset_name, output_dir, subset_bbox=None, geom_type='Polygon', rm_invalid=True):\n",
    "    return gdf\n",
    "def usa_eia_large_scale_pv_processing(gdf, dataset_name, output_dir, subset_bbox=None, geom_type='Polygon', rm_invalid=True):\n",
    "    return gdf\n",
    "def usa_eia_large_scale_pv_processing(gdf, dataset_name, output_dir, subset_bbox=None, geom_type='Polygon', rm_invalid=True):\n",
    "    return gdf\n",
    "\n",
    "def filter_duplicates(gdf, geom_type='Polygon', overlap_thresh=0.75):\n",
    "    \"\"\"\n",
    "    Remove duplicate geometries from a GeoDataFrame based on a specified overlap threshold.\n",
    "    \n",
    "    Args:\n",
    "        gdf (GeoDataFrame): Input GeoDataFrame.\n",
    "        geom_type (str): Geometry type to filter by. Default is 'Polygon'.\n",
    "        overlap_thresh (float): Overlap threshold for removing duplicates. Default is 0.75.\n",
    "        \n",
    "    Returns:\n",
    "        gdf (GeoDataFrame): GeoDataFrame with duplicates removed.\n",
    "    \"\"\"\n",
    "    # First identify exact duplicates\n",
    "    gdf = gdf.drop_duplicates('geometry')\n",
    "    \n",
    "    # Identify geometries that overlap substantially\n",
    "    overlaps = []\n",
    "    # Use spatial index for efficiency\n",
    "    spatial_index = gdf.sindex\n",
    "    \n",
    "    for idx, geom in enumerate(gdf.geometry):\n",
    "        # Find potential overlaps using the spatial index\n",
    "        possible_matches = list(spatial_index.intersection(geom.bounds))\n",
    "        # Remove self from matches\n",
    "        if idx in possible_matches:\n",
    "            possible_matches.remove(idx)\n",
    "        \n",
    "        for other_idx in possible_matches:\n",
    "            other_geom = gdf.iloc[other_idx].geometry\n",
    "            if geom.intersects(other_geom):\n",
    "                # Calculate overlap percentage (relative to the smaller polygon)\n",
    "                intersection_area = geom.intersection(other_geom).area\n",
    "                min_area = min(geom.area, other_geom.area)\n",
    "                overlap_percentage = intersection_area / min_area\n",
    "                \n",
    "                # If overlap is significant (e.g., >75%)\n",
    "                if overlap_percentage > overlap_thresh:\n",
    "                    # Keep the geometry with the larger area\n",
    "                    if geom.area < other_geom.area:\n",
    "                        overlaps.append(idx)\n",
    "                    \n",
    "                    else:\n",
    "                        overlaps.append(other_idx)\n",
    "                        break\n",
    "    \n",
    "    # Remove overlapping geometries\n",
    "    if overlaps:\n",
    "        print(f\"Removing {len(overlaps)} geometries with >{overlap_thresh*100}% overlap\")\n",
    "        gdf = gdf.drop(gdf.index[overlaps]).reset_index(drop=True)\n",
    "    \n",
    "    return gdf\n",
    "\n",
    "# basic processing for geojson, shapefiles, and already georeferenced data\n",
    "def process_geojson(geojson_files, dataset_name, output_dir=None, subset_bbox=None, geom_type='Polygon', rm_invalid=True, overlap_thresh=0.75, out_fmt='geoparquet'):\n",
    "    \"\"\"\n",
    "    Process a GeoJSON file and return a GeoDataFrame.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the GeoJSON file.\n",
    "        dataset_name (str): Name of the dataset.\n",
    "        geom_type (str): Geometry type to filter by. Default is 'Polygon'.\n",
    "        \n",
    "    Returns:\n",
    "        gdf (GeoDataFrame): Processed GeoDataFrame.\n",
    "    \"\"\"\n",
    "    if output_dir and not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    ds_dataframes = []\n",
    "\n",
    "    for fname in geojson_files:\n",
    "        if fname.endswith('.geojson') or fname.endswith('.json'):\n",
    "            # Check if the file is a valid GeoJSON\n",
    "            try:\n",
    "                gdf = gpd.read_file(fname)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {os.path.relpath(fname)}: {e}\")\n",
    "                continue\n",
    "            ds_dataframes.append(gdf)\n",
    "    \n",
    "    if len(ds_dataframes) == 0:\n",
    "        print(f\"No valid GeoJSON files found in {dataset_name}.\")\n",
    "        print(f\"Skipping dataset {dataset_name}\")\n",
    "        return None\n",
    "        \n",
    "    # Concatenate all dataframes into a single GeoDataFrame\n",
    "    gdf = gpd.GeoDataFrame(pd.concat(ds_dataframes, ignore_index=True))\n",
    "    # make sure the geometry column is included and named correctly\n",
    "    if 'geometry' not in gdf.columns:\n",
    "        gdf['geometry'] = gdf.geometry\n",
    "\n",
    "    # Basic info about the dataset\n",
    "    print(f\"Loaded geodataframe with raw counts of {len(gdf)} PV installations\")\n",
    "    print(f\"Coordinate reference system: {gdf.crs}\")\n",
    "    print(f\"Available columns: {gdf.columns.tolist()}\")\n",
    "    \n",
    "    # Add dataset name as a new column\n",
    "    gdf['dataset'] = dataset_name\n",
    "    \n",
    "    # Convert to WGS84 if not already in that CRS\n",
    "    if gdf.crs is not None and gdf.crs.to_string() != 'EPSG:4326':\n",
    "        # convert to WGS84 in cases of other crs (eg NAD83 for Cali dataset)\n",
    "        gdf = gdf.to_crs(epsg=4326)\n",
    "    if subset_bbox is not None:\n",
    "        # Filter the GeoDataFrame by the georeferenced bounding box\n",
    "        gdf = gdf.cx[subset_bbox[0]:subset_bbox[2], subset_bbox[1]:subset_bbox[3]]\n",
    "    \n",
    "    # DQ and cleaning\n",
    "    # check for missing and invalid geometries\n",
    "    invalid_geoms = gdf[gdf.geometry.is_empty | ~gdf.geometry.is_valid]\n",
    "    if len(invalid_geoms) > 0 and rm_invalid:\n",
    "        print(f\"Warning: {len(invalid_geoms)} invalid or empty geometries found and will be removed.\")\n",
    "        # Optionally remove invalid geometries\n",
    "        gdf = gdf[~gdf.geometry.is_empty & gdf.geometry.is_valid].reset_index(drop=True)\n",
    "    # Eliminating duplicates and geometries that overlap too much\n",
    "    if geom_type == 'Polygon':\n",
    "        gdf = filter_duplicates(gdf, geom_type=geom_type, overlap_thresh=overlap_thresh)\n",
    "\n",
    "    # perform any additional processing specific to the dataset for metadata and other attributes\n",
    "    if dataset_name == 'global_pv_inventory_sent2_2024':\n",
    "        print(\"Processing global_pv_inventory_sent2_2024 metadata\")\n",
    "        gdf = global_pv_inventory_sent2_processing(gdf, dataset_name, output_dir, subset_bbox=subset_bbox, geom_type=geom_type)\n",
    "    elif dataset_name == 'global_pv_inventory_sent2_spot_2021':\n",
    "        print(\"Processing global_pv_inventory_sent2_spot_2021 metadata\")\n",
    "        gdf = global_pv_inventory_spot_processing(gdf, dataset_name, output_dir, subset_bbox=subset_bbox, geom_type=geom_type)\n",
    "    elif dataset_name == 'ind_pv_solar_farms_2022':\n",
    "        print(\"Processing ind_pv_solar_farms_2022 metadata\")\n",
    "        gdf = india_pv_solar_farms_processing(gdf, dataset_name, output_dir, subset_bbox=subset_bbox, geom_type=geom_type)\n",
    "    elif dataset_name == 'usa_cali_usgs_pv_2016':\n",
    "        print(\"Processing usa_cali_usgs_pv_2016 metadata\")\n",
    "        gdf = usa_cali_usgs_pv_processing(gdf, dataset_name, output_dir, subset_bbox=subset_bbox, geom_type=geom_type)\n",
    "    elif dataset_name == 'usa_eia_large_scale_pv_2023':\n",
    "        print(\"Processing usa_eia_large_scale_pv_2023 metadata\")\n",
    "        gdf = usa_eia_large_scale_pv_processing(gdf, dataset_name, output_dir, subset_bbox=subset_bbox, geom_type=geom_type)\n",
    "    \n",
    "    # add some basic geometry info\n",
    "    # if not gdf.crs.is_geographic:\n",
    "    #     gdf['area_m2'] = gdf.geometry.area\n",
    "    # else:\n",
    "    #     # todo: check if other crs is more appropriate\n",
    "    gdf_proj = gdf['geometry'].to_crs(epsg=4326)\n",
    "    gdf['area_m2'] = gdf_proj.geometry.area\n",
    "    \n",
    "    # gdf['centroid_lon'] = gdf.geometry.centroid.x\n",
    "    gdf['centroid_lon'] = gdf_proj.geometry.centroid.x\n",
    "    # gdf['centroid_lat'] = gdf.geometry.centroid.y\n",
    "    gdf['centroid_lat'] = gdf_proj.geometry.centroid.y\n",
    "    # use gpd conversion argument\n",
    "    # gdf['bbox'] = gdf.geometry.apply(lambda geom: geom.bounds) \n",
    "\n",
    "    print(f\"After filtering and cleaning, we have {len(gdf)} PV installations\")\n",
    "    print(f\"Coordinate reference system: {gdf.crs}\")\n",
    "    print(f\"Available columns: {gdf.columns.tolist()}\")\n",
    "\n",
    "    if output_dir:\n",
    "        out_path = os.path.join(output_dir, f\"{dataset_name}_processed.{out_fmt}\")\n",
    "\n",
    "        if out_fmt == 'geoparquet':\n",
    "            gdf.to_parquet(out_path, \n",
    "                index=None, \n",
    "                compression='snappy',\n",
    "                geometry_encoding='WKB', \n",
    "                write_covering_bbox=True,\n",
    "                schema_version='1.1.0')\n",
    "        else:\n",
    "            gdf.to_file(out_path, driver='GeoJSON', index=None)\n",
    "        print(f\"Saved processed GeoDataFrame to {os.path.relpath(out_path)}\")\n",
    "    \n",
    "    return gdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11057816",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(selected_datasets)\n",
    "# go through the selected datasets and process them\n",
    "for ds in selected_datasets:\n",
    "    ds_files = selected_ds_files[ds]\n",
    "    ds_dir = get_ds_dir(ds)\n",
    "    out_dir = DATASET_DIR / 'raw' / 'labels' / 'geoparquet'\n",
    "    print(f\"Processing dataset {ds} with {len(ds_files)} files in {os.path.relpath(ds_dir)}\")\n",
    "    ds_gdf = process_geojson(\n",
    "                geojson_files=ds_files,\n",
    "                dataset_name=ds,\n",
    "                output_dir=out_dir\n",
    "    )\n",
    "    if ds_gdf is not None:\n",
    "        \n",
    "        display(ds_gdf.describe())\n",
    "        print(ds_gdf.info)\n",
    "        display(ds_gdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e917882",
   "metadata": {},
   "source": [
    "#### France West Europe PV Installations 2023\n",
    "\n",
    "From [research publication](https://doi.org/10.1038/s41597-023-01951-4): \n",
    "```\n",
    "The Git repository contains the raw crowdsourcing data and all the material necessary to re-generate our training dataset and technical validation.  \n",
    "It is structured as follows: the raw subfolder contains the raw annotation data from the two annotation campaigns and the raw PV installations’ metadata.  \n",
    "The replication subfolder contains the compiled data used to generate our segmentation masks.  \n",
    "The validation subfolder contains the compiled data necessary to replicate the analyses presented in the technical validation section.\n",
    "```\n",
    "\n",
    "We will be using the `replication` subfolder to generate our PV polygons geojson file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0867b736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(selected_metadata['fra_west_eur_pv_installations_2023'].keys())\n",
    "# print(selected_metadata['fra_west_eur_pv_installations_2023']['files'])\n",
    "# data_out = Path(os.getenv('DATA_PATH'))\n",
    "# fra_files = selected_metadata['fra_west_eur_pv_installations_2023']['files']\n",
    "# fra_out = selected_metadata['fra_west_eur_pv_installations_2023']['output_dir']\n",
    "# ds_sub = os.path.join(fra_out, 'replication')\n",
    "# fra_sub_files = '\\n'.join([os.path.relpath(f, data_out) for f in fra_files if f.startswith(ds_sub)])\n",
    "# print(f\"Subdir files:\\n{fra_sub_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5ecb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bespoke pre-processing for datsets not directly available in geojson or shapefile format\n",
    "# parse the point or polygon json files with geopandas, transform raw polygons or points features into proper geometry for geojson conversion\n",
    "from shapely.geometry import Polygon, Point, MultiPolygon\n",
    "import json\n",
    "\n",
    "# TODO: make function for processing of france json geometries\n",
    "\n",
    "def france_eur_pv_preprocess(ds_metadata, ds_subdir, metadata_dir='raw', crs=None, geom_type='Polygon'):\n",
    "    ds_dir = Path(ds_metadata['output_dir'])\n",
    "    data_dir = ds_dir / ds_subdir\n",
    "    metadata_file = 'raw-metadata_df.csv' if metadata_dir == 'raw' else 'metadata_df.csv'\n",
    "    metadata_file = ds_dir / metadata_dir / metadata_file\n",
    "    coords_file = \"polygon-analysis.json\" if geom_type == 'Polygon' else \"point-analysis.json\"\n",
    "    # keep files that are in the specified subdir and have the above filename\n",
    "    geom_files = [fpath for fpath in ds_metadata['files'] if fpath.startswith(data_dir) and fpath.endswith(coords_file)]\n",
    "    crs = crs or 'EPSG:4326' # default to WGS84\n",
    "\n",
    "    # load the metadata file\n",
    "    metadata_df = pd.read_csv(metadata_file)\n",
    "    print(f\"Loaded '{metadata_file.split('/')[-1]}' with {len(metadata_df)} rows\")\n",
    "\n",
    "    # load into geopandas, inspect the data, and add metadata_df to separate pd dataframe\n",
    "    raw_features = []\n",
    "    for geom_file_path in geom_files:\n",
    "        campaign_name = Path(geom_file_path).parent.name\n",
    "        print(f\"Processing {campaign_name} campaign...\")\n",
    "        \n",
    "        with open(geom_file_path, 'r') as f:\n",
    "            geom_data = json.load(f)\n",
    "        feat_types = set([f['type'] for f in geom_data])\n",
    "        print(f\"Feature types: {feat_types}\")\n",
    "    \n",
    "        for idx, feature_dict in enumerate(geom_data):\n",
    "            # Skip empty dictionaries\n",
    "            if not feature_dict:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                feature_id = feature_dict.get('id', idx) # Use index if ID is not present\n",
    "\n",
    "                # extract geometry and coordinates\n",
    "                if geom_type == 'Polygon':\n",
    "                    # feat_dict = [{'polygons': [{'points': {'x': <px_coord>, 'y': <px_coord>}, ...}]}, ...]\n",
    "                    coords = feature_dict['polygons']\n",
    "                    if isinstance(coords, list) and len(coords) > 0:\n",
    "                            # Handle multiple polygons\n",
    "                            polygons = []\n",
    "                            for poly_coords in coords:\n",
    "                                if len(poly_coords) >= 3:  # Need at least 3 points for a polygon\n",
    "                                    polygons.append(Polygon(poly_coords))\n",
    "                            \n",
    "                            if len(polygons) == 1:\n",
    "                                geometry = polygons[0]\n",
    "                            else:\n",
    "                                geometry = MultiPolygon(polygons)\n",
    "                                \n",
    "                            # Create feature dictionary with properties\n",
    "                            feature = {\n",
    "                                'id': feature_id,\n",
    "                                'campaign': campaign_name,\n",
    "                                'geometry': geometry\n",
    "                            }\n",
    "                    raw_features.append(feature)\n",
    "                elif geom_type == 'Point':\n",
    "                    # feat_dict = [{'clicks': [{'@type': 'Point', 'x': <px_coord>, 'y': <px_coord>}, ...]}, ...]\n",
    "                    coords = feature_dict['clicks']\n",
    "                    if isinstance(coords, list) and len(coords) > 0:\n",
    "                        points = []\n",
    "                        for point_coords in coords:\n",
    "                            if 'x' not in point_coords or 'y' not in point_coords:\n",
    "                                continue\n",
    "                            else:\n",
    "                                points.append(Point(point_coords['x'], point_coords['y']))\n",
    "                    raw_features.extend(points)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing feature {feature_dict}: {e}\")\n",
    "                continue\n",
    "\n",
    "    if raw_features:\n",
    "        # Convert to GeoDataFrame\n",
    "        gdf = gpd.GeoDataFrame(raw_features, crs=crs)\n",
    "        # add metadata to the gdf\n",
    "        if 'id' in gdf.columns:\n",
    "            gdf['id'] = gdf['id'].astype(str)\n",
    "        # Ensure CRS is set\n",
    "        if gdf.crs is None:\n",
    "            gdf.set_crs(crs, inplace=True)\n",
    "        elif str(gdf.crs) != crs:\n",
    "            gdf = gdf.to_crs(crs)\n",
    "        # need to add geotransform if available to convert pixel coords to lat/lon\n",
    "\n",
    "        # gdf['source_dataset'] = add in calling function\n",
    "    \n",
    "    return gdf, metadata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e74007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import the different datasets and convert to geoparquet with geopandas\n",
    "# def gdp_load_and_gpq_convert(dataset_name, label_fmt='geojson', crs=None, geom_type='Polygon'):\n",
    "\n",
    "#     crs = crs or 'EPSG:4326' # default to WGS84 if crs is None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "57ac8c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import pygeohash\n",
    "\n",
    "def geom_db_consolidate_dataset(\n",
    "    parquet_files: list[str],\n",
    "    out_db_file: str,\n",
    "    table_name: str = \"merged_geometry\",\n",
    "    geom_column: str = \"geometry\",\n",
    "    spatial_index: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Read a list of GeoParquet files into DuckDB, union them into one table,\n",
    "    create a spatial index on the geometry column, and persist to disk.\n",
    "\n",
    "    Args:\n",
    "        parquet_files: list of paths to input parquet files (must contain WKB or WKB‐encoded geometry column).\n",
    "        out_db_file: path to DuckDB database file (will be created or overwritten).\n",
    "        table_name: name of the consolidated table in DuckDB.\n",
    "        geom_column: name of the geometry column in the parquet files.\n",
    "        spatial_index: if True, create a spatial index on geom_column.\n",
    "    \"\"\"\n",
    "    # connect (creates file if missing)\n",
    "    conn = duckdb.connect(database=out_db_file, read_only=False)\n",
    "    # enable spatial extension\n",
    "    conn.execute(\"INSTALL spatial;\")\n",
    "    conn.execute(\"LOAD spatial;\")\n",
    "\n",
    "    # drop any existing table\n",
    "    conn.execute(f\"DROP TABLE IF EXISTS {table_name};\")\n",
    "\n",
    "    # build a UNION ALL query over all parquet files\n",
    "    scans = []\n",
    "    for path in parquet_files:\n",
    "        scans.append(f\"SELECT *, ST_GeometryFromWKB({geom_column}) AS geom FROM read_parquet('{path}')\")\n",
    "    union_sql = \"\\nUNION ALL\\n\".join(scans)\n",
    "\n",
    "    # create the consolidated table\n",
    "    conn.execute(f\"\"\"\n",
    "        CREATE TABLE {table_name} AS\n",
    "        {union_sql}\n",
    "    \"\"\".strip())\n",
    "\n",
    "    # optionally create a spatial index\n",
    "    if spatial_index:\n",
    "        # this builds an R‐tree index over the geom column\n",
    "        conn.execute(f\"CALL ST_CreateSpatialIndex('{table_name}', 'geom');\")\n",
    "\n",
    "    conn.close()\n",
    "    print(f\"Consolidated {len(parquet_files)} files into {out_db_file} → table '{table_name}'\")\n",
    "\n",
    "def gdf_merge_consolidate_dataset(\n",
    "    gdf_list: list[gpd.GeoDataFrame],\n",
    "    out_gpq_file: str,\n",
    "    geohash_precision: int = 7,\n",
    "    partition_col: str = \"geohash\"\n",
    ") -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Concatenate multiple GeoDataFrames, compute a geohash partition key for each feature,\n",
    "    and write out a partitioned GeoParquet file.\n",
    "\n",
    "    Args:\n",
    "        gdf_list: list of GeoDataFrames to concatenate (must share CRS).\n",
    "        out_gpq_file: path to output geoparquet (can be a directory for hive-style partitions).\n",
    "        geohash_precision: length of geohash string (controls spatial partition granularity).\n",
    "        partition_col: name of the new column to partition on.\n",
    "\n",
    "    Returns:\n",
    "        merged GeoDataFrame.\n",
    "    \"\"\"\n",
    "    if not gdf_list:\n",
    "        print(\"No GeoDataFrames provided.\")\n",
    "        return None\n",
    "\n",
    "    # ensure all frames share the same CRS\n",
    "    base_crs = gdf_list[0].crs\n",
    "    for gdf in gdf_list[1:]:\n",
    "        if gdf.crs != base_crs:\n",
    "            gdf.to_crs(base_crs, inplace=True)\n",
    "\n",
    "    # concatenate\n",
    "    merged = gpd.GeoDataFrame(\n",
    "        pd.concat(gdf_list, ignore_index=True),\n",
    "        crs=base_crs\n",
    "    )\n",
    "\n",
    "    # compute partition key via geohash of centroids\n",
    "    merged[\"centroid\"] = merged.geometry.centroid\n",
    "    merged[partition_col] = merged[\"centroid\"].apply(\n",
    "        lambda pt: pygeohash.encode(pt.y, pt.x, precision=geohash_precision)\n",
    "    )\n",
    "    merged.drop(columns=\"centroid\", inplace=True)\n",
    "\n",
    "    # write out partitioned GeoParquet\n",
    "    merged.to_parquet(\n",
    "        out_gpq_file,\n",
    "        index=False,\n",
    "        partition_cols=[partition_col],\n",
    "        compression=\"snappy\",\n",
    "        engine=\"pyarrow\"\n",
    "    )\n",
    "    print(f\"Wrote {len(merged)} features to {out_gpq_file} partitioned on '{partition_col}'\")\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f01ce6",
   "metadata": {},
   "source": [
    "# Visualization Functions for PV Data\n",
    "\n",
    "After processing the datasets into standardized geoparquet format, we'll use the following visualization libraries to explore and present the data:\n",
    "\n",
    "- **Folium**: For interactive web maps with various basemaps and markers\n",
    "- **Pydeck**: For high-performance 3D and large-scale visualizations\n",
    "- **Lonboard**: For GPU-accelerated geospatial visualization of large datasets\n",
    "\n",
    "Each library has specific strengths that we'll leverage for different visualization needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf64feb",
   "metadata": {},
   "source": [
    "## Folium Visualization Functions\n",
    "\n",
    "Folium is excellent for creating interactive web maps with various basemaps and markers. It's particularly useful for visualizing geographic distributions and creating choropleth maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3987442e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from folium.plugins import HeatMap, MarkerCluster\n",
    "\n",
    "def create_folium_cluster_map(gdf, zoom_start=3, title=\"PV Installation Clusters\"):\n",
    "    \"\"\"\n",
    "    Create a cluster map of PV installations using Folium.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gdf : GeoDataFrame\n",
    "        GeoDataFrame containing PV installation data with geometry column\n",
    "    zoom_start : int\n",
    "        Initial zoom level for the map\n",
    "    title : str\n",
    "        Title for the map\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    folium.Map\n",
    "        Interactive Folium map with clustered markers\n",
    "    \"\"\"\n",
    "    # Ensure the GeoDataFrame is in WGS84 (EPSG:4326) for Folium compatibility\n",
    "    if gdf.crs != \"EPSG:4326\":\n",
    "        gdf = gdf.to_crs(\"EPSG:4326\")\n",
    "    \n",
    "    # Get centroid of all points to center the map\n",
    "    center_lat = gdf.geometry.centroid.y.mean()\n",
    "    center_lon = gdf.geometry.centroid.x.mean()\n",
    "    \n",
    "    # Create base map\n",
    "    m = folium.Map(location=[center_lat, center_lon], zoom_start=zoom_start,\n",
    "                  tiles='CartoDB positron')\n",
    "    \n",
    "    # Add title\n",
    "    title_html = f'''\n",
    "             <h3 align=\"center\" style=\"font-size:16px\"><b>{title}</b></h3>\n",
    "             '''\n",
    "    m.get_root().html.add_child(folium.Element(title_html))\n",
    "    \n",
    "    # Add marker cluster\n",
    "    marker_cluster = MarkerCluster().add_to(m)\n",
    "    \n",
    "    # Add markers for each PV installation\n",
    "    for idx, row in gdf.iterrows():\n",
    "        # Get the centroid if the geometry is a polygon\n",
    "        if row.geometry.geom_type in ['Polygon', 'MultiPolygon']:\n",
    "            centroid = row.geometry.centroid\n",
    "            popup_text = f\"ID: {idx}\"\n",
    "            \n",
    "            # Add additional information if available in the dataframe\n",
    "            for col in ['capacity_mw', 'area_sqm', 'installation_date', 'source_dataset']:\n",
    "                if col in gdf.columns:\n",
    "                    popup_text += f\"<br>{col}: {row[col]}\"\n",
    "            \n",
    "            folium.Marker(\n",
    "                location=[centroid.y, centroid.x],\n",
    "                popup=folium.Popup(popup_text, max_width=300),\n",
    "                icon=folium.Icon(color='green', icon='solar-panel', prefix='fa')\n",
    "            ).add_to(marker_cluster)\n",
    "    \n",
    "    return m\n",
    "\n",
    "def create_folium_choropleth(gdf, column, bins=8, cmap='YlOrRd', \n",
    "                             title=\"PV Installation Density\"):\n",
    "    \"\"\"\n",
    "    Create a choropleth map of PV installations using Folium.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gdf : GeoDataFrame\n",
    "        GeoDataFrame containing PV installation data with geometry column\n",
    "    column : str\n",
    "        Column name to use for choropleth coloring\n",
    "    bins : int\n",
    "        Number of bins for choropleth map\n",
    "    cmap : str\n",
    "        Matplotlib colormap name\n",
    "    title : str\n",
    "        Title for the map\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    folium.Map\n",
    "        Interactive Folium choropleth map\n",
    "    \"\"\"\n",
    "    # Ensure the GeoDataFrame is in WGS84 (EPSG:4326)\n",
    "    if gdf.crs != \"EPSG:4326\":\n",
    "        gdf = gdf.to_crs(\"EPSG:4326\")\n",
    "    \n",
    "    # Get centroid of all points to center the map\n",
    "    center_lat = gdf.geometry.centroid.y.mean()\n",
    "    center_lon = gdf.geometry.centroid.x.mean()\n",
    "    \n",
    "    # Create base map\n",
    "    m = folium.Map(location=[center_lat, center_lon], zoom_start=3,\n",
    "                  tiles='CartoDB positron')\n",
    "    \n",
    "    # Add title\n",
    "    title_html = f'''\n",
    "             <h3 align=\"center\" style=\"font-size:16px\"><b>{title}</b></h3>\n",
    "             '''\n",
    "    m.get_root().html.add_child(folium.Element(title_html))\n",
    "    \n",
    "    # Create choropleth layer\n",
    "    folium.Choropleth(\n",
    "        geo_data=gdf,\n",
    "        name='choropleth',\n",
    "        data=gdf,\n",
    "        columns=[gdf.index.name if gdf.index.name else 'index', column],\n",
    "        key_on='feature.id',\n",
    "        fill_color=cmap,\n",
    "        fill_opacity=0.7,\n",
    "        line_opacity=0.2,\n",
    "        legend_name=column,\n",
    "        bins=bins\n",
    "    ).add_to(m)\n",
    "    \n",
    "    # Add hover functionality\n",
    "    style_function = lambda x: {'fillColor': '#ffffff', \n",
    "                                'color': '#000000', \n",
    "                                'fillOpacity': 0.1, \n",
    "                                'weight': 0.1}\n",
    "    highlight_function = lambda x: {'fillColor': '#000000', \n",
    "                                    'color': '#000000', \n",
    "                                    'fillOpacity': 0.5, \n",
    "                                    'weight': 0.1}\n",
    "    \n",
    "    # Add tooltips\n",
    "    folium.GeoJson(\n",
    "        gdf,\n",
    "        style_function=style_function,\n",
    "        highlight_function=highlight_function,\n",
    "        tooltip=folium.GeoJsonTooltip(\n",
    "            fields=[column],\n",
    "            aliases=[column.replace('_', ' ').title()],\n",
    "            style=(\"background-color: white; color: #333333; font-family: arial; font-size: 12px; padding: 10px;\")\n",
    "        )\n",
    "    ).add_to(m)\n",
    "    \n",
    "    # Add layer control\n",
    "    folium.LayerControl().add_to(m)\n",
    "    \n",
    "    return m\n",
    "\n",
    "def create_folium_heatmap(gdf, intensity_column=None, radius=15, \n",
    "                          title=\"PV Installation Heatmap\"):\n",
    "    \"\"\"\n",
    "    Create a heatmap of PV installations using Folium.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gdf : GeoDataFrame\n",
    "        GeoDataFrame containing PV installation data with geometry column\n",
    "    intensity_column : str, optional\n",
    "        Column name to use for heatmap intensity; if None, all points have equal weight\n",
    "    radius : int\n",
    "        Radius for heatmap points (in pixels)\n",
    "    title : str\n",
    "        Title for the map\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    folium.Map\n",
    "        Interactive Folium heatmap\n",
    "    \"\"\"\n",
    "    # Ensure the GeoDataFrame is in WGS84 (EPSG:4326)\n",
    "    if gdf.crs != \"EPSG:4326\":\n",
    "        gdf = gdf.to_crs(\"EPSG:4326\")\n",
    "    \n",
    "    # Get centroids for all geometries\n",
    "    if any(gdf.geometry.geom_type.isin(['Polygon', 'MultiPolygon'])):\n",
    "        centroids = gdf.geometry.centroid\n",
    "    else:\n",
    "        centroids = gdf.geometry\n",
    "    \n",
    "    # Get coordinates for heatmap\n",
    "    heat_data = [[point.y, point.x] for point in centroids]\n",
    "    \n",
    "    # Add intensity if specified\n",
    "    if intensity_column and intensity_column in gdf.columns:\n",
    "        heat_data = [[point.y, point.x, float(intensity)] \n",
    "                    for point, intensity in zip(centroids, gdf[intensity_column])]\n",
    "    \n",
    "    # Get centroid of all points to center the map\n",
    "    center_lat = sum(point[0] for point in heat_data) / len(heat_data)\n",
    "    center_lon = sum(point[1] for point in heat_data) / len(heat_data)\n",
    "    \n",
    "    # Create base map\n",
    "    m = folium.Map(location=[center_lat, center_lon], zoom_start=4,\n",
    "                  tiles='CartoDB positron')\n",
    "    \n",
    "    # Add title\n",
    "    title_html = f'''\n",
    "             <h3 align=\"center\" style=\"font-size:16px\"><b>{title}</b></h3>\n",
    "             '''\n",
    "    m.get_root().html.add_child(folium.Element(title_html))\n",
    "    \n",
    "    # Add heatmap layer\n",
    "    HeatMap(\n",
    "        heat_data,\n",
    "        radius=radius,\n",
    "        blur=10,\n",
    "        gradient={0.4: 'blue', 0.65: 'lime', 1: 'red'}\n",
    "    ).add_to(m)\n",
    "    \n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079e3e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = 'global_pv_inventory_sent2_spot_2021'\n",
    "ds_files = selected_ds_files[ds]\n",
    "ds_dir = get_ds_dir(ds)\n",
    "out_dir = DATASET_DIR / 'raw' / 'labels' / 'geoparquet'\n",
    "print(f\"Processing dataset {ds} with {len(ds_files)} files in {os.path.relpath(ds_dir)}\")\n",
    "ds_gdf = process_geojson(\n",
    "            geojson_files=ds_files,\n",
    "            dataset_name=ds,\n",
    "            output_dir=out_dir\n",
    ")\n",
    "if ds_gdf is not None:\n",
    "    display(ds_gdf.describe())\n",
    "    print(ds_gdf.info)\n",
    "    display(ds_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc8613e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(ds_gdf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3820b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter to only rows without NaN for folium viz\n",
    "ds_gdf = ds_gdf[ds_gdf['capacity_mw'].notna()]\n",
    "# Create Folium maps\n",
    "cluster_map = create_folium_cluster_map(ds_gdf, title=f\"{ds} - Cluster Map\")\n",
    "# cluster_map.save(os.path.join(out_dir, f\"{ds}_cluster_map.html\"))\n",
    "cluster_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9ea146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choropleth_map = create_folium_choropleth(ds_gdf, column='capacity_mw', title=f\"{ds} - Capacity Choropleth\")\n",
    "# choropleth_map.save(os.path.join(out_dir, f\"{ds}_choropleth_map.html\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fe9718",
   "metadata": {},
   "source": [
    "heatmap = create_folium_heatmap(ds_gdf, intensity_column='capacity_mw', title=f\"{ds} - Capacity Heatmap\")\n",
    "heatmap.save(os.path.join(out_dir, f\"{ds}_heatmap.html\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbe1845",
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap = create_folium_heatmap(ds_gdf, intensity_column='capacity_mw', title=f\"{ds} - Capacity Heatmap\")\n",
    "# heatmap.save(os.path.join(out_dir, f\"{ds}_heatmap.html\"))\n",
    "heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d04e252",
   "metadata": {},
   "source": [
    "## PyDeck Visualization Functions\n",
    "\n",
    "PyDeck is excellent for high-performance 3D visualizations and handling large datasets. It's particularly useful for creating layered maps with multiple types of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaac7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pydeck_scatterplot(gdf, color_column=None, size_scale=100, \n",
    "                             title=\"PV Installation Map\"):\n",
    "    \"\"\"\n",
    "    Create a scatterplot of PV installations using PyDeck.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gdf : GeoDataFrame\n",
    "        GeoDataFrame containing PV installation data with geometry column\n",
    "    color_column : str, optional\n",
    "        Column name to use for point coloring\n",
    "    size_scale : float\n",
    "        Scaling factor for point size\n",
    "    title : str\n",
    "        Title for the map\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pydeck.Deck\n",
    "        Interactive PyDeck map with scatterplot layer\n",
    "    \"\"\"\n",
    "    # Ensure the GeoDataFrame is in WGS84 (EPSG:4326)\n",
    "    if gdf.crs != \"EPSG:4326\":\n",
    "        gdf = gdf.to_crs(\"EPSG:4326\")\n",
    "    \n",
    "    # Convert to DataFrame with lat/lon columns\n",
    "    df = pd.DataFrame({\n",
    "        'lat': gdf.geometry.centroid.y,\n",
    "        'lon': gdf.geometry.centroid.x\n",
    "    })\n",
    "    \n",
    "    # Add additional columns from the original GeoDataFrame\n",
    "    for col in gdf.columns:\n",
    "        if col != 'geometry':\n",
    "            df[col] = gdf[col]\n",
    "    \n",
    "    # Handle color mapping\n",
    "    if color_column and color_column in df.columns:\n",
    "        # Check if the column is numeric\n",
    "        if pd.api.types.is_numeric_dtype(df[color_column]):\n",
    "            color_scale = [\n",
    "                [0, [65, 182, 196]],\n",
    "                [0.33, [127, 205, 187]],\n",
    "                [0.66, [199, 233, 180]],\n",
    "                [1, [237, 248, 177]]\n",
    "            ]\n",
    "            \n",
    "            # Normalize the values\n",
    "            df['color_value'] = (df[color_column] - df[color_column].min()) / (df[color_column].max() - df[color_column].min())\n",
    "            get_color = f\"[r, g, b]\"\n",
    "            \n",
    "            # Create a calculated color column using the scale\n",
    "            df['r'] = df['color_value'].apply(lambda x: int(\n",
    "                np.interp(x, [scale[0] for scale in color_scale], [scale[1][0] for scale in color_scale])\n",
    "            ))\n",
    "            df['g'] = df['color_value'].apply(lambda x: int(\n",
    "                np.interp(x, [scale[0] for scale in color_scale], [scale[1][1] for scale in color_scale])\n",
    "            ))\n",
    "            df['b'] = df['color_value'].apply(lambda x: int(\n",
    "                np.interp(x, [scale[0] for scale in color_scale], [scale[1][2] for scale in color_scale])\n",
    "            ))\n",
    "            \n",
    "        else:\n",
    "            # For categorical data, use hash of category for color\n",
    "            unique_cats = df[color_column].unique()\n",
    "            color_map = {cat: [int(h) % 256 for h in str(hash(cat))[:3]] for cat in unique_cats}\n",
    "            df['r'] = df[color_column].map(lambda x: color_map[x][0])\n",
    "            df['g'] = df[color_column].map(lambda x: color_map[x][1])\n",
    "            df['b'] = df[color_column].map(lambda x: color_map[x][2])\n",
    "            \n",
    "        get_color = \"[r, g, b]\"\n",
    "    else:\n",
    "        # Default color: green for solar panels\n",
    "        get_color = \"[0, 128, 0]\"  # Green\n",
    "    \n",
    "    # Calculate point size - use area if available\n",
    "    if 'area_sqm' in df.columns:\n",
    "        get_size = f\"Math.sqrt(area_sqm) * {size_scale/100}\"\n",
    "    elif 'capacity_mw' in df.columns:\n",
    "        get_size = f\"Math.sqrt(capacity_mw) * {size_scale/10}\"\n",
    "    else:\n",
    "        get_size = str(size_scale)\n",
    "    \n",
    "    # Create ScatterplotLayer\n",
    "    layer = pdk.Layer(\n",
    "        'ScatterplotLayer',\n",
    "        df,\n",
    "        get_position=['lon', 'lat'],\n",
    "        get_radius=get_size,\n",
    "        get_fill_color=get_color,\n",
    "        pickable=True,\n",
    "        opacity=0.8,\n",
    "        stroked=True,\n",
    "        filled=True\n",
    "    )\n",
    "    \n",
    "    # Set initial view state to center on data\n",
    "    view_state = pdk.ViewState(\n",
    "        longitude=df['lon'].mean(),\n",
    "        latitude=df['lat'].mean(),\n",
    "        zoom=3,\n",
    "        pitch=0\n",
    "    )\n",
    "    \n",
    "    # Create tooltip\n",
    "    tooltip = {\n",
    "        \"html\": \"<b>ID:</b> {index}<br>\"\n",
    "    }\n",
    "    \n",
    "    # Add additional fields to tooltip if available\n",
    "    for col in ['capacity_mw', 'area_sqm', 'installation_date', 'source_dataset']:\n",
    "        if col in df.columns:\n",
    "            tooltip[\"html\"] += f\"<b>{col.replace('_', ' ').title()}:</b> {{{col}}}<br>\"\n",
    "    \n",
    "    if color_column:\n",
    "        tooltip[\"html\"] += f\"<b>{color_column.replace('_', ' ').title()}:</b> {{{color_column}}}\"\n",
    "    \n",
    "    # Create deck\n",
    "    deck = pdk.Deck(\n",
    "        layers=[layer],\n",
    "        initial_view_state=view_state,\n",
    "        tooltip=tooltip,\n",
    "        map_style='light'\n",
    "    )\n",
    "    \n",
    "    return deck\n",
    "\n",
    "def create_pydeck_polygons(gdf, color_column=None, extrusion_column=None, \n",
    "                          extrusion_scale=100, title=\"PV Installation Map\"):\n",
    "    \"\"\"\n",
    "    Create a 3D polygon map of PV installations using PyDeck.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gdf : GeoDataFrame\n",
    "        GeoDataFrame containing PV installation data with Polygon geometry\n",
    "    color_column : str, optional\n",
    "        Column name to use for polygon coloring\n",
    "    extrusion_column : str, optional\n",
    "        Column name to use for polygon height extrusion\n",
    "    extrusion_scale : float\n",
    "        Scaling factor for extrusion height\n",
    "    title : str\n",
    "        Title for the map\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pydeck.Deck\n",
    "        Interactive PyDeck map with polygon layer\n",
    "    \"\"\"\n",
    "    # Ensure the GeoDataFrame is in WGS84 (EPSG:4326)\n",
    "    if gdf.crs != \"EPSG:4326\":\n",
    "        gdf = gdf.to_crs(\"EPSG:4326\")\n",
    "    \n",
    "    # Filter to only include polygons\n",
    "    poly_gdf = gdf[gdf.geometry.geom_type.isin(['Polygon', 'MultiPolygon'])]\n",
    "    \n",
    "    if len(poly_gdf) == 0:\n",
    "        return \"Error: No polygon geometries found in the GeoDataFrame.\"\n",
    "    \n",
    "    # Convert to a format PyDeck can use\n",
    "    polygon_data = []\n",
    "    \n",
    "    for idx, row in poly_gdf.iterrows():\n",
    "        geom = row.geometry\n",
    "        \n",
    "        # Handle both Polygon and MultiPolygon\n",
    "        polygons = [geom] if geom.geom_type == 'Polygon' else list(geom.geoms)\n",
    "        \n",
    "        for poly in polygons:\n",
    "            # Extract exterior coordinates\n",
    "            exterior_coords = list(poly.exterior.coords)\n",
    "            coords = [[point[0], point[1]] for point in exterior_coords]\n",
    "            \n",
    "            # Create a feature for each polygon\n",
    "            feature = {\n",
    "                'polygon': coords,\n",
    "                'index': idx\n",
    "            }\n",
    "            \n",
    "            # Add additional properties\n",
    "            for col in poly_gdf.columns:\n",
    "                if col != 'geometry':\n",
    "                    feature[col] = row[col] if not pd.isna(row[col]) else None\n",
    "            \n",
    "            polygon_data.append(feature)\n",
    "    \n",
    "    # Create DataFrame from polygon data\n",
    "    df = pd.DataFrame(polygon_data)\n",
    "    \n",
    "    # Handle color mapping\n",
    "    if color_column and color_column in df.columns:\n",
    "        # Check if the column is numeric\n",
    "        if pd.api.types.is_numeric_dtype(df[color_column]):\n",
    "            color_scale = [\n",
    "                [0, [65, 182, 196]],\n",
    "                [0.33, [127, 205, 187]],\n",
    "                [0.66, [199, 233, 180]],\n",
    "                [1, [237, 248, 177]]\n",
    "            ]\n",
    "            \n",
    "            # Normalize the values\n",
    "            df['color_value'] = (df[color_column] - df[color_column].min()) / (df[color_column].max() - df[color_column].min())\n",
    "            \n",
    "            # Create a calculated color column using the scale\n",
    "            df['r'] = df['color_value'].apply(lambda x: int(\n",
    "                np.interp(x, [scale[0] for scale in color_scale], [scale[1][0] for scale in color_scale])\n",
    "            ))\n",
    "            df['g'] = df['color_value'].apply(lambda x: int(\n",
    "                np.interp(x, [scale[0] for scale in color_scale], [scale[1][1] for scale in color_scale])\n",
    "            ))\n",
    "            df['b'] = df['color_value'].apply(lambda x: int(\n",
    "                np.interp(x, [scale[0] for scale in color_scale], [scale[1][2] for scale in color_scale])\n",
    "            ))\n",
    "            \n",
    "            get_color = \"[r, g, b]\"\n",
    "        else:\n",
    "            # For categorical data, use hash of category for color\n",
    "            unique_cats = df[color_column].unique()\n",
    "            color_map = {cat: [int(h) % 256 for h in str(hash(str(cat)))[:3]] for cat in unique_cats}\n",
    "            df['r'] = df[color_column].map(lambda x: color_map[x][0])\n",
    "            df['g'] = df[color_column].map(lambda x: color_map[x][1])\n",
    "            df['b'] = df[color_column].map(lambda x: color_map[x][2])\n",
    "            \n",
    "            get_color = \"[r, g, b]\"\n",
    "    else:\n",
    "        # Default color: green for solar panels\n",
    "        get_color = \"[0, 128, 0]\"  # Green\n",
    "    \n",
    "    # Handle extrusion\n",
    "    if extrusion_column and extrusion_column in df.columns:\n",
    "        get_elevation = f\"{extrusion_column} * {extrusion_scale}\"\n",
    "    else:\n",
    "        get_elevation = str(extrusion_scale)\n",
    "    \n",
    "    # Create PolygonLayer\n",
    "    layer = pdk.Layer(\n",
    "        'PolygonLayer',\n",
    "        df,\n",
    "        get_polygon='polygon',\n",
    "        get_fill_color=get_color,\n",
    "        get_elevation=get_elevation,\n",
    "        elevation_scale=1,\n",
    "        extruded=True,\n",
    "        filled=True,\n",
    "        wireframe=True,\n",
    "        pickable=True,\n",
    "        opacity=0.6,\n",
    "        auto_highlight=True\n",
    "    )\n",
    "    \n",
    "    # Find center of polygons for the view\n",
    "    all_coords = []\n",
    "    for poly in df['polygon']:\n",
    "        all_coords.extend(poly)\n",
    "    \n",
    "    center_lon = np.mean([coord[0] for coord in all_coords])\n",
    "    center_lat = np.mean([coord[1] for coord in all_coords])\n",
    "    \n",
    "    # Set initial view state\n",
    "    view_state = pdk.ViewState(\n",
    "        longitude=center_lon,\n",
    "        latitude=center_lat,\n",
    "        zoom=10,\n",
    "        pitch=45,\n",
    "        bearing=0\n",
    "    )\n",
    "    \n",
    "    # Create tooltip\n",
    "    tooltip = {\n",
    "        \"html\": \"<b>ID:</b> {index}<br>\"\n",
    "    }\n",
    "    \n",
    "    # Add additional fields to tooltip if available\n",
    "    for col in ['capacity_mw', 'area_sqm', 'installation_date', 'source_dataset']:\n",
    "        if col in df.columns:\n",
    "            tooltip[\"html\"] += f\"<b>{col.replace('_', ' ').title()}:</b> {{{col}}}<br>\"\n",
    "    \n",
    "    if color_column:\n",
    "        tooltip[\"html\"] += f\"<b>{color_column.replace('_', ' ').title()}:</b> {{{color_column}}}<br>\"\n",
    "    \n",
    "    if extrusion_column:\n",
    "        tooltip[\"html\"] += f\"<b>{extrusion_column.replace('_', ' ').title()}:</b> {{{extrusion_column}}}\"\n",
    "    \n",
    "    # Create deck\n",
    "    deck = pdk.Deck(\n",
    "        layers=[layer],\n",
    "        initial_view_state=view_state,\n",
    "        tooltip=tooltip,\n",
    "        map_style='light'\n",
    "    )\n",
    "    \n",
    "    return deck\n",
    "\n",
    "def create_pydeck_heatmap(gdf, weight_column=None, intensity=1, radius=1000,\n",
    "                         title=\"PV Installation Heatmap\"):\n",
    "    \"\"\"\n",
    "    Create a heatmap of PV installations using PyDeck.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gdf : GeoDataFrame\n",
    "        GeoDataFrame containing PV installation data with geometry column\n",
    "    weight_column : str, optional\n",
    "        Column name to use for heatmap weighting\n",
    "    intensity : float\n",
    "        Intensity of the heatmap\n",
    "    radius : float\n",
    "        Radius of influence for each point (in meters)\n",
    "    title : str\n",
    "        Title for the map\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pydeck.Deck\n",
    "        Interactive PyDeck heatmap\n",
    "    \"\"\"\n",
    "    # Ensure the GeoDataFrame is in WGS84 (EPSG:4326)\n",
    "    if gdf.crs != \"EPSG:4326\":\n",
    "        gdf = gdf.to_crs(\"EPSG:4326\")\n",
    "    \n",
    "    # Convert to DataFrame with lat/lon columns\n",
    "    df = pd.DataFrame({\n",
    "        'lat': gdf.geometry.centroid.y,\n",
    "        'lon': gdf.geometry.centroid.x\n",
    "    })\n",
    "    \n",
    "    # Add weight column if specified\n",
    "    if weight_column and weight_column in gdf.columns:\n",
    "        df['weight'] = gdf[weight_column]\n",
    "        get_weight = 'weight'\n",
    "    else:\n",
    "        get_weight = 1\n",
    "    \n",
    "    # Create HeatmapLayer\n",
    "    layer = pdk.Layer(\n",
    "        'HeatmapLayer',\n",
    "        df,\n",
    "        get_position=['lon', 'lat'],\n",
    "        get_weight=get_weight,\n",
    "        pickable=False,\n",
    "        opacity=0.8,\n",
    "        radius_pixels=radius/100,  # Convert meters to pixels roughly\n",
    "        intensity=intensity,\n",
    "        threshold=0.05,\n",
    "        color_range=[\n",
    "            [1, 152, 189],\n",
    "            [73, 227, 206],\n",
    "            [216, 254, 181],\n",
    "            [254, 237, 177],\n",
    "            [254, 173, 84],\n",
    "            [209, 55, 78]\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Set initial view state to center on data\n",
    "    view_state = pdk.ViewState(\n",
    "        longitude=df['lon'].mean(),\n",
    "        latitude=df['lat'].mean(),\n",
    "        zoom=3,\n",
    "        pitch=0\n",
    "    )\n",
    "    \n",
    "    # Create deck\n",
    "    deck = pdk.Deck(\n",
    "        layers=[layer],\n",
    "        initial_view_state=view_state,\n",
    "        map_style='light'\n",
    "    )\n",
    "    \n",
    "    return deck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5b8c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_pydeck_heatmap(ds_gdf, weight_column='capacity_mw', title=f\"{ds} - Capacity Heatmap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df04a910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out NaN area\n",
    "ds_gdf = ds_gdf[ds_gdf['area_m2'].notna()]\n",
    "create_pydeck_polygons(ds_gdf, color_column='capacity_mw', extrusion_column='area_m2', title=f\"{ds} - Capacity 3D Map\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f917a060",
   "metadata": {},
   "source": [
    "## Lonboard Visualization Functions\n",
    "\n",
    "Lonboard is a GPU-accelerated geospatial visualization library that's excellent for handling very large datasets. It's particularly useful for creating high-performance interactive visualizations of millions of data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1e5c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lonboard_map(gdf, color_column=None, size_column=None, size_scale=1,\n",
    "                       title=\"PV Installation Map\"):\n",
    "    \"\"\"\n",
    "    Create an interactive map of PV installations using Lonboard.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gdf : GeoDataFrame\n",
    "        GeoDataFrame containing PV installation data with geometry column\n",
    "    color_column : str, optional\n",
    "        Column name to use for point coloring\n",
    "    size_column : str, optional\n",
    "        Column name to use for point sizing\n",
    "    size_scale : float\n",
    "        Scaling factor for point size\n",
    "    title : str\n",
    "        Title for the map\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    lonboard.Map\n",
    "        Interactive Lonboard map\n",
    "    \"\"\"\n",
    "    # Ensure the GeoDataFrame is in WGS84 (EPSG:4326)\n",
    "    if gdf.crs != \"EPSG:4326\":\n",
    "        gdf = gdf.to_crs(\"EPSG:4326\")\n",
    "    \n",
    "    # Handle color mapping if specified\n",
    "    if color_column and color_column in gdf.columns:\n",
    "        color = gdf[color_column]\n",
    "    else:\n",
    "        color = None\n",
    "    \n",
    "    # Handle size mapping if specified\n",
    "    if size_column and size_column in gdf.columns:\n",
    "        size = gdf[size_column] * size_scale\n",
    "    else:\n",
    "        size = size_scale\n",
    "    \n",
    "    # Create the map\n",
    "    m = lonboard.Map()\n",
    "    \n",
    "    # Handle different geometry types\n",
    "    if all(gdf.geometry.geom_type.isin(['Point'])):\n",
    "        # For point geometries\n",
    "        m.add_layer(\n",
    "            lonboard.ScatterplotLayer(\n",
    "                gdf,\n",
    "                get_color=color,\n",
    "                get_radius=size,\n",
    "                opacity=0.8,\n",
    "                pickable=True,\n",
    "                auto_highlight=True\n",
    "            )\n",
    "        )\n",
    "    elif all(gdf.geometry.geom_type.isin(['Polygon', 'MultiPolygon'])):\n",
    "        # For polygon geometries\n",
    "        m.add_layer(\n",
    "            lonboard.GeoJsonLayer(\n",
    "                gdf,\n",
    "                get_fill_color=color,\n",
    "                get_line_color=[0, 0, 0, 200],\n",
    "                get_line_width=2,\n",
    "                opacity=0.8,\n",
    "                pickable=True,\n",
    "                auto_highlight=True\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        # For mixed geometries, convert to points (centroids) for simplicity\n",
    "        gdf_centroids = gdf.copy()\n",
    "        gdf_centroids['geometry'] = gdf_centroids.geometry.centroid\n",
    "        \n",
    "        m.add_layer(\n",
    "            lonboard.ScatterplotLayer(\n",
    "                gdf_centroids,\n",
    "                get_color=color,\n",
    "                get_radius=size,\n",
    "                opacity=0.8,\n",
    "                pickable=True,\n",
    "                auto_highlight=True\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    return m\n",
    "\n",
    "def create_lonboard_heatmap(gdf, weight_column=None, radius=1000,\n",
    "                          intensity=1, title=\"PV Installation Heatmap\"):\n",
    "    \"\"\"\n",
    "    Create a heatmap of PV installations using Lonboard.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gdf : GeoDataFrame\n",
    "        GeoDataFrame containing PV installation data with geometry column\n",
    "    weight_column : str, optional\n",
    "        Column name to use for heatmap weighting\n",
    "    radius : float\n",
    "        Radius of influence for each point (in meters)\n",
    "    intensity : float\n",
    "        Intensity of the heatmap\n",
    "    title : str\n",
    "        Title for the map\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    lonboard.Map\n",
    "        Interactive Lonboard heatmap\n",
    "    \"\"\"\n",
    "    # Ensure the GeoDataFrame is in WGS84 (EPSG:4326)\n",
    "    if gdf.crs != \"EPSG:4326\":\n",
    "        gdf = gdf.to_crs(\"EPSG:4326\")\n",
    "    \n",
    "    # Handle weight mapping if specified\n",
    "    if weight_column and weight_column in gdf.columns:\n",
    "        weight = gdf[weight_column]\n",
    "    else:\n",
    "        weight = 1\n",
    "    \n",
    "    # Get centroids for all geometries\n",
    "    gdf_centroids = gdf.copy()\n",
    "    if not all(gdf.geometry.geom_type.isin(['Point'])):\n",
    "        gdf_centroids['geometry'] = gdf_centroids.geometry.centroid\n",
    "    \n",
    "    # Create the map\n",
    "    m = lonboard.Map()\n",
    "    \n",
    "    # Add heatmap layer\n",
    "    m.add_layer(\n",
    "        lonboard.HeatmapLayer(\n",
    "            gdf_centroids,\n",
    "            get_weight=weight,\n",
    "            radius_pixels=int(radius/100),  # Convert meters to pixels roughly\n",
    "            intensity=intensity,\n",
    "            threshold=0.05,\n",
    "            color_range=[\n",
    "                [1, 152, 189, 255],\n",
    "                [73, 227, 206, 255],\n",
    "                [216, 254, 181, 255],\n",
    "                [254, 237, 177, 255],\n",
    "                [254, 173, 84, 255],\n",
    "                [209, 55, 78, 255]\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return m\n",
    "\n",
    "def create_lonboard_aggregation(gdf, resolution=8, color_scale='viridis',\n",
    "                              title=\"PV Installation Density\"):\n",
    "    \"\"\"\n",
    "    Create a hexbin aggregation map of PV installations using Lonboard.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gdf : GeoDataFrame\n",
    "        GeoDataFrame containing PV installation data with geometry column\n",
    "    resolution : int\n",
    "        Resolution of hexbins (higher = more detailed)\n",
    "    color_scale : str\n",
    "        Matplotlib colormap name for coloring\n",
    "    title : str\n",
    "        Title for the map\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    lonboard.Map\n",
    "        Interactive Lonboard hexbin aggregation map\n",
    "    \"\"\"\n",
    "    # Ensure the GeoDataFrame is in WGS84 (EPSG:4326)\n",
    "    if gdf.crs != \"EPSG:4326\":\n",
    "        gdf = gdf.to_crs(\"EPSG:4326\")\n",
    "    \n",
    "    # Get centroids for all geometries\n",
    "    gdf_centroids = gdf.copy()\n",
    "    if not all(gdf.geometry.geom_type.isin(['Point'])):\n",
    "        gdf_centroids['geometry'] = gdf_centroids.geometry.centroid\n",
    "    \n",
    "    # Create the map\n",
    "    m = lonboard.Map()\n",
    "    \n",
    "    # Add hexbin layer\n",
    "    m.add_layer(\n",
    "        lonboard.H3HexagonLayer(\n",
    "            gdf_centroids,\n",
    "            get_hex_id=lambda row: h3.geo_to_h3(row.geometry.y, row.geometry.x, resolution),\n",
    "            get_fill_color=\"colorScale\",\n",
    "            color_scale=color_scale,\n",
    "            opacity=0.8,\n",
    "            pickable=True,\n",
    "            auto_highlight=True\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0dee37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_lonboard_aggregation(ds_gdf, resolution=7, color_scale='viridis', title=f\"{ds} - Density Map\")\n",
    "# create_lonboard_map(ds_gdf, color_column='capacity_mw', size_column='area_sqm', size_scale=100, title=f\"{ds} - Capacity Map\")\n",
    "# create_lonboard_heatmap(ds_gdf, weight_column='capacity_mw', radius=1000, intensity=1, title=f\"{ds} - Capacity Heatmap\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa0ad17",
   "metadata": {},
   "source": [
    "## Example Usage\n",
    "\n",
    "Here are some examples of how to use these visualization functions with your processed PV datasets. After loading your geoparquet files into GeoDataFrames, you can use these functions to create interactive visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96e51af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage (commented out until you have processed your datasets)\n",
    "\"\"\"\n",
    "# Load a processed geoparquet file\n",
    "gdf = gpd.read_parquet('data/geoparquet/combined_pv_dataset.parquet')\n",
    "\n",
    "# Basic visualizations with each library\n",
    "# 1. Create a Folium cluster map\n",
    "folium_map = create_folium_cluster_map(gdf, zoom_start=2, title=\"Global PV Installations\")\n",
    "display(folium_map)\n",
    "\n",
    "# 2. Create a PyDeck 3D visualization\n",
    "if 'capacity_mw' in gdf.columns:\n",
    "    pydeck_map = create_pydeck_polygons(\n",
    "        gdf, \n",
    "        color_column='source_dataset',\n",
    "        extrusion_column='capacity_mw',\n",
    "        extrusion_scale=100,\n",
    "        title=\"3D PV Installation Capacity\"\n",
    "    )\n",
    "    display(pydeck_map)\n",
    "\n",
    "# 3. Create a Lonboard heatmap for large datasets\n",
    "lonboard_map = create_lonboard_heatmap(\n",
    "    gdf,\n",
    "    weight_column='area_sqm' if 'area_sqm' in gdf.columns else None,\n",
    "    radius=2000,\n",
    "    intensity=2,\n",
    "    title=\"Global PV Installation Density\"\n",
    ")\n",
    "display(lonboard_map)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8431be",
   "metadata": {},
   "source": [
    "## Advanced Visualization: Multi-layer Comparison\n",
    "\n",
    "For more sophisticated analysis, you might want to compare multiple datasets or visualize multiple attributes simultaneously. Here's an example of how to create a multi-layer visualization using PyDeck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fb1d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of multi-layer visualization (commented out until datasets are processed)\n",
    "\"\"\"\n",
    "def create_multi_dataset_comparison(gdfs_dict, base_color_scale=None):\n",
    "    '''\n",
    "    Create a multi-layer comparison of different PV datasets\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gdfs_dict : dict\n",
    "        Dictionary of {dataset_name: gdf} pairs\n",
    "    base_color_scale : list, optional\n",
    "        Base color scale to use for differentiation\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pydeck.Deck\n",
    "        Interactive PyDeck map with multiple layers\n",
    "    '''\n",
    "    if base_color_scale is None:\n",
    "        base_color_scale = [\n",
    "            [255, 0, 0],  # Red\n",
    "            [0, 255, 0],  # Green\n",
    "            [0, 0, 255],  # Blue\n",
    "            [255, 255, 0],  # Yellow\n",
    "            [255, 0, 255],  # Magenta\n",
    "            [0, 255, 255],  # Cyan\n",
    "        ]\n",
    "    \n",
    "    # Create layers list\n",
    "    layers = []\n",
    "    \n",
    "    # Track all coordinates to determine view center\n",
    "    all_lats = []\n",
    "    all_lons = []\n",
    "    \n",
    "    # Create a layer for each dataset with a unique color\n",
    "    for i, (name, gdf) in enumerate(gdfs_dict.items()):\n",
    "        # Ensure the GeoDataFrame is in WGS84 (EPSG:4326)\n",
    "        if gdf.crs != \"EPSG:4326\":\n",
    "            gdf = gdf.to_crs(\"EPSG:4326\")\n",
    "        \n",
    "        # Get color for this dataset\n",
    "        color_idx = i % len(base_color_scale)\n",
    "        color = base_color_scale[color_idx]\n",
    "        \n",
    "        # Convert to DataFrame with lat/lon columns\n",
    "        df = pd.DataFrame({\n",
    "            'lat': gdf.geometry.centroid.y,\n",
    "            'lon': gdf.geometry.centroid.x,\n",
    "            'dataset': name\n",
    "        })\n",
    "        \n",
    "        # Add additional columns from the original GeoDataFrame\n",
    "        for col in gdf.columns:\n",
    "            if col != 'geometry':\n",
    "                df[col] = gdf[col]\n",
    "        \n",
    "        # Create ScatterplotLayer for this dataset\n",
    "        layer = pdk.Layer(\n",
    "            'ScatterplotLayer',\n",
    "            df,\n",
    "            get_position=['lon', 'lat'],\n",
    "            get_radius=100,\n",
    "            get_fill_color=color + [180],  # Add alpha value\n",
    "            pickable=True,\n",
    "            opacity=0.8,\n",
    "            stroked=True,\n",
    "            filled=True,\n",
    "            id=f\"scatter-{name}\"  # Add ID for legend\n",
    "        )\n",
    "        \n",
    "        layers.append(layer)\n",
    "        \n",
    "        # Track coordinates\n",
    "        all_lats.extend(df['lat'].tolist())\n",
    "        all_lons.extend(df['lon'].tolist())\n",
    "    \n",
    "    # Set initial view state to center on all data\n",
    "    view_state = pdk.ViewState(\n",
    "        longitude=np.mean(all_lons),\n",
    "        latitude=np.mean(all_lats),\n",
    "        zoom=3,\n",
    "        pitch=0\n",
    "    )\n",
    "    \n",
    "    # Create tooltip\n",
    "    tooltip = {\n",
    "        \"html\": \"<b>Dataset:</b> {dataset}<br>\"\n",
    "    }\n",
    "    \n",
    "    # Create deck\n",
    "    deck = pdk.Deck(\n",
    "        layers=layers,\n",
    "        initial_view_state=view_state,\n",
    "        tooltip=tooltip,\n",
    "        map_style='light'\n",
    "    )\n",
    "    \n",
    "    return deck\n",
    "\n",
    "# After processing your datasets:\n",
    "# gdfs = {\n",
    "#    'Global PV Inventory': gpd.read_parquet('data/geoparquet/global_pv_inventory.parquet'),\n",
    "#    'USA PV Data': gpd.read_parquet('data/geoparquet/usa_pv_data.parquet'),\n",
    "#    'UK PV Data': gpd.read_parquet('data/geoparquet/uk_pv_data.parquet')\n",
    "# }\n",
    "# multi_comparison = create_multi_dataset_comparison(gdfs)\n",
    "# display(multi_comparison)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1326633e",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "These visualization functions provide a comprehensive toolkit for exploring and presenting your PV installation data. Each library has its strengths:\n",
    "\n",
    "- **Folium**: Best for quick interactive web maps with various basemaps and standard visualization types\n",
    "- **PyDeck**: Excellent for 3D visualizations and handling larger datasets with complex visualizations\n",
    "- **Lonboard**: Best performance for very large datasets with GPU acceleration\n",
    "\n",
    "You can customize these functions further based on your specific analysis needs and the attributes available in your processed datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eo-pv-cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
