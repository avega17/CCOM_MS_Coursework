{\color{gray}\hrule}
\begin{center}
\section{[CCOM6050] Spatial Hierarchical Clustering Algorithms applied to DGGS}
\textbf{Here we will go into detail on our algorithm design and approach for optimization of STAC queries when generating our dataset.}
\bigskip
\end{center}
{\color{gray}\hrule}

\begin{multicols}{2}

\subsection{Data Ingestion and Pre-processing}
% * *(Potential for Map Point Reduction (MPR) inspired by Oje et al., 2025, if initial PV data is excessively dense/redundant before H3 aggregation).*

\paragraph{Dataset Acquisition and Management} 
We download PV installation datasets from various academic repositories including Zenodo, figshare, GitHub, and ScienceBase. 
The datasets are available in multiple vector file formats (GeoJSON, GeoPackage, Shapefile), which we standardize using our data pipeline. 
We utilize the \texttt{datahugger} library to fetch datasets from Zenodo and figshare, \texttt{sciencebasepy} for accessing the USGS ScienceBase Catalog, 
and custom functions for GitHub-hosted datasets. Dataset metadata is stored in a structured JSON configuration file to track sources, DOIs, and formats. 

\paragraph{Standardization to GeoParquet}
We convert all vector dataset files to GeoParquet format using \texttt{geopandas}. GeoParquet extends the Apache Parquet columnar storage format with geospatial capabilities, providing significant advantages:
\begin{itemize}
    \item \textit{Efficient storage} with built-in compression, reducing file sizes by up to 10x compared to GeoJSON
    \item \textit{Columnar storage format} optimized for analytical queries and filtering operations
    \item \textit{Spatial indexing and predicate pushdown} capabilities for efficient geospatial operations
    \item \textit{Wide ecosystem compatibility} with modern geospatial data tools
\end{itemize}

This provides a space complexity reduction. For example, the cumulative size of the 5 datasets(cite) we worked with this semester was 690.5 MB in GeoJSON and GeoPackage formats, 
while after consolidation and conversion to GeoParquet the size was reduced to 107.3MB. This is largely due to the inherent compression capabilities of the Parquet format, from removing unused columns, 
and filtering out invalid geometries. 

\paragraph{Consolidation and Deduplication}
After standardizing individual datasets, we:
\begin{enumerate}
    \item Consolidate all datasets into a single DuckDB database with spatial extensions enabled
    \item Create a unified schema with standardized columns (geometry, area, source dataset, etc.)
    \item Perform spatial deduplication to remove overlapping polygons using spatial indices
    \item For CV models, filter to only retain valid polygon geometries (POLYGON, MULTI-POLYGON) for segmentation training
\end{enumerate}

The running time for the deduplication process is reduced by using spatial indices as for each record we only need to compare nearby geometries as established by the spatial index.


\paragraph{Geospatial Enrichment}
We enrich the consolidated PV dataset with administrative boundary information from the Overture Maps project:
\begin{enumerate}
    \item Spatial join with Overture divisions (countries, regions, counties)
    \item Administrative division matching (ISO country codes and region names, and division\_ids for future querying) for each PV installation
    \item Aggregation statistics at country and region levels
    \item Choropleth visualizations of PV distribution by country and region
    \item Leafmap interactive scatterplot map for visualizing PV installations coordinates in Jupyter notebooks
    \item Adding geographic context allows us to train by country or even regional models adapted to local conditions
    \item Enables efficient querying of PV installations by administrative divisions
\end{enumerate}

\paragraph{H3 Indexing and Spatial Aggregation}
To efficiently organize and query the global PV dataset, we utilize H3, the hierarchical hexagonal DGGS discussed above.
H3 provides a hierarchical structure of hexagonal grid cells, allowing for efficient spatial indexing and querying. There are \texttt{duckdb} and \texttt{python} bindings available for H3,
which we use to index the PV dataset and perform spatial operations.
\begin{enumerate}
    \item Each PV installation is assigned an H3 index at optimal resolution based on its size
    \item Common H3 resolution (level 5, $\sim$250km$^2$ hexagons) added for regional aggregation
    \item Efficient spatial clustering and adaptive subdivisions using H3 hierarchy
    \item Spatial joins with geospatial data layers (Overture Maps) using H3 indices as common keys
\end{enumerate}


% * Algorithm for assigning PV labels to H3 cells at a chosen resolution.
%     * Discussion on selecting H3 resolution(s).
% * Derivation of H3 cell-level features (e.g., PV count, total PV area, installation date statistics).
% * *(Optional: Augmentation with Overture Maps Land Cover or other STAC-derived features per H3 cell).*

\subsection{Mutual Reachibility Graph from H3 Grid Cells [Future Work]}
A Mutual Reachibility Graph (MRG) is an undirected, weighted graph where each node represents a point in the dataset and edges represent the mutual reachability distance between points.
As one of the first steps in our clustering algorithm, we will create a MRG using the centroids of the H3 grid cells containing PV installations instead of the raw input data points $P$.
One advantage this provides is that spatially close PV installations are contained within the same H3 cell, which means we turn our set of raw input data points $P$ into a much smaller set of H3 cells $C$ that contain the PV installations.
This greatly reduces the number of points we need to consider when building our graph depending on the desired resolution of the H3 cells.
For the MRG edge weights, we use the H3 grid distance between cells $c_a$ and $c_b$ as our mutual reachability distance $d_{MRG}(c_a, c_b)$ metric. 
To reduce the number of edges we need to consider, we can limit the number of neighbors we consider  to only the $k$ nearest neighbors or only those within a certain H3 distance. 
A naive approach would be to compute the distance between all pairs of H3 cells, which would take $O(N^2)$ time, where $N$ is the number of H3 cells with PV installations.
However, this can be mitigated by for each cell $c_a$ only considering the $k$ nearest neighbors or neighbors within a fixed H3 distance to reduce the number of edges we need to compute. 
This is particularly critical for tree construction algorithms like Kruskal's or Prim's, whose runtimes scales based on the number of edges we are considering for graph construction.

Nodes: H3 cells containing PV installations at a specified resolution.
Edges: "Path" of adjacent H3 grid cells that connect two H3 cells.
Edge Weight Metric: h3.grid\_distance(cell\_a, cell\_b) to define topological proximity. 
This integer distance (number of hops) will be the primary factor for MST construction, prioritizing connectivity.
% MST Edge Representation: While the MST algorithm operates on these weights, for later use (especially negative sampling), the paths corresponding to MST edges are also considered. For an MST edge connecting cell_A and cell_B, the sequence of H3 cells forming this direct connection can be materialized using h3.grid_path_cells(cell_A, cell_B). These "path cells" represent the immediate spatial context between clustered PV cells.

Graph representation (e.g., adjacency list for sparse graph if only considering neighbors within a certain H3 distance for initial graph formation before MST).
% * Algorithm: Mutual Reachability Graph (MRG) construction.
% * Nodes: H3 cells containing PV installations.
% * Edge Weight Definition:
%     * Primary: `h3.grid_distance(cell_a, cell_b)` to define topological proximity.
%     * *(Discussion of potential hybrid approaches if feature similarity is incorporated).*
% * Graph representation (e.g., adjacency list for sparse graph if only considering neighbors within a certain H3 distance).

\subsection{MST Construction: Running Time and Parallel Optimizations [Future Work]}
% * Algorithm: Parallel GeoFilterKruskal (GFK) with MemoGFK optimization (Wang et al., 2021).
% * Justification: Scalability for large numbers of H3 cells, memory efficiency.
% * *(If HDBSCAN*-like density focus is explored: discuss core distance definition for H3 cells and Wang et al.'s specialized well-separation).*

\label{subsec:mst_construction}

Once the H3 cells PV data points are identified and the graph nodes are defined (H3 cell centroids) with edge weights based on \texttt{h3.grid\_distance}, the next crucial step is the construction of a Minimum Spanning Tree (MST). The MST will form the backbone for the subsequent hierarchical clustering and dendrogram generation (in \cite{Wang_Yu_Gu_Shun_2021}). Therefore we need efficient algorithms for MST construction, especially given the potentially large number of H3 cells ($N$) involved in a global or regional analysis despite the reduction in...



\paragraph{Strategies for Dense or Geometric Graphs: The EMST Case}
For Euclidean Minimum Spanning Trees (EMST), where points exist in a geometric space, the graph is also \textit{implicitly dense}. Wang et al. (2021) address this by leveraging Well-Separated Pair Decompositions (WSPDs) \cite{Wang_Fast_Parallel_EMST_HDBSCAN_2021}. A WSPD reduces the number of effective "pairs" (and thus potential edges) to consider from $N^2$ to $N$. They then apply a parallelized version of Kruskal's algorithm, termed GeoFilterKruskal (GFK). Their parallel EMST algorithm achieves $O(N^2)$ work in constant dimensions but, importantly, has a polylogarithmic depth ($O(\log^2 N)$), enabling substantial parallel speedups. 
% A key contribution of Wang et al. is the \textbf{MemoGFK optimization}. Instead of materializing the full WSPD (which can still be memory-intensive), MemoGFK dynamically traverses a k-d tree (built on point coordinates) in rounds, retrieving only WSPD pairs whose connecting edges are likely to be part of the MST within specific weight ranges. This significantly reduces both memory footprint (up to 10x) and computation time (up to 8x) in practice \cite{Wang_Fast_Parallel_EMST_HDBSCAN_2021}. 


\paragraph{MST Construction with H3 \texttt{grid\_distance}}
In our work, edge weights are primarily defined by \texttt{h3.grid\_distance}, which is \textit{a topological grid distance rather than a Euclidean one}. For their HDBSCAN* variant, the authors construct an MST based on a MRG as described above where edge weights incorporate a "core distance" metric reflecting local density. 
\begin{enumerate}
    \item \textbf{Graph Sparsification:} To avoid the $O(N^2)$ complexity, we first construct a sparse graph. For each H3 cell $c_i$ containing PV installations, we only consider edges to its neighbors. Neighbors can be defined as:
    \begin{itemize}
        \item H3 cells within a small $k$-ring (e.g., using \texttt{h3.k\_ring(cell, k\_val)} for $k' = 1 \text{ or } 2$).
        \item H3 cells within a maximum \texttt{h3.grid\_distance} threshold.
    \end{itemize}
    This reduces the number of edges $E_{sparse}$ to approximately $O(N \cdot \text{avg\_degree})$, where the average degree is relatively small and constant (e.g., 6 for $k'=1$ in H3)
    \item \textbf{Applying MST Algorithms on Sparse Graph:} Once the sparse graph with $E_{sparse}$ edges is constructed with \texttt{h3.grid\_distance}, standard MST algorithms can be applied more efficiently. For instance, Kruskal's algorithm would have a complexity related to $O(E_{sparse} \log E_{sparse})$ or $O(E_{sparse} \log N)$.
    \item \textbf{Parallel Optimizations:}
        \begin{itemize}
            \item Parallel versions of Kruskal's (e.g., using parallel sort and parallel Union-Find operations) or Boruvka's algorithm are well-suited for this sparse graph.
            \item The GFK algorithm in \cite{Wang_Yu_Gu_Shun_2021}...
        \end{itemize}
\end{enumerate}
The choice of sparsification strategy (e.g., $k'$ for $k$-ring) will be a trade-off between graph density (and thus MST computation time) and ensuring the true MST based on all pairwise H3 distances is captured or closely approximated. However, as the authors showed in their runtime decomposition figure, the MST Construction step has by far the shortest running time compared to the WSPD and Dendogram computations.

\paragraph{Justification and Importance}
The effort in constructing the MST efficiently is well-justified as the relatively fast computation of MSTs, especially with parallel and memory optimizations on sparse graphs, makes this approach viable for large-scale, planetary datasets of H3 cells.
% If adapting such a density-based approach to our H3 cells, the definition of core distance for an H3 cell (e.g., based on its PV content and the content of its H3 neighbors) and the specialized well-separation criteria proposed by Wang et al. would be relevant for constructing this specific type of MST \cite{Wang_Fast_Parallel_EMST_HDBSCAN_2021}.



\subsection{Generating a Hierarchy of Clusters: Dendogram Construction and H3 multi-resolution [Future Work]}

% * Algorithm(s):
%     * RC-Tree Tracing (RCTT) (Dhulipala et al., 2024) for CPU-based parallelism.
%     * PANDORA (Sao et al., 2024) for GPU-accelerated computation.
% * Justification: Work-optimality (or near-optimality), practical speedups, robustness to dendrogram skew.

\subsection{Optimized STAC Querying [Future Work]}
The primary goal of the proposed H3-cell based hierarchical clustering algorithm is not simply to aid in our clustering process, but
% * Algorithm for generating STAC query parameters (bounding boxes, geometries, time ranges) based on the identified H3 cell cluster footprints.
% * Strategy for minimizing redundant queries (e.g., merging overlapping cluster query geometries).

% \lipsum[1][10-15]

% \subsubsection{Statistical STF algorithm candidates}
% (add citations)

% \begin{itemize}
%     \item Weight Function-based methods
%         \begin{enumerate}
%             \item STARFM (Spatial and Temporal Adaptive Reflectance Fusion Model): \\
%             One of the earliest and most widely used methods. It assumes land cover change is minimal between image acquisition dates  
%             (ok for our application) and uses spectral similarity and spatial distance to weight contributions from known fine-resolution 
%             pixels to predict reflectance for dates where only coarse imagery is available. 
%             \item ESTARFM (Enhanced STARFM): \\
%             An extension of STARFM designed to perform better in heterogeneous landscapes by considering spectral unmixing concepts implicitly 
%             and using conversion coefficients derived from two fine/coarse image pairs before and after the desired prediction date. 
%         \end{enumerate}
%     \item Unmixing-based methods
%         \begin{enumerate}
%             \item STDFA (Spatio-Temporal Data Fusion Approach): \\
%             Combines spectral unmixing and spatial interpolation to predict fine-resolution reflectance. 
%         \end{enumerate}
%     \item Regression-based methods
% \end{itemize}

% \subsubsection{Deep Learning STF algorithm candidates}
% \begin{itemize}
%     \item CNN-based methods
%     \item GAN-based methods
% \end{itemize}

% \subsection{Implementation}
% (add links to colab jupyter notebooks) \\
% (add essential code excerpts to highlight algorithm details)

% \subsection{Complexity Analysis}

% \begin{itemize}
%     \item CV factors: image size, number of bands, number of samples
%     \item DL factors: number of layers, batch size, number of epochs, fp weight size
%     \item non-DL factors: window size, other parameters 
%     \item Big O notation
%     \item Running time 
%     \item Memory usage
% \end{itemize}

\clearpage

\end{multicols}

\clearpage

