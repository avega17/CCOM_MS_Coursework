\section{Background}

\begin{multicols}{2}

\subsubsection{Earth Observation and Remote Sensing}

Remote sensing (RS) refers to the process of acquiring images and data of the planet’s surface using a variety of remote sensors in satellite and aerial vehicles (add citations from my prev paper). 
These sensors analyze electromagnetic radiation reflected or emitted from objects on the Earth’s surface, which is then processed to extract information about the objects and their properties. 
Besides traditional electro-optical (ie panchromatic and 3-channel RGB images (cite)) modern RS employs a variety of sensors and modalities such as multispectral 
(four or more non-overlapping bands in the electromagnetic spectrum), hyperspectral (more than 100 narrow bands), and even active sensors such as microwave altimeters or Synthetic
Aperture Radar (SAR) that emit their own radiation and measure the reflected signal to ”see” at night and through atmospheric obstructions like clouds and fog (cite myself or refs from prev paper). 
Analysis of source data from these remote sensors must handle a variety of nuanced characteristics of each sensor and the produced data. For our purposes and the scope of this report, we will limit our discussion
to the following characteristics: 

\begin{itemize}
    \item Spatial resolution: The level of detail in an image, typically measured in meters per pixel.
    \item Spectral resolution: The ability of a sensor to distinguish between different wavelengths of light.
    \item Temporal resolution: The frequency at which a sensor captures data over the same location.
    \item Radiometric resolution: The sensitivity of a sensor to detect variations in intensity, often represented by bit depth.
    \item Earth Observation (EO) data types:
        \begin{enumerate}
            \item Raster data: Gridded data representing continuous surfaces.
            \item Vector data: Discrete data represented as points, lines, or polygons.
            \item Time series data: Sequential data capturing changes over time over a specific area.
            \item Geospatial data cubes: Multidimensional arrays combining spatial, temporal, and spectral dimensions.
        \end{enumerate}
\end{itemize}

In the context of data engineering, geospatial data brings several optimization and operations that can ease the process of analyzing large volumes of this type of data.
One of the most relevant concepts for our project is a spatial index 
% A fundamental challenge in RS is the tradeoff that an individual sensor's orbit and design must make between spatial, spectral, and temporal resolution. 


\subsubsection{Computer Vision}

Computer Vision (CV) is a subfield of Artificial Intelligence and a discipline that deals with the problem of interpreting and extracting meaningful information from images (cite myself or refs from prev paper) 
in a manner similar to human vision. This field has seen significant advancements in recent years, particularly with the rise of deep learning methods and the spread of large, labeled datasets for many applications. 
RS and EO has seen an explosion of interest, publications, and datasets for DL methods since 2015 (cite EO paper from cloud computing course). Relevant CV tasks for our purposes include:

\begin{enumerate}
    \item Scene Classification: Assigning a label to an entire image based on its content (e.g., classifying an image as containing a PV array, rooftop, or vegetation).
    \item Object Detection: Identifying and localizing specific object classes (e.g., PV panels) within an image with (georeferenced) bounding boxes.
    \item Semantic Segmentation: Classifying each pixel in an image into predefined categories (e.g., PV panel array, rooftop, vegetation, background).
    \item Instance Segmentation: Similar to semantic segmentation, but differentiates between \textit{individual} instances (e.g., distinguishing between different PV panel arrays). 
\end{enumerate}

Some relevant CV architectures include Convolutional Neural Networks (CNNs), Vision Transformers (ViTs), Generative Adversarial Networks (GANs), and CNN-Transformer hybrids. 

% - Importance of satellite imagery for monitoring renewable energy infrastructure.
% - Characteristics of PV arrays as seen from space (spectral, spatial).
% - Brief overview of relevant sensor types (multispectral, VHR).

\subsection{SpatioTemporal Asset Catalogs}
\label{subsec:stac} % Added label for cross-referencing

A SpatioTemporal Asset Catalog (STAC) is a specification schema that provides a standardized, open, and interoperable way to describe and catalog geospatial information. 
It enables \textbf{efficient searching, discovery, and access} to imagery, sensor data, and other Earth observation (EO) products. 
STAC has emerged as a crucial component in modern geospatial data infrastructure and workflows, addressing the challenges of 
managing and accessing vast volumes of EO data when performing large-scale analysis.

Key features and benefits of STAC include:
\begin{itemize}
    \item \textbf{Standardization}: STAC defines a common language for describing geospatial assets, using JSON to structure metadata. 
    This includes spatial extent (geometry), temporal coverage (datetime), data providers, licensing, and links to the actual data files (assets) and related resources. 
    \item \textbf{Discoverability}: The STAC metadata structure enables powerful search capabilities. 
    Users can query catalogs based on spatial (e.g. single point, or Polygon features) and temporal criteria, as well as other metadata fields like cloud\_cover, making it easier to find relevant data across diverse collections and providers without needing to understand provider-specific APIs.
    \item \textbf{Accessibility}: STAC catalogs typically link directly to the underlying data assets, often hosted in cloud object storage (e.g. AWS S3, Google Cloud Storage). 
    This allows for direct access to data, often in cloud-optimized formats like Cloud-Optimized GeoTIFF (COG), 
    enabling users to stream or process data without downloading entire image strips.
    \item \textbf{Interoperability}: The specification is designed to be extensible, allowing communities to add specific metadata fields relevant to their domain 
    while maintaining core compatibility. This has led to fast and widespread adoption and the development of a rich ecosystem of tools and services that support STAC. 
    \item \textbf{Cloud-Native Focus}: STAC is inherently cloud-native. It facilitates the creation of large-scale, dynamic catalogs of EO data that can be easily accessed and processed in the cloud, reducing data duplication and transfer costs.
\end{itemize}

The STAC specification consists of several core components:
\begin{itemize}
    \item \textbf{Item}: The fundamental unit in STAC, representing a single spatiotemporal asset (e.g., a satellite scene) described by its metadata and links to data files.
    \item \textbf{Collection}: A group of related STAC Items that share common metadata, such as sensor type, processing level, or geographic region.
    \item \textbf{Catalog}: A flexible structure that organizes STAC Collections and Items, often hierarchically, to facilitate browsing and discovery. Catalogs can link to other Catalogs, Collections, or Items.
    \item \textbf{API}: STAC also defines an API specification (STAC API) that provides a \textit{standardized} way to query and serve STAC metadata \textit{over the web}. This allows for dynamic searching and filtering of large catalogs.
\end{itemize}

The shift towards STAC represents a paradigm change from traditional data download workflows to more dynamic, query-driven access. 
Instead of downloading large, monolithic datasets, users can precisely identify and access only the data they need, often processing it in place and at scale in the cloud.
This approach is critical for handling the ever-increasing volume of EO data (hundreds of TB's daily!) and enabling scalable, on-demand geospatial analysis. 
The STAC ecosystem includes a variety of open-source tools that lower the barrier to adoption, such as \texttt{pystac} (Python library for creating and manipulating STAC objects), 
\href{https://radiantearth.github.io/stac-browser/#/external/maxar-opendata.s3.amazonaws.com/events/catalog.json?.language=en}{STAC Browser} (web-based exploration of STAC catalogs), 
and clients for various programming languages.


\subsection{Discrete Global Grid Systems}
% * Introduction to DGGS concepts.
% * Specifics of H3: hexagonal, hierarchical, indexing capabilities (`h3.geo_to_h3`, `h3.k_ring`, `h3.grid_distance`).
% * Rationale for using H3 as the foundational grid for spatial aggregation and analysis (Li et al., 2024, for grid-based significance thinking; 
% Oje et al., 2025, for context on DGGS performance, even if HierGP is adaptive).

\subsection{Minimum Spanning Trees for Spatial and Hierarchical Clustering}
% * Concept of MSTs in graph theory.
% * Application of MSTs in clustering: identifying natural groupings by cutting edges.
% * Advantages for spatial data: ability to find clusters of arbitrary shapes (Gagolewski et al., 2023).

\subsection{Research Gaps and Contribution Goals}
    \begin{enumerate}
        \item Measure performance impact of SOTA Computer Vision architectures (e.g. ViT, Swin, CNN-Transformer hybrids) currently lacking in most recent publications and compare to established segmentation baselines (e.g. UNet, FPN, PAN, etc.)
        \item Regional, and global surveys are limited to large-scale farms using medium resolution sensors ($\sim10m$/pixel). On the other hand, studies using VHR aerial imagery usually only have coverage for local, city-scale surveys.
        We will use \textit{global 30cm yearly basemaps} or open access catalogs from VHR MSI sensors, primarily from Maxar, to perform a global survey of PV installations that includes both large-scale farms and distributed rooftop PV systems.
        \item Almost all notable studies (with the exception of (cite GloSoFarID)) exclusively use RGB image bands. Measure impact of use of PV-specific spectral indices\cite{He_universal_pv_spectral_index_2024} and specifically the benefits of including NIR + SWIR bands available in Maxar sensors
        \item Develop a solution that consciously tackles the challenges identified in \cite{Hu_solar_array_pitfalls_2022} for evaluating and performing comparisons of different remote sensing solar array assessment methodologies
        (distribution drift, test data quality, level of spatial-aggregation, and proprietary data)
    \end{enumerate}


\end{multicols}


% \subsubsection{Data Fusion}

% \begin{enumerate}
%     \item Definition: combining data from multiple sources to achieve improved information quality or inference compared to using sources individually\cite{Castanedo_trad_data_fusion_2013} 
%     \item (Geospatial) Coherence\cite{Ghamisi_Multisource_and_Multitemporal_Data_Fusion_in_Remote_Sensing_2019}
%     \item Remote Sensing Fusion types\cite{Ghamisi_Multisource_and_Multitemporal_Data_Fusion_in_Remote_Sensing_2019}\cite{Li_DL_multimodal_RS_data_fusion_review_2022}
%     \begin{itemize}
%         \item Spatio-Spectral Fusion: Enhancing spectral resolution using spatial information (e.g., Pansharpening) or vice versa.\cite{Ghamisi_Multisource_and_Multitemporal_Data_Fusion_in_Remote_Sensing_2019}\cite{Zhang_panchromatic_and_msi_fusion_for_RS_and_EO_2023}
%         \item Spatio-Temporal Fusion: Combining high spatial resolution/low temporal frequency data with low spatial resolution/high temporal frequency data to generate a fused dataset with 
%         high resolution in both domains.\cite{Ghamisi_Multisource_and_Multitemporal_Data_Fusion_in_Remote_Sensing_2019} \textbf{This is the core fusion type for the second subproblem of this project}. 
%     \end{itemize}
%     \item Data Fusion Abstraction Levels\cite{Hussain_DL_Data_Fusion_review_2024}
%     \begin{itemize}
%         \item Early fusion (pixel/signal level): Combining raw sensor data directly 
%         \item Intermediate fusion (feature level): Extracting relevant features from each sensor first and then combining them
%         \item Late fusion (symbol/decision level): Each source produces an independent decision, which is then combined to produce a final output
%         % \item Hybrid fusion (combination of levels)
%     \end{itemize}
%     \item  A review from Castanedo\cite{Castanedo_trad_data_fusion_2013} also categorizes fusion based on the relationship between data sources (complementary, redundant, cooperative)
%     \item The rise of Deep Learning (DL) has significantly impacted the field, with reviews listing CNNs, LSTMs, GANs, and Transformers as the most relevant architectures for data fusion\cite{Li_DL_multimodal_RS_data_fusion_review_2022}\cite{Hussain_DL_Data_Fusion_review_2024}.
% \end{enumerate}

