# **Project 1 Implementation Plan: Senate Polarization**

This document outlines the strategic approach for each exercise in the CCOM6994 Project 1, focusing on leveraging DuckDB for efficient data ingestion and processing.

## **Global Note on Visualizations**

Across all exercises, all plots and visualizations should be generated by functions. This allows for:

1. **Reproducibility:** Easily re-generating a plot with updated data.  
2. **Flexibility:** Re-using the same plotting function for the final dashboard (Exercise 6).  
3. **Clarity:** Keeping the main analysis loop clean, with plotting logic encapsulated elsewhere.

For Exercise 6, these functions will be modified to **return Plotly figure objects** rather than just displaying a Matplotlib plot.

## **0\. Notebook Setup (One-Time)**

* **Action:** Run one-time setup commands at the top of the notebook to install dependencies for new metrics and the dashboard.  
* **Logic:**  
  1. **Install Dunn Index Library:**  
     \!git clone \[https://github.com/jqmviegas/jqm\_cvi.git\](https://github.com/jqmviegas/jqm\_cvi.git)  
     import sys  
     sys.path.append('jqm\_cvi')

  2. **Install Dash:**  
     \!pip install dash plotly

  3. **Import Metrics & Libraries:**  
     \# From scikit-learn  
     from sklearn.metrics import davies\_bouldin\_score, calinski\_harabasz\_score, silhouette\_score  
     from sklearn.metrics.pairwise import cosine\_similarity  
     from sklearn.decomposition import PCA  
     from sklearn.cluster import KMeans

     \# From cloned repo  
     from cvi import dunn

     \# Other standard imports  
     import pandas as pd  
     import numpy as np  
     import duckdb  
     import requests

     \# Plotting & Dashboard imports  
     import plotly.express as px  
     import plotly.graph\_objects as go  
     from dash import Dash, dcc, html, Input, Output  
     import ipywidgets as widgets  
     from IPython.display import display

## **Exercise 1: Parallel Ingestion & Core Table Creation**

**Goal:** Replace the sequential Python for loop with DuckDB's parallel CSV reader to ingest all 87th-116th Senate vote files. We will then create a unified, clean table to serve as the data source for the Python analysis loop.

### **1\. Database Setup**

* **Action:** Initialize an in-memory DuckDB connection. We will also set up a persistent .duckdb file for storing the results of our analysis (like the individual cohesion scores).  
* **Why:** In-memory is fastest for transient processing. A persistent file (senate\_analysis.duckdb) is ideal for storing analysis *results* that we want to query later without re-running the entire pipeline.  
* **Logic:**  
  \# Connect to a persistent file AND use in-memory for temp tables  
  con \= duckdb.connect(database='senate\_analysis.duckdb', read\_only=False)  
  con.sql("SET temp\_directory \= 'temp.tmp'") \# For large intermediate results

### **2\. Parallel Ingestion: Vote Data**

* **Action:** Use DuckDB's read\_csv function to read all vote files from the senate\_votes/ directory into a table. We'll use a TEMP table for initial ingestion.  
* **Key Functions/Parameters:**  
  * **Globbing:** 'senate\_votes/S\*\_votes.csv'  
  * **filename=True:** To extract the session number.  
  * **union\_by\_name=True:** To handle schema differences.  
* **Sample SQL:**  
  CREATE OR REPLACE TEMP TABLE raw\_votes AS  
  SELECT \*  
  FROM read\_csv(  
      'senate\_votes/S\*\_votes.csv',  
      auto\_detect \= true,  
      filename \= true,  
      union\_by\_name \= true  
  );

### **3\. Parallel Ingestion: Member Metadata (Enhanced)**

* **Action:** Ingest the HSall\_members.csv file from the VoteView URL into a persistent table members.  Use python's urllib or request as appropriate to fetch the data from URL if not present in local directory.  
* **Note on defaultdict / KeyError:** This SQL CASE statement solves the KeyError problem by binning all non-D/R/I parties into 'Other', effectively acting as a defaultdict.  
* **Sample SQL:**  
  CREATE OR REPLACE TABLE members AS  
  SELECT  
      CAST(icpsr AS VARCHAR) AS icpsr,  
      congress AS session\_num,  
      bioname AS senator\_name,  
      party\_code,  
      CASE  
          WHEN party\_code \= 100 THEN 'D'  
          WHEN party\_code \= 200 THEN 'R'  
          WHEN party\_code \= 328 THEN 'I'  
          ELSE 'Other'  
      END AS political\_party  
  FROM read\_csv(  
      '\[https://voteview.com/static/data/out/members/HSall\_members.csv\](https://voteview.com/static/data/out/members/HSall\_members.csv)',  
      auto\_detect \= true  
  );

### **4\. Transformation & Cleaning: Final Votes Table**

* **Action:** Create a final, *persistent* table senate\_votes\_processed from raw\_votes.  
* **Sample SQL:**  
  CREATE OR REPLACE TABLE senate\_votes\_processed AS  
  SELECT  
      regexp\_extract(filename, 'S(\\d{3})\_votes.csv', 1\) AS session\_num,  
      CAST(icpsr AS VARCHAR) AS icpsr,  
      \* EXCLUDE (filename, icpsr)  
  FROM raw\_votes;

### **5\. Python Analysis Loop (The New Loop)**

* **Action:** The new Python loop will iterate over *session numbers* retrieved from DuckDB. We will initialize all metric lists and data structures before this loop.  
* **Logic:**  
  1. **Initialize Metric Storage:**  
     \# Ex 1  
     silhouette\_scores \= \[\]  
     \# Ex 3  
     party\_alignment\_scores \= \[\]  
     session\_viz\_data \= {} \# For interactive scatter plot  
     \# Ex 4  
     dunn\_scores \= \[\]  
     db\_scores \= \[\]  
     ch\_scores \= \[\]  
     crosstab\_separation\_scores \= \[\]  
     \# Ex 5  
     party\_cohesion\_stats \= \[\]

  2. Get Sessions:  
     sessions \= con.sql("SELECT DISTINCT session\_num FROM senate\_votes\_processed ORDER BY session\_num").df()\['session\_num'\]  
  3. Start Loop:  
     for session in sessions:  
  4. **Inside the loop (Initial Step):**  
     * query \= f"SELECT \* FROM senate\_votes\_processed WHERE session\_num \= '{session}'"  
     * session\_df \= con.sql(query).to\_pandas().set\_index('icpsr')  
     * session\_df \= session\_df.drop(columns=\['session\_num'\])  
     * (Original notebook logic: clean NaNs, run PCA, run KMeans \-\> get X\_pca, labels, session\_df.index)  
     * score \= silhouette\_score(X\_pca, labels)  
     * silhouette\_scores.append({'session': session, 'score': score})  
     * (Exercises 3, 4, and 5 will add logic here)  
  5. **After the loop:** Convert all lists into final Pandas DataFrames.  
     * silhouette\_df \= pd.DataFrame(silhouette\_scores)  
     * (Rest of DataFrame conversions)

### **6\. Plotting (Initial)**

* **Action:** After the loop, create a DataFrame silhouette\_df from the scores.  
* **Why:** This silhouette\_df is the source for the first visualization function.

## **Exercise 2: Dynamic Data Ingestion & Enhanced Plotting**

**Goal:** Fetch all Senate data up to the present (119th Congress) directly from the source, ingest it, and enhance the time-series plot to highlight significant shifts.

### **1\. Dynamic Data Fetching**

* **Action:** Python function to fetch new vote files from voteview.com. **This should be run before the main analysis loop in Ex 1.5.**  
* **Logic:**  
  1. Find Max Session: max\_session \= con.sql("SELECT MAX(CAST(session\_num AS INTEGER)) FROM senate\_votes\_processed").fetchone()\[0\]  
  2. target\_congress \= 119  
  3. new\_files\_list \= \[\]  
  4. for session in range(max\_session \+ 1, target\_congress \+ 1):  
     * session\_str \= str(session).zfill(3)  
     * url \= f"https://voteview.com/static/data/out/votes/S{session\_str}\_votes.csv"  
     * response \= requests.get(url)  
     * If response.status\_code \== 200:  
       * filepath \= f"senate\_votes/S{session\_str}\_votes.csv"  
       * with open(filepath, "wb") as f: f.write(response.content)  
       * new\_files\_list.append(filepath)  
     * else: print(f"Failed to download session {session\_str}")

### **2\. Ingestion of New Data**

* **Action:** Append newly downloaded CSVs into senate\_votes\_processed.  
* **Logic:**  
  * if new\_files\_list:  
    * con.sql(f""" INSERT INTO senate\_votes\_processed SELECT regexp\_extract(filename, 'S(\\d{{3}})\_votes.csv', 1\) AS session\_num, CAST(icpsr AS VARCHAR) AS icpsr, \* EXCLUDE (filename, icpsr) FROM read\_csv( {new\_files\_list}, auto\_detect \= true, filename \= true, union\_by\_name \= true ); """)

### **3\. Re-run Analysis**

* **Action:** The main analysis loop (Ex 1.5) is run *after* this step, so it will naturally query the fully populated senate\_votes\_processed table.

### **4\. Plotting Enhancement: Highlight Significant Shifts**

* **Action:** Enhance the silhouette score plotting function (to be defined in Ex 6).  
* **Logic:**  
  1. Input: silhouette\_df.  
  2. df\['rolling\_avg'\] \= df\['score'\].rolling(window=7, center=True, min\_periods=1).mean()  
  3. df\['delta'\] \= df\['score'\] \- df\['rolling\_avg'\]  
  4. delta\_std \= df\['delta'\].std()  
  5. df\['significant\_shift'\] \= df\['delta'\].abs() \> (1.0 \* delta\_std)  
  6. **Visualization:** The Plotly function will plot score (line), rolling\_avg (dashed line), and use significant\_shift to overlay highlighted markers (e.g., red circles).

## **Exercise 3: Cluster-Party Correlation & Interactive Analysis**

**Goal:** Quantify the alignment between K-means clusters and political parties. Create a new time-series metric and an interactive scatter plot.

### **1\. Augment Python Analysis Loop (From Ex 1.5)**

* **Action:** Add new logic *inside* the main for session in sessions: loop, after the silhouette score calculation.  
* **Logic (inside the loop):**  
  1. **Get labels, X\_pca, session\_df.index** (From existing logic).  
  2. cluster\_df \= pd.DataFrame({'cluster\_label': labels}, index=session\_df.index)  
  3. query \= f"SELECT icpsr, political\_party, senator\_name FROM members WHERE session\_num \= {session}"  
  4. party\_df \= con.sql(query).to\_pandas().set\_index('icpsr')  
  5. merged\_df \= cluster\_df.join(party\_df, on='icpsr').dropna(subset=\['political\_party'\])  
  6. **Classify Clusters:**  
     * cluster\_0\_party \= merged\_df\[merged\_df\['cluster\_label'\] \== 0\]\['political\_party'\].mode().get(0, 'Unknown')  
     * cluster\_1\_party \= merged\_df\[merged\_df\['cluster\_label'\] \== 1\]\['political\_party'\].mode().get(0, 'Unknown')  
     * cluster\_party\_map \= {0: cluster\_0\_party, 1: cluster\_1\_party}  
  7. **Calculate Mismatch Metric:**  
     * merged\_df\['cluster\_party'\] \= merged\_df\['cluster\_label'\].map(cluster\_party\_map)  
     * merged\_df\['is\_mismatch'\] \= merged\_df\['political\_party'\] \!= merged\_df\['cluster\_party'\]  
     * mismatch\_pct \= merged\_df\['is\_mismatch'\].mean() \* 100  
     * party\_alignment\_scores.append({'session': session, 'mismatch\_pct': mismatch\_pct})  
  8. **Store Data for Visualization:**  
     * pca\_df \= pd.DataFrame(X\_pca, columns=\['PC1', 'PC2'\], index=session\_df.index)  
     * session\_viz\_data\[session\] \= merged\_df.join(pca\_df)  
  * **(This merged\_df will be needed in Ex 4 and Ex 5\)**

### **2\. New Time-Series Plot (Mismatch %)**

* **Action:** Create a plotting function for party-cluster mismatch (for Ex 6).  
* **Logic:**  
  1. After loop: alignment\_df \= pd.DataFrame(party\_alignment\_scores)  
  2. Function plot\_mismatch(alignment\_df) creates a Plotly line plot of session vs. mismatch\_pct.

### **3\. Interactive Scatter Plot**

* **Action:** Create a *separate* cell (from the main dashboard) for an ipywidgets-based interactive scatter plot.  
* **Logic:**  
  1. def setup\_interactive\_scatter(sessions\_list, data\_dict):  
  2. session\_dropdown \= widgets.Dropdown(options=sessions\_list, ...)  
  3. def plot\_session\_scatter(session):  
     * df \= data\_dict\[session\]  
     * fig \= px.scatter(df, x='PC1', y='PC2', color='political\_party', symbol='cluster\_label', ...)  
     * mismatch\_df \= df\[df\['is\_mismatch'\]\]  
     * fig.add\_trace(go.Scatter(x=mismatch\_df\['PC1'\], y=mismatch\_df\['PC2'\], mode='markers', ...))  
     * fig.show()  
  4. widgets.interactive(plot\_session\_scatter, session=session\_dropdown)

## **Exercise 4: Additional Polarization Metrics**

**Goal:** Calculate and plot three new cluster validity indices (Dunn, Davies-Bouldin, Calinski-Harabasz) and one party-based separation metric (Crosstab).

### **1\. Augment Python Analysis Loop (Again)**

* **Action:** Add logic *inside* the main for session in sessions: loop, after the merged\_df from Exercise 3 is created.  
* **Logic (inside the loop):**  
  1. **Get X\_pca and labels:** (From existing logic).  
  2. **Calculate sklearn Metrics:**  
     * db\_score \= davies\_bouldin\_score(X\_pca, labels)  
     * ch\_score \= calinski\_harabasz\_score(X\_pca, labels)  
     * db\_scores.append({'session': session, 'db\_score': db\_score})  
     * ch\_scores.append({'session': session, 'ch\_score': ch\_score})  
  3. **Calculate Dunn Index:**  
     * dunn\_score \= dunn(X\_pca, labels)  
     * dunn\_scores.append({'session': session, 'dunn\_score': dunn\_score})  
  4. **Calculate Crosstab Separation Metric:**  
     * Get merged\_df (From Exercise 3 logic).  
     * ct \= pd.crosstab(merged\_df\['political\_party'\], merged\_df\['cluster\_label'\])  
     * Filter for 'D', 'R': ct\_main\_parties \= ct.loc\[ct.index.isin(\['D', 'R'\])\]  
     * (Handle cases with 0/1 parties/clusters, e.g., using .reindex())  
     * if 0 not in ct\_main\_parties.columns: ct\_main\_parties\[0\] \= 0  
     * if 1 not in ct\_main\_parties.columns: ct\_main\_parties\[1\] \= 0  
     * if 'D' not in ct\_main\_parties.index: ct\_main\_parties.loc\['D'\] \= 0  
     * if 'R' not in ct\_main\_parties.index: ct\_main\_parties.loc\['R'\] \= 0  
     * scenario\_1\_correct \= ct\_main\_parties.loc\['D', 0\] \+ ct\_main\_parties.loc\['R', 1\]  
     * scenario\_2\_correct \= ct\_main\_parties.loc\['D', 1\] \+ ct\_main\_parties.loc\['R', 0\]  
     * total\_members \= ct\_main\_parties.sum().sum()  
     * separation\_pct \= max(scenario\_1\_correct, scenario\_2\_correct) / total\_members \* 100 if total\_members \> 0 else 0  
     * crosstab\_separation\_scores.append({'session': session, 'separation\_pct': separation\_pct})

### **2\. Store and Plot**

* **Action:** After the loop, convert all new score lists into Pandas DataFrames (dunn\_df, db\_df, ch\_df, crosstab\_df).  
* **Next Step:** These DataFrames are ready to be used by plotting functions for the Exercise 6 dashboard.

## **Exercise 5: Intra-Party Cohesion (Cosine Similarity)**

**Goal:** Calculate party cohesion using cosine similarity on PCA vectors. Store individual senator scores in DuckDB and plot aggregate (mean, median, variance) cohesion over time.

### **1\. Create New DuckDB Table (One-Time)**

* **Action:** Before the loop, create a persistent table to store *individual* senator cohesion scores. This will be used in Exercise 6\.  
* **Sample SQL:**  
  CREATE OR REPLACE TABLE senator\_party\_cohesion (  
      session\_num VARCHAR,  
      icpsr VARCHAR,  
      political\_party VARCHAR,  
      cohesion\_score DOUBLE  
  );

### **2\. Augment Python Analysis Loop (Again)**

* **Action:** Add logic *inside* the main for session in sessions: loop, after X\_pca and merged\_df (from Ex 3\) are ready.  
* **Logic (inside the loop):**  
  1. **Get X\_pca and merged\_df:** (From existing logic).  
  2. **Join PCA and Party Info:**  
     * pca\_df \= pd.DataFrame(X\_pca, columns=\['PC1', 'PC2'\], index=session\_df.index)  
     * pca\_party\_df \= merged\_df.join(pca\_df).dropna(subset=\['political\_party', 'PC1'\])  
     * pca\_party\_df \= pca\_party\_df\[pca\_party\_df\['political\_party'\].isin(\['D', 'R'\])\]  
  3. **Calculate Cohesion Scores:**  
     * session\_party\_scores \= \[\] (temp list for this session's senators)  
     * for party in \['D', 'R'\]:  
       * party\_df \= pca\_party\_df\[pca\_party\_df\['political\_party'\] \== party\]  
       * if len(party\_df) \< 2: continue (Can't compare cohesion for a single senator)  
       * vectors \= pca\_df.loc\[party\_df.index, \['PC1', 'PC2'\]\]  
       * sim\_matrix \= cosine\_similarity(vectors)  
       * np.fill\_diagonal(sim\_matrix, np.nan) (Ignore self-similarity)  
       * \# Calculate mean similarity for each senator to their party peers  
       * mean\_similarities \= np.nanmean(sim\_matrix, axis=1)  
       * temp\_scores\_df \= pd.DataFrame({ 'icpsr': party\_df.index, 'political\_party': party, 'cohesion\_score': mean\_similarities, 'session\_num': session })  
       * session\_party\_scores.append(temp\_scores\_df)  
  4. **Store Individual Scores in DuckDB (for Ex 6):**  
     * if session\_party\_scores:  
       * session\_cohesion\_df \= pd.concat(session\_party\_scores)  
       * \# Register as temp view to load into persistent table  
       * con.register('session\_cohesion\_df\_view', session\_cohesion\_df)  
       * con.sql("INSERT INTO senator\_party\_cohesion SELECT \* FROM session\_cohesion\_df\_view")  
  5. **Calculate and Store Aggregate Stats (for Ex 5 Plot):**  
     * if session\_party\_scores:  
       * d\_scores \= session\_cohesion\_df\[session\_cohesion\_df\['political\_party'\] \== 'D'\]\['cohesion\_score'\]  
       * r\_scores \= session\_cohesion\_df\[session\_cohesion\_df\['political\_party'\] \== 'R'\]\['cohesion\_score'\]  
       * stats \= {'session': session}  
       * stats\['d\_mean\_cohesion'\] \= d\_scores.mean() if not d\_scores.empty else np.nan  
       * stats\['d\_median\_cohesion'\] \= d\_scores.median() if not d\_scores.empty else np.nan  
       * stats\['d\_var\_cohesion'\] \= d\_scores.var() if not d\_scores.empty else np.nan  
       * stats\['r\_mean\_cohesion'\] \= r\_scores.mean() if not r\_scores.empty else np.nan  
       * stats\['r\_median\_cohesion'\] \= r\_scores.median() if not r\_scores.empty else np.nan  
       * stats\['r\_var\_cohesion'\] \= r\_scores.var() if not r\_scores.empty else np.nan  
       * party\_cohesion\_stats.append(stats)

### **3\. New Time-Series Plots (Party Cohesion)**

* **Action:** Create plotting functions for aggregate party cohesion (for Ex 6).  
* **Logic:**  
  1. After loop: cohesion\_stats\_df \= pd.DataFrame(party\_cohesion\_stats)  
  2. def plot\_cohesion\_mean\_median(cohesion\_stats\_df):  
     * Plot session vs. d\_mean\_cohesion (blue line)  
     * Plot session vs. r\_mean\_cohesion (red line)  
  3. def plot\_cohesion\_variance(cohesion\_stats\_df):  
     * Plot session vs. d\_var\_cohesion (blue line)  
     * Plot session vs. r\_var\_cohesion (red line)

## **Exercise 6: Visualization Dashboard with Dash**

**Goal:** Create a 2x3 grid of interactive time-series plots using Dash to display all polarization metrics.

### **1\. Define Plotly-based Plotting Functions**

* **Action:** Create a set of functions that take a DataFrame and return a plotly.graph\_objects figure.  
* **Example plot\_silhouette(df):**  
  def plot\_silhouette(df):  
      \# Add rolling avg and shift columns  
      df\['rolling\_avg'\] \= df\['score'\].rolling(window=7, center=True, min\_periods=1).mean()  
      df\['delta'\] \= df\['score'\] \- df\['rolling\_avg'\]  
      delta\_std \= df\['delta'\].std()  
      df\['significant\_shift'\] \= df\['delta'\].abs() \> (1.0 \* delta\_std)  
      shift\_df \= df\[df\['significant\_shift'\]\]

      \# Create plot  
      fig \= px.line(df, x='session', y='score', title='Silhouette Score (Polarization)')

      \# Add rolling average trace  
      fig.add\_trace(go.Scatter(x=df\['session'\], y=df\['rolling\_avg'\], mode='lines',  
                               line=dict(dash='dash', color='gray'), name='Rolling Avg.'))

      \# Add significant shift markers  
      fig.add\_trace(go.Scatter(x=shift\_df\['session'\], y=shift\_df\['score'\], mode='markers',  
                               marker=dict(color='red', size=8, symbol='circle-open'), name='Significant Shift'))

      fig.update\_layout(showlegend=False)  
      return fig

* **Other functions to create:**  
  * def plot\_mismatch(alignment\_df):  
    * return px.line(alignment\_df, x='session', y='mismatch\_pct', title='Party-Cluster Mismatch %')  
  * def plot\_cohesion(cohesion\_stats\_df):  
    * return px.line(cohesion\_stats\_df, x='session', y=\['d\_mean\_cohesion', 'r\_mean\_cohesion'\], title='Mean Party Cohesion (Cosine Sim.)')  
  * def plot\_crosstab\_sep(crosstab\_df):  
    * return px.line(crosstab\_df, x='session', y='separation\_pct', title='Crosstab Party Separation %')  
  * def plot\_cluster\_indices(db\_df, ch\_df, dunn\_df):  
    * **Note:** These indices have different scales. Plotting on one chart is difficult. It's better to make 2-3 separate, smaller plots or use subplots. For simplicity, we can plot them separately.  
    * fig\_db \= px.line(db\_df, x='session', y='db\_score', title='Davies-Bouldin')  
    * fig\_ch \= px.line(ch\_df, x='session', y='ch\_score', title='Calinski-Harabasz')  
    * fig\_dunn \= px.line(dunn\_df, x='session', y='dunn\_score', title='Dunn Index')  
    * (We will need to adjust the 2x3 grid logic to accommodate this)  
  * def plot\_cohesion\_variance(cohesion\_stats\_df):  
    * return px.line(cohesion\_stats\_df, x='session', y=\['d\_var\_cohesion', 'r\_var\_cohesion'\], title='Party Cohesion Variance')

### **2\. Define Dashboard Layout**

* **Action:** In a new notebook cell, define the Dash app layout. We'll adjust the grid to accommodate all our plots. A 3x2 grid might be better.  
* **Logic:**  
  app \= Dash(\_\_name\_\_)

  \# \--- Create all figure objects AFTER the loop \---  
  fig\_silhouette \= plot\_silhouette(silhouette\_df)  
  fig\_crosstab \= plot\_crosstab\_sep(crosstab\_df)  
  fig\_mismatch \= plot\_mismatch(alignment\_df)  
  fig\_cohesion \= plot\_cohesion(cohesion\_stats\_df)  
  fig\_cohesion\_var \= plot\_cohesion\_variance(cohesion\_stats\_df)

  \# For cluster indices, let's combine DB and Dunn (lower is better)  
  \# and plot CH (higher is better) separately.  
  fig\_db\_dunn \= go.Figure()  
  fig\_db\_dunn.add\_trace(go.Scatter(x=db\_df\['session'\], y=db\_df\['db\_score'\], name='Davies-Bouldin'))  
  fig\_db\_dunn.add\_trace(go.Scatter(x=dunn\_df\['session'\], y=dunn\_df\['dunn\_score'\], name='Dunn Index', yaxis='y2'))  
  fig\_db\_dunn.update\_layout(  
      title='Cluster Indices (Lower=Better)',  
      yaxis=dict(title='DB Score'),  
      yaxis2=dict(title='Dunn Score', overlaying='y', side='right')  
  )  
  \# (Note: Calinski-Harabasz has a very different scale, maybe replace cohesion variance)

  app.layout \= html.Div(\[  
      html.H1("Senate Polarization Dashboard"),

      html.Div(className="plot-grid", style={'display': 'grid', 'gridTemplateColumns': '1fr 1fr 1fr', 'gap': '10px'}, children=\[

          \# Row 1  
          dcc.Graph(id='silhouette-plot', figure=fig\_silhouette),  
          dcc.Graph(id='crosstab-plot', figure=fig\_crosstab),  
          dcc.Graph(id='mismatch-plot', figure=fig\_mismatch),

          \# Row 2  
          dcc.Graph(id='cohesion-plot', figure=fig\_cohesion),  
          dcc.Graph(id='cohesion-var-plot', figure=fig\_cohesion\_var),  
          dcc.Graph(id='db-dunn-plot', figure=fig\_db\_dunn) \# 6th plot  
      \])  
  \])

### **3\. Run the Dashboard**

* **Action:** Run the app *inline* in the notebook.  
* **Logic:**  
  app.run(jupyter\_mode="inline", port=8050)

  This will output a link and render the interactive dashboard directly in the notebook's output cell.

### **4\. Stretch Goal (If Time Permits): Add Callback Interactivity**

* **Action:** Add a dropdown to select a senator and plot their individual cohesion score over time by querying the DuckDB table.  
* **Layout Addition:**  
  \# Get senator list (e.g., from last session)  
  \# This query is better as it gets all senators who \*ever\* had a cohesion score  
  senator\_options\_query \= """  
      SELECT DISTINCT m.senator\_name, m.icpsr  
      FROM members m  
      JOIN senator\_party\_cohesion c ON m.icpsr \= c.icpsr  
  """  
  senator\_options \= \[{'label': row\[0\], 'value': row\[1\]}  
                     for row in con.sql(senator\_options\_query).fetchall()\]

  app.layout.children.append(  
      html.Div(\[  
          html.H2("Individual Senator Cohesion"),  
          dcc.Dropdown(id='senator-dropdown', options=senator\_options, value=senator\_options\[0\]\['value'\]),  
          dcc.Graph(id='individual-cohesion-plot')  
      \])  
  )

* **Callback Function:**  
  @app.callback(  
      Output('individual-cohesion-plot', 'figure'),  
      Input('senator-dropdown', 'value')  
  )  
  def update\_senator\_plot(selected\_icpsr):  
      \# Query DuckDB for this senator's full history  
      query \= f"""  
          SELECT c.session\_num, c.cohesion\_score, m.political\_party  
          FROM senator\_party\_cohesion c  
          JOIN members m ON c.icpsr \= m.icpsr AND c.session\_num \= m.session\_num  
          WHERE c.icpsr \= '{selected\_icpsr}'  
          ORDER BY c.session\_num  
      """  
      senator\_df \= con.sql(query).to\_pandas()

      if senator\_df.empty:  
          return go.Figure().update\_layout(title=f"No cohesion data for selected senator")

      party \= senator\_df\['political\_party'\].iloc\[0\]  
      if party not in \['D', 'R'\]:  
           return go.Figure().update\_layout(title=f"No cohesion data for {party} party members")

      \# Get party average  
      party\_avg\_df \= cohesion\_stats\_df\[\['session', f'{party.lower()}\_mean\_cohesion'\]\]  
      party\_avg\_df \= party\_avg\_df.rename(columns={'session': 'session\_num', f'{party.lower()}\_mean\_cohesion': 'party\_average'})

      \# Join and plot  
      plot\_df \= senator\_df.merge(party\_avg\_df, on='session\_num')

      fig \= px.line(plot\_df, x='session\_num', y=\['cohesion\_score', 'party\_average'\],  
                    title=f"Cohesion Score vs. Party Average ({party})")  
      fig.update\_traces(selector=dict(name='cohesion\_score'), line=dict(width=4))  
      fig.update\_traces(selector=dict(name='party\_average'), line=dict(dash='dash'))

      return fig

## **References**

### **Data Sources**

* **VoteView Data Portal:** https://voteview.com/data  
* **Senate Vote CSVs (Pattern):** https://voteview.com/static/data/out/votes/S\<NUM\>\_votes.csv  
* **All Members Metadata:** https://voteview.com/static/data/out/members/HSall\_members.csv

### **DuckDB**

* **Multiple CSV Files Overview:** https://duckdb.org/docs/stable/data/multiple\_files/overview\#csv  
* **Python CSV Ingestion:** https://duckdb.org/docs/stable/clients/python/data\_img/data\_ingestion\#csv-files  
* **CSV Reading Tips:** https://duckdb.org/docs/stable/data/csv/tips  
* **Handling Faulty CSVs:** https://duckdb.org/docs/stable/data/csv/reading\_faulty\_csv\_files  
* **DuckDB Tricks (Blog):** https://duckdb.org/2024/08/19/duckdb-tricks-part-1  
* **Taming Wild CSVs (Blog):** https://motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering/  
* **Persisting CSVs (Blog):** https://motherduck.com/blog/csv-files-persist-duckdb-solution/  
* **Video: Taming Wild CSVs:** https://www.youtube.com/watch?v=pHeVP92O9zc  
* **DataFrame Glossary:** https://motherduck.com/glossary/DataFrame/

### **Python Libraries & Techniques**

* **defaultdict Tutorial:** https://realpython.com/python-defaultdict/  
* **Handling KeyError:** https://www.datacamp.com/tutorial/python-keyerror  
* **Dunn Index Implementation Repo:** https://github.com/jqmviegas/jqm\_cvi/tree/master

### **Polarization Metrics**

* **Dunn Index:**  
  * GeeksforGeeks: https://www.geeksforGeeks.org/machine-learning/dunn-index-and-db-index-cluster-validity-indices-set-1/  
  * Theory PDF: https://github.com/jqmviegas/jqm\_cvi/blob/master/theory.pdf  
  * Wikipedia: https://en.wikipedia.org/wiki/Dunn\_index  
* **Davies-Bouldin Index:**  
  * GeeksforGeeks: https://www.geeksforGeeks.org/machine-learning/davies-bouldin-index/  
  * Wikipedia: https://en.wikipedia.org/wiki/Davies%E2%80%93Bouldin\_index  
  * scikit-learn Docs: https://scikit-learn.org/stable/modules/clustering.html\#davies-bouldin-index  
  * scikit-learn API: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.davies\_bouldin\_score.html  
* **Calinski-Harabasz Index:**  
  * Towards Data Science: https://towardsdatascience.com/calinski-harabasz-index-for-k-means-clustering-evaluation-using-python-4fefeeb2988e/  
  * Wikipedia: https://en.wikipedia.org/wiki/Calinski%E2%80%93Harabasz\_index  
  * scikit-learn Docs: https://scikit-learn.org/stable/modules/clustering.html\#calinski-harabasz-index  
* scikit-learn API: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.calinski\_harabasz\_score.html

### **Plotly & Dash**

* **Plotly Getting Started:** https://plotly.com/python/getting-started/  
* **Plotly v3 Notebook Example:** https://plotly.com/python/v3/ipython-notebooks/baltimore-vital-signs/  
* **GeeksforGeeks Plotly Tutorial:** https://www.geeksforGeeks.org/data-visualization/using-plotly-for-interactive-data-visualization-in-python/  
* **Dash Tutorial:** https://dash.plotly.com/tutorial  
* **Dash in Jupyter:** https://dash.plotly.com/dash-in-jupyter  
* **Jupyter Support Update (GitHub):** https://github.com/plotly/jupyter-dash?tab=readme-ov-file\#notice-as-of-dash-v211-jupyter-support-is-built-into-the-main-dash-package  
* **Plotly Figure Structure:** https://plotly.com/python/figure-structure/  
* **Creating & Updating Figures:** https://plotly.com/python/creating-and-updating-figures/