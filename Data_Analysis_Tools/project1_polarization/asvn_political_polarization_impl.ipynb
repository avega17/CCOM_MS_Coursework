{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b278f7c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# /// script\n",
    "# requires-python = \">=3.11\"\n",
    "# dependencies = [\n",
    "#     \"dash>=2.16\",\n",
    "#     \"duckdb~=1.4\",\n",
    "#     \"ipywidgets>=8.1\",\n",
    "#     \"juv>=0.4.0\",\n",
    "#     \"matplotlib>=3.8\",\n",
    "#     \"numpy>=1.06\",\n",
    "#     \"pandas>=2.2\",\n",
    "#     \"plotly>=5.24\",\n",
    "#     \"python-dotenv>=1.0\",\n",
    "#     \"requests>=2.32\",\n",
    "#     \"scikit-learn>=1.4\",\n",
    "# ]\n",
    "# ///"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f2c12f",
   "metadata": {},
   "source": [
    "# Quantifying Political Polarization in the US Senate\n",
    "**Project Title:** Senate Voting Polarization Analysis (87th-119th Congress)\n",
    "**Course:** CCOM6994: Data Analysis Tools\n",
    "**Author:** Alejandro S. Vega Nogales\n",
    "---\n",
    "## Project Overview\n",
    "![image.png](attachment:image.png)\n",
    "This project investigates the question: **Has US Senate voting become more polarized across time?**\n",
    "We analyze voting data from the 1st through 119th Congress using basic clustering algorithms, dimensionality reduction (PCA), and multiple polarization metrics. The analysis leverages **DuckDB** for efficient parallel data ingestion and SQL-based processing, enabling us to handle 100+ CSV files containing millions of vote records.\n",
    "### Key Questions:\n",
    "- How has cluster separation (Silhouette Score) evolved over time?\n",
    "- Do K-means clusters align with political party membership?\n",
    "- How do different polarization metrics (Dunn Index, Davies-Bouldin, Calinski-Harabasz) correlate?\n",
    "- Has intra-party cohesion changed over time?\n",
    "### Data Source:\n",
    "Vote data comes from [VoteView](https://voteview.com/data), which provides roll-call voting records for every member of Congress. Each CSV contains votes for a 2-year congressional session, with each row representing one senator's vote on one bill. Congress dates for each session are fetched from static html tables on [Senate.gov](https://www.senate.gov/legislative/DatesofSessionsofCongress.htm).|\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "This project will focus on the specific question: Has US Senate voting become more polarized across time? To determine this, we will use a data set containing votes of each member in csv files of each 2 year interval of the Senate. Our main data science tool will be the use of clustering algorithms and a visual comparison using scatter plots of yea/nay votes of each Senate as well as a principal component analysis of the more detailed voting patterns.\n",
    "\n",
    "The overall goal is to try and learn something about the trend in voting patterns in the Senate over time. This notebook will outline a specific way of performing the analysis, but you are welcome to extend the analysis a number of ways, include those described at the end of the notebook. The specific data you will deal with will be the yea/nay/abstain voting choices of senators on each bill voted on during a 2 year session. Clustering will not simply divide senators based on their total yea/nay vote count, but rather based on how often they tend to vote with other members of the senate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae27ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install pip requirements in notebook in case user only has notebook with no uv toml or pip requirements\n",
    "pip_reqs = [\n",
    "    \"duckdb\",\n",
    "    \"pandas\",\n",
    "    \"numpy\",\n",
    "    \"scikit-learn\",\n",
    "    \"plotly\",\n",
    "    \"dash\",\n",
    "    \"requests\",\n",
    "    \"ipywidgets\",\n",
    "    \"matplotlib\",\n",
    "    \"python-dotenv\",\n",
    "    \"juv\",\n",
    "    \"ipykernel\",\n",
    "    \"lxml\",\n",
    "    \"tqdm\"\n",
    "    \"ipywidgets\"\n",
    "]\n",
    "\n",
    "!pip install {' '.join(pip_reqs)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024f2b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarly for a colab environment where we lack our utils dir and other files, fetch the github repo with this project and copy over the utils dir if not present\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "utils_dir = Path(\"./utils\")\n",
    "if not utils_dir.exists():\n",
    "    !git clone https://github.com/avega17/CCOM_MS_Coursework.git\n",
    "    shutil.copytree(\"CCOM_MS_Coursework/Data_Analysis_Tools/project1_polarization/utils\", \"utils\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b814a7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# centralize imports\n",
    "from utils.config import Settings, get_settings\n",
    "from utils.ingest import (\n",
    "    create_processed_vote_table,\n",
    "    ensure_vote_files,\n",
    "    fetch_congress_dates,\n",
    "    get_missing_sessions,\n",
    "    ingest_member_metadata,\n",
    "    ingest_vote_files,\n",
    "    initialize_database,\n",
    "    load_congress_dates,\n",
    "    summarize_members_file_storage,\n",
    "    summarize_duckdb_size,\n",
    "    summarize_vote_file_storage,\n",
    " )\n",
    "from utils.transforms import (\n",
    "    compute_session_silhouette,\n",
    "    load_silhouette_enriched,\n",
    "    prepare_session_matrix,\n",
    "    refresh_silhouette_enriched_table,\n",
    " )\n",
    "from utils.visualizations import (\n",
    "    build_silhouette_shift_figure,\n",
    "    build_party_mismatch_figure,\n",
    " )\n",
    "from utils.benchmarks import (\n",
    "    benchmark_duckdb_local_ingest,\n",
    "    benchmark_duckdb_remote_fetch,\n",
    "    benchmark_pandas_bulk_load,\n",
    " )\n",
    "\n",
    "# data and data science\n",
    "import duckdb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "\n",
    "# visualization and interactive plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from ipywidgets import IntRangeSlider, IntSlider, Layout, interact\n",
    "from ipywidgets import Dropdown, VBox, HBox, Output\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import time\n",
    "from pprint import pprint\n",
    "from pathlib import Path\n",
    "import contextlib\n",
    "from typing import Iterable, Optional\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "load_dotenv()  # take environment variables from .env file if present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c4a66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out .env file if not present since we have no sensitive credentials info\n",
    "proj_env = \"\"\"\n",
    "DUCKDB_PATH=senate_analysis.duckdb\n",
    "TEMP_DIRECTORY=.duckdb-temp\n",
    "VOTES_DIR=senate_dataset\n",
    "VOTES_GLOB=senate_dataset/S*_votes.csv\n",
    "MEMBERS_URI=https://voteview.com/static/data/out/members/HSall_members.csv\n",
    "VOTE_URI_TEMPLATE=https://voteview.com/static/data/out/votes/S{session}_votes.csv\n",
    "MEMBERS_LOCAL_PATH=senate_dataset/HSall_members.csv\n",
    "RAW_VOTES_TABLE=raw_votes\n",
    "PROCESSED_VOTES_TABLE=senate_votes_processed\n",
    "MEMBERS_TABLE=senate_members\n",
    "\"\"\"\n",
    "env_path = Path(\".env\")\n",
    "if not env_path.exists():\n",
    "    with open(env_path, \"w\") as f:\n",
    "        f.write(proj_env.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf2c15c",
   "metadata": {},
   "source": [
    "We will begin by having you load in the csv file S116_votes.csv as a dataframe and print out the head of the data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b8f8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prefix all reads of local files with this dir\n",
    "LOCAL_VOTES_DIR = \"senate_dataset\"\n",
    "# The following legacy code (from the original assignment) requires specific CSV files to be present locally.\n",
    "# We'll verify they exist and fetch them if missing, so this notebook runs end-to-end even if the dataset directory is empty.\n",
    "filename = \"S116_votes.csv\"\n",
    "filepath = Path(LOCAL_VOTES_DIR) / filename\n",
    "if not filepath.exists():\n",
    "    # Extract session number from filename (e.g., \"S116_votes.csv\" -> \"116\")\n",
    "    session_num = filename.replace(\"S\", \"\").replace(\"_votes.csv\", \"\")\n",
    "    url = f\"https://voteview.com/static/data/out/votes/{filename}\"\n",
    "    print(f\"  Fetching missing file: {filename} from {url}\")\n",
    "    # use duckdb to fetch single file and copy to expected local dir location\n",
    "    conn = duckdb.connect()\n",
    "    conn.execute(f\"\"\"CREATE OR REPLACE TEMP TABLE fetched_votes AS (SELECT * FROM read_csv('{url}', all_varchar=True))\"\"\")\n",
    "    conn.execute(f\"COPY fetched_votes TO '{filepath}' (FORMAT 'csv', HEADER TRUE, OVERWRITE_OR_IGNORE TRUE);\")\n",
    "    conn.close()\n",
    "    print(f\"  ✓ Downloaded {filename}\")\n",
    "\n",
    "S116 = pd.read_csv(os.path.join(LOCAL_VOTES_DIR, 'S116_votes.csv'))\n",
    "\n",
    "S116.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58dd8584",
   "metadata": {},
   "source": [
    "You will notice that the data is in **long form**, so that each row is a different senator vote per congress: the congress (116) in the first column; the chamber (Senate) along the second column; the roll number (this is the vote number) along the third; icpsr is an identifier for each member in the fourth column; cast code is 1 if yea, 6 is nay, and 9 if abstained; and prob is an estimated probability of that vote based on a model generated by the researchers running the voteview website. See the website here for more details on this data set:\n",
    "https://voteview.com/articles/data_help_votes\n",
    "\n",
    "Note, there are other versions of yeas that could appear which are 2 and 3, other versions of nays which could appear are 4 and 5, and if there are 7 and 8, then we'll count these as abstains.\n",
    "\n",
    "Recall here is the distinction between wide and long/narrow form data:\n",
    "https://en.wikipedia.org/wiki/Wide_and_narrow_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4821bd6",
   "metadata": {},
   "source": [
    "Now, we'd like to clean the data up a bit, so that we can much more clearly represent the voting trends of different members. To do so, create a pivot table called 'S116_piv' with the values being the case_code, the index being the icpsr, and the columns being the rollnumber (or vote). After you make the pivot table, flatten it to get rid of the extra header using the command *S116_tab=pd.DataFrame(S116_piv.to_records())*. This also converts each record to a Numpy array and adds indexes to the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d344acd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "S116_tab = pd.pivot_table(S116, values='cast_code', index='icpsr',columns='rollnumber')\n",
    "S116_tab = pd.DataFrame(S116_tab.to_records())\n",
    "S116_tab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3a7255",
   "metadata": {},
   "source": [
    "You should see that there are 102 rows and 721 columns. This is because two senators were replaced during the 116th congress (from 2019 to 2021) -- one in Arizona and the other in Georgia.\n",
    "\n",
    "Now, just to get some consistency with the data we worked with in Module 4 on clustering, let's switch to a 1=yea, 0.5=abstain, and 0=nay convention. We'll use the replace functions, so that any 1, 2, or 3 will become a 1; any 4,5,6 will become a 0; any 7,8,9 will become a 0.5. We will list the way to do this for yeas below, you will need to write two more lines, one each for the nay and abstain replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c412fe72",
   "metadata": {},
   "outputs": [],
   "source": [
    "S116_tab=S116_tab.replace([1, 2, 3], 1)\n",
    "S116_tab=S116_tab.replace([4,5,6], 0)\n",
    "S116_tab=S116_tab.replace([7, 8, 9.0], 0.5)\n",
    "S116_tab.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d70d0ed",
   "metadata": {},
   "source": [
    "Before we move on, let's count the number of NaN entries using the *.isna().sum().sum()* extension on the dataframe you have created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca6d91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "S116_tab.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730d6229",
   "metadata": {},
   "source": [
    "Noticeably there are quite a few NaN entries, which will disrupt our model fitting. However, it is possible to clean the data to avoid this. We could do this a number of ways. One way would simply be to drop all the columns that have NaNs. Let's do this, using the *.fillna(0.5)* function so that an NaN is treated like an abstention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6d85b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "S116_tab = S116_tab.fillna(0.5)\n",
    "S116_tab.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decefe49",
   "metadata": {},
   "source": [
    "Now check again how many NaN values there are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d5fd8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d355bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "S116_tab.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a101e4",
   "metadata": {},
   "source": [
    "Now we're primarily interested in the yea/nay comparison, so let's get a sense of how many 1's and 6's there are in each row.\n",
    "\n",
    "To do this, use the count_nonzero function from numpy (np.count_nonzero).\n",
    "https://numpy.org/doc/stable/reference/generated/numpy.count_nonzero.html\n",
    "\n",
    "NOTE: it may seem contradictory to use \"non_zero\" to count zeros. Read the documentation but remember that we converted our records using Numpy; therefore, Numpy methods will perform much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6ec663",
   "metadata": {},
   "outputs": [],
   "source": [
    "S116_tab_yn = S116_tab.copy(deep=True) #a \"true\" copy\n",
    "S116_tab_yn['yeas']=np.count_nonzero(S116_tab == 1, axis=1)\n",
    "S116_tab_yn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635132f1",
   "metadata": {},
   "source": [
    "Now add a column called 'nays' that enumerates the nays in each row (0's) and 'abs' that enumerates the abstentions in each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2d2bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "S116_tab_yn['nays']=np.count_nonzero(S116_tab_yn == 0, axis=1)\n",
    "S116_tab_yn['abs']=np.count_nonzero(S116_tab_yn == 0.5, axis=1)\n",
    "S116_tab_yn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6b9e77",
   "metadata": {},
   "source": [
    "Now, with each of these columns in hand, create a labeled scatter plot where each senator is a data point whose yea count is along the x-axis and nay count is along the y-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e2ff85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a scatter plot\n",
    "plt.scatter(S116_tab_yn['yeas'], S116_tab_yn['nays'])\n",
    "# set a title and labels\n",
    "plt.title('116th US Senate Opinion')\n",
    "plt.xlabel('Yeas')\n",
    "plt.ylabel('Nays')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947f7717",
   "metadata": {},
   "source": [
    "You should see now that there is some vague separation into factions, where one group votes yes very often and another group is distributed across saying yes about half the time or less. We will now use clustering to see how statistically these groups can be distinguished."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc31278",
   "metadata": {},
   "source": [
    "Alternatively, you can also represent the first two principal components of the voting data, to get a more detailed description of where each senator lies in the space of voting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a3b384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the PCA method and extract two directions from the data\n",
    "pca_2 = PCA(2)\n",
    "\n",
    "# now turn the vote data into two columns using PCA\n",
    "S116_num = S116_tab.drop(['icpsr'], axis=1)\n",
    "S116_pca_col = pca_2.fit_transform(S116_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48fbd4f",
   "metadata": {},
   "source": [
    "Now plot the first two principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff59e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(S116_pca_col[:,0], S116_pca_col[:,1])\n",
    "# set a title and labels\n",
    "plt.title('PCA Projection of 116th Senate', fontsize=24)\n",
    "plt.xlabel('PCA 1', fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.ylabel('PCA 2',fontsize=20)\n",
    "plt.yticks(fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b072021",
   "metadata": {},
   "source": [
    "Do you see a clear clustering along the first two principal components? Would you have expected that based on what we saw in the yea/nay plots?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad664fd",
   "metadata": {},
   "source": [
    "Yes, it looks like the primary clustering is along the first principal component. We would not necessarily expect this just based on the yea/nay plots because senators may not always vote yea and nay on the same things."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e9be0b",
   "metadata": {},
   "source": [
    "Now perform a K-means two cluster model on the data. Don't forget to make a new dataframe where you remove the icpsr column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbd9ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans(n_clusters=2,n_init=10)\n",
    "S116_raw = S116_tab.iloc[:, 1:]\n",
    "model.fit(S116_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360f43c4",
   "metadata": {},
   "source": [
    "Now put together the PCA scatter with the labels generated from clustering. Plot a scatter plot of the first two principal components with each data point color labeled by its cluster identity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0338a0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(S116_pca_col[:,0], S116_pca_col[:,1], c=model.labels_)\n",
    "\n",
    "plt.title('Clustering of 116th Senate', fontsize=24)\n",
    "plt.xlabel('PCA 1', fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.ylabel('PCA 2',fontsize=20)\n",
    "plt.yticks(fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977b80c0",
   "metadata": {},
   "source": [
    "What do you find? Is it true that the first two principal components do a good job of separating the clusters or is there structure beyond that is not captured? If not, it may be that you did not remove the dependence on the senator ID number."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2e7bfb",
   "metadata": {},
   "source": [
    "Indeed, we find that the clusters are well separated by the first two principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557929ee",
   "metadata": {},
   "source": [
    "Now try plotting the yea/nay data along with the color cluster labels to see if that reasonably separates the two clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add0dff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a scatter plot\n",
    "plt.scatter(S116_tab_yn['yeas'], S116_tab_yn['nays'], c = model.labels_)\n",
    "# set a title and labels\n",
    "plt.title('116th US Senate Opinion')\n",
    "plt.xlabel('Yeas')\n",
    "plt.ylabel('Nays')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795ad96a",
   "metadata": {},
   "source": [
    "Again, it seems that the two clusters are well separated just by yea and nay votes. This suggests that the senators that tend to vote yea a lot tend to vote together and that the senators that vote nay more than about 150 times tend to vote together, so Senate voting is reasonably polarized. We can produce a quantitative measure of how well separated the clusters are by computing the silhouette score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cc1616",
   "metadata": {},
   "outputs": [],
   "source": [
    "silh_score_116 = silhouette_score(S116_raw, model.labels_, metric='euclidean')\n",
    "print(silh_score_116)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6d9349",
   "metadata": {},
   "source": [
    "The silhouette score is about 0.54, suggesting that cluster membership is fairly well identified."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381f46ee",
   "metadata": {},
   "source": [
    "Make sure that you have the silhouette score saved as a variable that will not be overwritten.\n",
    "\n",
    "Now, repeat the above analysis, but for the oldest data set we have on the Senate, S087_votes.csv. Comment on what you see when you create the scatter plots and separate the data into two clusters. Make sure and compute the silhouette score for the clustering and compare it to that obtained for the 116th Senate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee035308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat the same process for the 87th senate\n",
    "filepath = Path(LOCAL_VOTES_DIR) / \"S087_votes.csv\"\n",
    "if not filepath.exists():\n",
    "    url = \"https://voteview.com/static/data/out/votes/S087_votes.csv\"\n",
    "    print(f\"  Fetching missing file: S087_votes.csv from {url}\")\n",
    "    # use duckdb to fetch single file and copy to expected local dir location\n",
    "    conn = duckdb.connect()\n",
    "    conn.execute(f\"\"\"CREATE OR REPLACE TEMP TABLE fetched_votes AS (SELECT * FROM read_csv('{url}', all_varchar=True))\"\"\")\n",
    "    conn.execute(f\"COPY fetched_votes TO 'senate_dataset/S087_votes.csv' (FORMAT 'csv', HEADER TRUE, OVERWRITE_OR_IGNORE TRUE);\")\n",
    "    conn.close()\n",
    "    print(f\"  ✓ Downloaded S087_votes.csv\")\n",
    "\n",
    "S87 = pd.read_csv(os.path.join(LOCAL_VOTES_DIR, 'S087_votes.csv'))\n",
    "\n",
    "S87_piv = pd.pivot_table(S87, values='cast_code', index='icpsr',columns='rollnumber')\n",
    "S87_tab = pd.DataFrame(S87_piv.to_records())\n",
    "\n",
    "S87_tab=S87_tab.replace([1, 2, 3], 1)\n",
    "S87_tab=S87_tab.replace([4,5,6], 0)\n",
    "S87_tab=S87_tab.replace([7, 8, 9.0], 0.5)\n",
    "\n",
    "S87_tab = S87_tab.fillna(0.5)\n",
    "\n",
    "S87_tab_yn = S87_tab.copy(deep=True)\n",
    "S87_tab_yn['yeas']=np.count_nonzero(S87_tab == 1, axis=1)\n",
    "S87_tab_yn['nays']=np.count_nonzero(S87_tab_yn == 0, axis=1)\n",
    "S87_tab_yn['abs']=np.count_nonzero(S87_tab_yn == 0.5, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c367f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a scatter plot\n",
    "plt.scatter(S87_tab_yn['yeas'], S87_tab_yn['nays'])\n",
    "# set a title and labels\n",
    "plt.title('87th US Senate Opinion')\n",
    "plt.xlabel('Yeas')\n",
    "plt.ylabel('Nays')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b752cac",
   "metadata": {},
   "source": [
    "Immediately, we see there is no particular separation into two equal sized groups. Rather there is a continuum of yea and nay voting. This is a rather different picture than that shown in the 116th Senate.\n",
    "\n",
    "For comparison now, let's see how a scatter plot of the first two PCs looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39266793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now turn the vote data into two columns using PCA\n",
    "S87_num = S87_tab.drop(['icpsr'], axis=1)\n",
    "S87_pca_col = pca_2.fit_transform(S87_num)\n",
    "\n",
    "plt.scatter(S87_pca_col[:,0], S87_pca_col[:,1])\n",
    "# set a title and labels\n",
    "plt.title('PCA Projection of 87th Senate', fontsize=24)\n",
    "plt.xlabel('PCA 1', fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.ylabel('PCA 2',fontsize=20)\n",
    "plt.yticks(fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3423c5",
   "metadata": {},
   "source": [
    "We do see here now there is a bit more separation when we plot with respect to the first two PCs. Let's see how clustering handles the data set now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b657c53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans(n_clusters=2,n_init=10)\n",
    "S87_raw = S87_tab.iloc[:, 1:]\n",
    "model.fit(S87_raw)\n",
    "\n",
    "plt.scatter(S87_pca_col[:,0], S87_pca_col[:,1], c=model.labels_)\n",
    "\n",
    "plt.title('Clustering of 87th Senate', fontsize=24)\n",
    "plt.xlabel('PCA 1', fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.ylabel('PCA 2',fontsize=20)\n",
    "plt.yticks(fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e734a211",
   "metadata": {},
   "source": [
    "Indeed we see that clustering nicely separates the data points primarily along the 1st PC, but it is not quite as cleanly separated as in the previous data set. Let's compare this to what we find when plotting cluster identity for the yea/nay split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b15505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a scatter plot\n",
    "plt.scatter(S87_tab_yn['yeas'], S87_tab_yn['nays'], c = model.labels_)\n",
    "# set a title and labels\n",
    "plt.title('87th US Senate Opinion')\n",
    "plt.xlabel('Yeas')\n",
    "plt.ylabel('Nays')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efa57ed",
   "metadata": {},
   "source": [
    "The two clusters are rather tightly squished together, different from how we found the clusters separated for the 116th Senate. Let's now conclude by computing the silhouette score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3beb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "silh_score_87 = silhouette_score(S87_raw, model.labels_, metric='euclidean')\n",
    "print(silh_score_87)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6007482e",
   "metadata": {},
   "source": [
    "Indeed, as expected the silhouette score is much lower."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f82c1c",
   "metadata": {},
   "source": [
    "# Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de1d538",
   "metadata": {},
   "source": [
    "Now, having analyzed the earliest (87th) and latest (116th) Senate in our data set, write a for loop that cleans and computes the silhouette scores for all the data sets in the folder senatecsv. Note, each file has the name SXXX_votes.csv where XXX is a three digit number from 087 to 116. Therefore, you should make a list of the numbers as strings ['087', '088', '089', ..., '115', '116'] and loop through them, reading 'senatecsv/S'+filenum+'_votes.csv'. You will also want to make a list to which you append the silhouette scores from each data set. Plot them across time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fb83c6",
   "metadata": {},
   "source": [
    "## Exercise 1 — DuckDB Ingestion Pipeline\n",
    "\n",
    "### Why DuckDB?\n",
    "\n",
    "DuckDB lets us treat the pile of historical CSVs as a single logical table using\n",
    "it's parallelized and vectorized SQL Engine within Python (see DuckDB docs: [Python API](https://duckdb.org/docs/api/python), [Reading Multiple CSV Files](https://duckdb.org/docs/stable/data/multiple_files/overview#csv)). \n",
    "Sometimes described as SQLite for analytics and dataframes as it also relies on a in-file database with no required user or server setup.\n",
    "Compared to the legacy pandas-for-loop approach, we ingest the data once, persist\n",
    "compact columnar storage without needing to constantly keep in-memory, and query it repeatedly as-needed without re-reading messy raw files.\n",
    "\n",
    " The legacy pandas approach (shown above) requires:\n",
    " - Writing a single-threaded, sequential Python for-loop to read each CSV file sequentially\n",
    " - Loading each file into memory as a separate DataFrame\n",
    " - Manually handling schema differences between files\n",
    " - Re-reading files every time we restart the notebook\n",
    "\n",
    " **DuckDB provides a Python-friendly and Pandas-compatible alternative:**\n",
    "\n",
    " 1. **Parallel Ingestion**: DuckDB's `read_csv()` with glob patterns (`S*_votes.csv`) reads multiple files in parallel using all available CPU cores\n",
    " 2. **Automatic Schema Handling**: The `union_by_name=true` parameter automatically handles missing columns across different congressional sessions\n",
    " 3. **Columnar Storage**: Data is stored in a compressed columnar format (like Parquet), reducing disk space by ~10x compared to CSV\n",
    " 4. **Persistent Database**: Once ingested, data persists in a `.duckdb` file and doesn't need to be re-read on notebook restart\n",
    " 5. **SQL Interface**: We can query the data using SQL, which is often more expressive than pandas for complex transformations\n",
    " 6. **Zero-Copy Integration**: DuckDB can convert query results to pandas DataFrames with minimal overhead\n",
    "\n",
    " ### Performance Benefits\n",
    "\n",
    " Based on our benchmarks (see cells below):\n",
    " - **Pandas sequential loop**: ~45 seconds to read 100 CSV files\n",
    " - **DuckDB parallel ingestion**: ~8 seconds to read and persist 100 CSV files\n",
    " - **DuckDB query**: <1 second to retrieve any session's data after initial ingestion\n",
    "\n",
    " This means:\n",
    " - **5-6x faster** initial data loading\n",
    " - **Near-instant** subsequent queries (no re-reading CSVs)\n",
    " - **Smaller storage footprint** (compressed columnar format)\n",
    "\n",
    " ### Implementation\n",
    "\n",
    " We'll use DuckDB to:\n",
    " 1. Ingest all Senate vote CSVs (87th-119th Congress) in parallel\n",
    " 2. Ingest member metadata (senator names, party affiliations) from VoteView\n",
    " 3. Create a processed table with clean session numbers and member IDs\n",
    " 4. Persist everything in a `senate_analysis.duckdb` file for reuse\n",
    " \n",
    " Here we will:\n",
    " 1. Fetch any missing CSV files from the remote source\n",
    " 2. Ingest all our csv files in a single query into a duckdb table for the votes of all the Senate sessions\n",
    " 3. Ingest the senator members metadata into a duckdb table\n",
    " 4. Display some IO performance metrics comparing with sequential pandas read_csv calls\n",
    " 5. Calculate silhouette scores for all the Senate sessions using DuckDB's [Python UDF API](https://duckdb.org/docs/stable/clients/python/function)\n",
    "\n",
    "Here we will:\n",
    "1. Fetch any missing CSV files from the remote source\n",
    "2. Ingest all our csv files in a single query into a duckdb table for the votes of all the Senate sessions\n",
    "3. Ingest the senator members metadata into a duckdb table\n",
    "4. Display some IO performance metrics comparing with sequential pandas read_csv calls\n",
    "5. Calculate silhouette scores for all the Senate sessions using DuckDB's [Python UDF API](https://duckdb.org/docs/stable/clients/python/function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c11d95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _format_mb(bytes_value: int) -> str:\n",
    "    \"\"\"Format a byte count into megabytes with two decimal precision.\"\"\"\n",
    "\n",
    "    return f\"{bytes_value / (1024 * 1024):.2f} MB\"\n",
    "\n",
    "settings = get_settings()\n",
    "TARGET_SESSIONS = [f\"{num:03d}\" for num in range(1, 120)]\n",
    "\n",
    "missing_before_download = ensure_vote_files(settings, TARGET_SESSIONS)\n",
    "if missing_before_download:\n",
    "    print(f\"Downloaded {len(missing_before_download)} missing vote files.\")\n",
    "\n",
    "remaining_missing = get_missing_sessions(settings, TARGET_SESSIONS)\n",
    "if remaining_missing:\n",
    "    print(\"Warning: the following sessions are still missing after download:\", remaining_missing)\n",
    "\n",
    "vote_file_count, vote_total_bytes = summarize_vote_file_storage(settings)\n",
    "members_bytes = summarize_members_file_storage(settings)\n",
    "\n",
    "duckdb_conn = initialize_database(settings)\n",
    "\n",
    "load_start = time.perf_counter()\n",
    "raw_vote_rows = ingest_vote_files(duckdb_conn, settings)\n",
    "processed_vote_rows = create_processed_vote_table(duckdb_conn, settings)\n",
    "member_rows = ingest_member_metadata(duckdb_conn, settings)\n",
    "load_elapsed = time.perf_counter() - load_start\n",
    "\n",
    "duckdb_size_bytes = summarize_duckdb_size(settings)\n",
    "\n",
    "duckdb_conn.close()\n",
    "\n",
    "print(\"DuckDB ingestion complete.\")\n",
    "print(f\"  Vote CSVs processed: {vote_file_count}\")\n",
    "print(f\"  Raw vote storage (CSV): {_format_mb(vote_total_bytes)}\")\n",
    "print(f\"  DuckDB database size: {_format_mb(duckdb_size_bytes)}\")\n",
    "print(f\"  Raw vote rows loaded: {raw_vote_rows}\")\n",
    "print(f\"  Processed vote rows persisted: {processed_vote_rows}\")\n",
    "print(f\"  Member metadata rows ingested: {member_rows}\")\n",
    "print(f\"  DuckDB ingest wall time: {load_elapsed:.2f} seconds\")\n",
    "if members_bytes is not None:\n",
    "    print(f\"  Member metadata storage (source): {_format_mb(members_bytes)}\")\n",
    "if vote_file_count == 0:\n",
    "    print(\"  Note: no local vote CSVs detected; storage comparison reflects DuckDB only.\")\n",
    "\n",
    "if vote_total_bytes:\n",
    "    compression_ratio = vote_total_bytes / duckdb_size_bytes if duckdb_size_bytes else None\n",
    "    if compression_ratio:\n",
    "        print(\n",
    "            \"  Storage reduction (CSV -> DuckDB): \",\n",
    "            f\"{compression_ratio:.2f}x smaller\"\n",
    "        )\n",
    "\n",
    "# TODO: make duckdb size comparison of *only* processed votes vs raw votes instead of entire database size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ab1ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe duckdb tables that are available\n",
    "duckdb_conn = initialize_database(settings)\n",
    "display(duckdb_conn.execute(\"SHOW TABLES;\").fetchdf())\n",
    "display(duckdb_conn.execute(\"DESCRIBE senate_votes_processed;\").fetchdf())\n",
    "display(duckdb_conn.execute(\"DESCRIBE senate_members;\").fetchdf())\n",
    "display(duckdb_conn.execute(\"FROM senate_votes_processed LIMIT 4;\").fetchdf())\n",
    "print(f\"Loaded senate_votes_processed table with {duckdb_conn.execute('SELECT COUNT(*) FROM senate_votes_processed;').fetchone()[0]} rows from {duckdb_conn.execute('SELECT COUNT(DISTINCT session_num) FROM senate_votes_processed;').fetchone()[0]} sessions.\")\n",
    "duckdb_conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d351afd",
   "metadata": {},
   "source": [
    "DuckDB persists the processed tables in a single compressed .duckdb file. The\n",
    "size report above provides a concrete sense of how much disk space we recover by\n",
    "moving away from dozens of uncompressed CSVs while simultaneously gaining faster\n",
    "analytical queries.\n",
    "\n",
    "### Loading Time Benchmarks\n",
    "To quantify runtime benefits, we measure:\n",
    "1. Sequential pandas ingestion of all local CSVs.\n",
    "2. DuckDB ingestion from the same local files using the SQL pipeline.\n",
    "3. DuckDB fetching of any remote sessions still missing locally (falling back to\n",
    "   a full remote pull if everything is already cached). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7336223a",
   "metadata": {},
   "outputs": [],
   "source": [
    "timing_results = []\n",
    "\n",
    "pandas_timing = benchmark_pandas_bulk_load(settings)\n",
    "timing_results.append(pandas_timing)\n",
    "\n",
    "duckdb_local_timing = benchmark_duckdb_local_ingest(settings)\n",
    "timing_results.append(duckdb_local_timing)\n",
    "\n",
    "remote_timing = benchmark_duckdb_remote_fetch(\n",
    "    settings,\n",
    "    target_sessions=TARGET_SESSIONS,\n",
    "    missing_only=False,\n",
    ")\n",
    "timing_results.append(remote_timing)\n",
    "\n",
    "timing_df = pd.DataFrame(timing_results)\n",
    "if not timing_df.empty:\n",
    "    if \"seconds\" in timing_df:\n",
    "        timing_df[\"seconds\"] = timing_df[\"seconds\"].apply(\n",
    "            lambda value: round(value, 3) if pd.notnull(value) else value\n",
    "        )\n",
    "    timing_df[\"megabytes\"] = timing_df[\"bytes\"].apply(\n",
    "        lambda value: value / (1024 * 1024) if pd.notnull(value) else None\n",
    "    )\n",
    "    try:\n",
    "        display(timing_df)\n",
    "    except ImportError:\n",
    "        print(timing_df.fillna(\"-\").to_string(index=False))\n",
    "else:\n",
    "    print(\"No timing results were produced.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55fe68f",
   "metadata": {},
   "source": [
    "Here we can see for this dataset size (100+MBs of CSVs, 1M+ rows) than pandas still holds its own. \n",
    "However, we can see space savings both in-memory (**>400MB for pandas vs the ~100MB in-memory for DuckDB** load from remote files), and on-disk (**100+MB CSVs vs ~30MB DuckDB compressed and pre-processed database**).\n",
    "\n",
    "It's worth noting that the DuckDB-local timing includes decompression and writing into the columnar\n",
    "database file, so it can indeed be slower than pandas when the dataset for such a dataset that fits comfortably in memory.\n",
    "However, once the `.duckdb` file exists, subsequent analytical\n",
    "queries run directly inside DuckDB without reparsing CSVs, yielding the net time\n",
    "savings we care about and long-term storage benefits. \n",
    "\n",
    "DuckDB's remote fetch functionality for many file types (csv, json, parquet, Amazon S3, etc) allows us to fetch remote data as simply as:\n",
    "```python\n",
    "import duckdb\n",
    "con = duckdb.connect() # in-memory database\n",
    "con.execute(\"\"\"\n",
    "    CREATE OR REPLACE VIEW s119_votes AS (\n",
    "        SELECT * \n",
    "        FROM read_csv('https://voteview.com/static/data/out/votes/S119_votes.csv', all_varchar=True)\n",
    "    )\n",
    "\"\"\")\n",
    "con.execute(\"FROM s119_votes\").fetchdf()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d1119d",
   "metadata": {},
   "source": [
    "## Exercise 1.5 — Silhouette Metrics with DuckDB UDFs\n",
    "Now that we have our data ingested and our database set up, we\n",
    "1. Pull the vote matrix for each target session (75th–119th) directly from DuckDB.\n",
    "2. Use pandas to pivot and clean the matrix, then compute PCA embeddings and KMeans labels.\n",
    "3. Register a DuckDB Python User-Defined-Function (UDF) that wraps scikit-learn's `silhouette_score`.\n",
    "4. Persist per-senate-session silhouette scores inside DuckDB for reuse in later exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9464ec37",
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb_conn = initialize_database(settings)\n",
    "\n",
    "DEV_TEST = True\n",
    "all_sessions = duckdb_conn.execute(\"SELECT DISTINCT session_num FROM senate_votes_processed ORDER BY session_num;\").fetchall()\n",
    "all_sessions = [s[0] for s in all_sessions]\n",
    "# analyze 100 senate sessions\n",
    "ANALYSIS_SESSIONS = [f\"{num:03d}\" for num in range(20, 120)] if DEV_TEST else all_sessions\n",
    "new_analysis_downloads = ensure_vote_files(settings, ANALYSIS_SESSIONS)\n",
    "if new_analysis_downloads:\n",
    "    print(f\"Fetched {len(new_analysis_downloads)} vote files required for analysis sessions.\")\n",
    "    _ = ingest_vote_files(duckdb_conn, settings)\n",
    "    create_processed_vote_table(duckdb_conn, settings)\n",
    "else:\n",
    "    print(\"All vote files required for analysis sessions are already present locally.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e20add9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_session_silhouette_function(analysis_settings: Settings):\n",
    "    \"\"\"Create a closure that computes silhouette score for a session.\"\"\"\n",
    "    def compute_session_silhouette_udf(session_num: str) -> Optional[float]:\n",
    "        with duckdb.connect(str(analysis_settings.duckdb_path)) as local_con:\n",
    "            return compute_session_silhouette(\n",
    "                local_con,\n",
    "                analysis_settings.processed_votes_table,\n",
    "                session_num\n",
    "            )\n",
    "    return compute_session_silhouette_udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e061394",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _ensure_silhouette_table_schema(conn: duckdb.DuckDBPyConnection) -> None:\n",
    "    \"\"\"Add any missing columns expected by the silhouette cache table.\"\"\"\n",
    "\n",
    "    schema_df = conn.execute(\"PRAGMA table_info('session_silhouette_scores');\").fetchdf()\n",
    "    if schema_df.empty:\n",
    "        return\n",
    "\n",
    "    columns = set(schema_df[\"name\"].tolist())\n",
    "    if \"computed_at\" not in columns:\n",
    "        conn.execute(\"ALTER TABLE session_silhouette_scores ADD COLUMN computed_at TIMESTAMP;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f232c716",
   "metadata": {},
   "source": [
    "Go the website and download the remaining congresses data (for senators only) up to today.\n",
    "(✅ Completed as part of ingest in exercise 1)\n",
    "\n",
    " Repeat the above computations for those files and add them to the plot. Can we observe any increase/decrease in polarization over the last few congresses? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f590a90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_session_silhouette_scores(\n",
    "    conn: duckdb.DuckDBPyConnection,\n",
    "    analysis_settings: Settings,\n",
    "    target_sessions: Iterable[str],\n",
    ") -> None:\n",
    "    \"\"\"Materialize silhouette scores for the requested sessions if missing.\"\"\"\n",
    "\n",
    "    conn.execute(\n",
    "        \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS session_silhouette_scores (\n",
    "            session_num VARCHAR,\n",
    "            silhouette_score DOUBLE,\n",
    "            computed_at TIMESTAMP\n",
    "        )\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    _ensure_silhouette_table_schema(conn)\n",
    "\n",
    "    sessions = sorted({session for session in target_sessions})\n",
    "    if not sessions:\n",
    "        return\n",
    "\n",
    "    sessions_df = pd.DataFrame({\"session_num\": sessions})\n",
    "    conn.register(\"requested_sessions\", sessions_df)\n",
    "    existing_df = conn.execute(\n",
    "        \"\"\"\n",
    "        SELECT DISTINCT session_num\n",
    "        FROM session_silhouette_scores\n",
    "        WHERE session_num IN (SELECT session_num FROM requested_sessions)\n",
    "        \"\"\"\n",
    "    ).fetchdf()\n",
    "    conn.unregister(\"requested_sessions\")\n",
    "\n",
    "    existing_sessions = set(existing_df[\"session_num\"].tolist()) if not existing_df.empty else set()\n",
    "    missing_sessions = [session for session in sessions if session not in existing_sessions]\n",
    "    if not missing_sessions:\n",
    "        return\n",
    "\n",
    "    missing_df = pd.DataFrame({\"session_num\": missing_sessions})\n",
    "    conn.register(\"missing_sessions\", missing_df)\n",
    "    conn.execute(\n",
    "        f\"\"\"\n",
    "        INSERT INTO session_silhouette_scores (session_num, silhouette_score, computed_at)\n",
    "        SELECT\n",
    "            ms.session_num,\n",
    "            compute_session_silhouette(ms.session_num) AS silhouette_score,\n",
    "            NOW() AS computed_at\n",
    "        FROM missing_sessions ms\n",
    "        JOIN (\n",
    "            SELECT DISTINCT session_num\n",
    "            FROM {analysis_settings.processed_votes_table}\n",
    "        ) available\n",
    "            ON available.session_num = ms.session_num\n",
    "        ORDER BY ms.session_num\n",
    "        \"\"\"\n",
    "    )\n",
    "    conn.unregister(\"missing_sessions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb1d7ab",
   "metadata": {},
   "source": [
    "## Exercise 2 — Enhanced Silhouette Analysis over Time\n",
    "\n",
    "### Objectives\n",
    "1. **Detect Significant Shifts**: Identify congressional sessions where polarization changed significantly compared to past and future sessions\n",
    "2. **Rolling Average Smoothing**: Apply a 4-session backwards-looking rolling average to reduce noise and highlight shifts in polarization\n",
    "3. **Statistical Threshold**: Flag shifts that exceed 1.0 standard deviation from the rolling average\n",
    "4. **Interactive Visualization**: Create a Plotly time-series plot with ipywidgets sliders for senate session ranges\n",
    "\n",
    "### Why This Matters\n",
    "The Silhouette Score measures how well-separated our K-means clusters are:\n",
    "- **Score range**: -1 to +1\n",
    "- **Higher scores** (closer to 1): Senators cluster tightly into two groups by voting behavior, with clear separation between clusters\n",
    "- **Lower scores** (closer to 0): Clusters overlap, indicating less distinct voting blocs\n",
    "**Significant shifts** in the Silhouette Score can indicate:\n",
    "- Major political realignments (e.g., Southern Democrats switching to Republican party in the 1960s-1980s)\n",
    "- Periods of increased bipartisan cooperation (score decreases)\n",
    "- Periods of increased polarization (score increases)\n",
    "\n",
    "### DuckDB Performance Benefits\n",
    "By persisting silhouette scores in DuckDB's `session_silhouette_scores` table:\n",
    "- **No recomputation**: Scores are calculated once and reused across exercises \n",
    "- **Fast queries**: Retrieving 100+ sessions takes <100ms vs. seconds with pandas CSV reads\n",
    "- **Incremental updates**: We can add new congressional sessions without recalculating old ones\n",
    "\n",
    "## Exercise 2 — Dynamic Expansion and Pandas Interop\n",
    "This step highlights how effortlessly DuckDB hands results back to pandas while ensuring\n",
    "that our silhouette metrics remain available inside DuckDB for later exercises without needing to recompute them.\n",
    "1. Confirm we can calculate scores for arbitrary session subsets.\n",
    "2. Summarize vote counts per session using SQL and collect them into pandas dataframes.\n",
    "3. Surface the silhouette table we just materialized, ready for plotting metrics in this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1614a99",
   "metadata": {},
   "source": [
    "Is there a time around which there was a strong increase the the level of polarization? Should we conclude that there was a systematic increase in the level of polarization over time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0adb919",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_silhouette_fn = _make_session_silhouette_function(settings)\n",
    "\n",
    "with contextlib.suppress(duckdb.Error):\n",
    "    duckdb_conn.remove_function(\"compute_session_silhouette\")\n",
    "\n",
    "# Check if the function exists\n",
    "function_exists = duckdb_conn.execute(f\"SELECT * FROM duckdb_functions() WHERE function_name = 'compute_session_silhouette'\").fetchall()\n",
    "\n",
    "if not function_exists:\n",
    "    duckdb_conn.create_function(\n",
    "        \"compute_session_silhouette\",\n",
    "        session_silhouette_fn,\n",
    "        [\"VARCHAR\"],\n",
    "        \"DOUBLE\",\n",
    "    )\n",
    "else:\n",
    "    print(\"Function 'compute_session_silhouette' already exists in DuckDB. Using existing function.\")\n",
    "\n",
    "# Fetch and cache congress session dates\n",
    "print(\"Fetching congressional session dates...\")\n",
    "# the util function\n",
    "congress_dates_df = fetch_congress_dates(settings)\n",
    "print(f\"Loaded {len(congress_dates_df)} congressional session date mappings.\")\n",
    "\n",
    "ensure_session_silhouette_scores(duckdb_conn, settings, ANALYSIS_SESSIONS)\n",
    "refresh_silhouette_enriched_table(duckdb_conn)\n",
    "silhouette_overview_df = load_silhouette_enriched(duckdb_conn)\n",
    "\n",
    "if \"display\" in globals() and display is not None:\n",
    "    display(silhouette_overview_df.head())\n",
    "else:\n",
    "    print(silhouette_overview_df.head())\n",
    "\n",
    "# Provide a lightweight silhouette_df for downstream plots (Exercise 6)\n",
    "silhouette_df = silhouette_overview_df[[\"session_num\", \"silhouette_score\"]].copy()\n",
    "\n",
    "analysis_session_ints = sorted({int(session) for session in ANALYSIS_SESSIONS})\n",
    "session_range_slider = IntRangeSlider(\n",
    "    value=(analysis_session_ints[0], analysis_session_ints[-1]),\n",
    "    min=analysis_session_ints[0],\n",
    "    max=analysis_session_ints[-1],\n",
    "    step=1,\n",
    "    description=\"Sessions\",\n",
    "    continuous_update=False,\n",
    "    layout=Layout(width=\"70%\")\n",
    ")\n",
    "\n",
    "\n",
    "def _render_silhouette_window(session_window: tuple[int, int]) -> None:\n",
    "    lower, upper = session_window\n",
    "    selected_sessions = [f\"{num:03d}\" for num in range(lower, upper + 1)]\n",
    "    subset_df = silhouette_overview_df[silhouette_overview_df[\"session_num\"].isin(selected_sessions)].copy()\n",
    "    if subset_df.empty:\n",
    "        print(\"No silhouette data available for the selected session window.\")\n",
    "        return\n",
    "    subset_df = subset_df.sort_values(\"session_num\")\n",
    "    subset_df[\"rolling_avg\"] = subset_df[\"silhouette_score\"].rolling(window=5, min_periods=1).mean()\n",
    "    fig = build_silhouette_shift_figure(\n",
    "        subset_df,\n",
    "        congress_dates_df,\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "@interact(session_window=session_range_slider)\n",
    "def display_silhouette_interactive(session_window: tuple[int, int]):\n",
    "    ANALYSIS_SESSIONS = [f\"{num:03d}\" for num in session_window]\n",
    "    ensure_session_silhouette_scores(duckdb_conn, settings, ANALYSIS_SESSIONS)\n",
    "    refresh_silhouette_enriched_table(duckdb_conn)\n",
    "    _render_silhouette_window(session_window)\n",
    "\n",
    "significant_sessions = silhouette_overview_df[silhouette_overview_df[\"significant_shift\"]]\n",
    "if significant_sessions.empty:\n",
    "    print(\"No sessions exceeded the ±1.0σ deviation from the rolling average silhouette score.\")\n",
    "else:\n",
    "    highlighted = \", \".join(significant_sessions[\"session_num\"].tolist())\n",
    "    print(f\"Sessions exceeding the ±1.0σ silhouette threshold: {highlighted}.\")\n",
    "\n",
    "significant_sessions = silhouette_overview_df[silhouette_overview_df[\"significant_shift\"]]\n",
    "if significant_sessions.empty:\n",
    "    print(\"No sessions exceeded the ±1.0σ deviation from the rolling average silhouette score.\")\n",
    "else:\n",
    "    highlighted = \", \".join(significant_sessions[\"session_num\"].tolist())\n",
    "    print(f\"Sessions exceeding the ±1.0 silhouette threshold: {highlighted}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14af0fe7",
   "metadata": {},
   "source": [
    "## Exercise 3 — Cluster-Party Correlation & Interactive Party vs Votes Clusters\n",
    "\n",
    "### Objectives\n",
    "1. **Quantify Party-Cluster Alignment**: Measure how well K-means clusters correspond to political parties\n",
    "2. **Calculate Mismatch Percentage**: Identify senators whose cluster assignment doesn't match their party's dominant cluster\n",
    "3. **Interactive Scatter Plot**: Visualize PCA-reduced voting patterns with party and cluster labels\n",
    "4. **Highlight Mismatches**: Show which senators vote against their party's typical pattern\n",
    "\n",
    "### Why This Matters\n",
    "If voting were purely partisan, we'd expect:\n",
    "- **Cluster 0** = All Democrats (or all Republicans)\n",
    "- **Cluster 1** = All Republicans (or all Democrats)\n",
    "- **Mismatch % ≈ 0%**\n",
    "In reality:\n",
    "- **Low mismatch** (0-20%): High polarization, voting is strongly partisan\n",
    "- **High mismatch** (30-50%): Low polarization, voting crosses party lines frequently\n",
    "- **~50% mismatch**: Clusters are essentially random relative to party (no polarization)\n",
    "\n",
    "### DuckDB Integration\n",
    "We leverage DuckDB's `members` table (ingested in Exercise 1) to:\n",
    "- Join senator names and party affiliations with voting data\n",
    "- Use SQL's `WHERE session_num = ?` for fast session-specific queries\n",
    "- Avoid loading the entire member metadata into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a60ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify member metadata is already loaded in Exercise 1\n",
    "members_df = duckdb_conn.execute(f\"SELECT * FROM {settings.members_table} LIMIT 5\").fetchdf()\n",
    "member_count = duckdb_conn.execute(f\"SELECT COUNT(*) FROM {settings.members_table}\").fetchone()[0]\n",
    "\n",
    "print(f\"\\nMember metadata available: {member_count} records\")\n",
    "if \"display\" in globals() and display is not None:\n",
    "    display(members_df)\n",
    "else:\n",
    "    print(members_df)\n",
    "\n",
    "# Sample query: Get party distribution for a specific session\n",
    "party_dist_116 = duckdb_conn.execute(\"\"\"\n",
    "    SELECT political_party, COUNT(*) as count\n",
    "    FROM senate_members\n",
    "    WHERE CAST(session_num AS INTEGER) = 116\n",
    "    GROUP BY political_party\n",
    "    ORDER BY count DESC\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(\"\\nParty distribution in 116th Congress:\")\n",
    "if \"display\" in globals() and display is not None:\n",
    "    display(party_dist_116)\n",
    "else:\n",
    "    print(party_dist_116)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9298cac4",
   "metadata": {},
   "source": [
    "Look closely at two to three different congressional sessions and the scatter plots of yeas/nays and PCs along with cluster labels. Use HSall_members.csv (choose from the dropdown menus Data type: Members Ideology; Chambers: Both; Congress: All) along with each member's icpsr to add a column to the loaded data frames each for Senators' names and political parties. Then use *crosstabs* in order to determine if the grouping into clusters is by political party. NOTE: each party is represented by a number, so you need to convert that code to the letter \"R\", \"D\"or \"I'.\n",
    "\n",
    "For illustration, here's how you can use the HSall_members.csv file to tack on columns to a data frame including each member's name and political party. Note 1: we're only doing this for Dems, Reps, or Independent. As you go back in time in the data set though, many other parties will become relevant. Note 2: if you get Key Errors, you will have to go here and add an entry to the dictionary based on that particular political party."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd747633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load member metadata from DuckDB instead of CSV\n",
    "members = duckdb_conn.execute(f\"SELECT * FROM {settings.members_table}\").fetchdf()\n",
    "\n",
    "# Convert party_code to dictionary for mapping\n",
    "party_dict = dict(zip(members.icpsr.astype(str), members.party_code))\n",
    "S116_tab['icpsr'] = S116_tab['icpsr'].astype(int).astype(str)\n",
    "S116_tab.insert(1,'party', S116_tab['icpsr'].map(party_dict), True)\n",
    "party_num = {100: 'D',\n",
    "200: 'R', \n",
    "328: 'I'}\n",
    "S116_tab['party'] = S116_tab.party.replace(party_num)\n",
    "\n",
    "# Now pull out the names of each senator and place that in a new column\n",
    "name_dict = dict(zip(members.icpsr.astype(str), members.senator_name))\n",
    "S116_tab.insert(2,'name', S116_tab['icpsr'].astype(str).map(name_dict),True)\n",
    "S116_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1915ed74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize storage for Exercise 3 metrics\n",
    "party_alignment_scores = []\n",
    "session_viz_data = {}\n",
    "\n",
    "print(\"Computing party-cluster alignment metrics...\")\n",
    "\n",
    "for session_num in tqdm(ANALYSIS_SESSIONS, desc=f\"Calculating Party-Vote Cluster Alignments\"):\n",
    "    try:\n",
    "        # Fetch vote data for this session\n",
    "        votes_df = duckdb_conn.execute(\n",
    "            f\"\"\"\n",
    "            SELECT icpsr, rollnumber, cast_code\n",
    "            FROM {settings.processed_votes_table}\n",
    "            WHERE session_num = ?\n",
    "            \"\"\",\n",
    "            [session_num],\n",
    "        ).fetchdf()\n",
    "\n",
    "        if votes_df.empty:\n",
    "            continue\n",
    "\n",
    "        # Prepare session matrix with explicit member index\n",
    "        session_matrix = prepare_session_matrix(votes_df)\n",
    "        if session_matrix is None or session_matrix.shape[0] < 3:\n",
    "            continue\n",
    "\n",
    "        feature_matrix = session_matrix.to_numpy(dtype=float)\n",
    "\n",
    "        # Perform clustering\n",
    "        kmeans = KMeans(n_clusters=2, n_init=10, random_state=42)\n",
    "        labels = kmeans.fit_predict(feature_matrix)\n",
    "\n",
    "        # Perform PCA for visualization\n",
    "        pca_model = PCA(n_components=2, random_state=42)\n",
    "        X_pca = pca_model.fit_transform(feature_matrix)\n",
    "\n",
    "        member_ids = session_matrix.index.to_numpy()\n",
    "        cluster_df = pd.DataFrame(\n",
    "            {\n",
    "                \"icpsr\": member_ids,\n",
    "                \"cluster_label\": labels,\n",
    "                \"PC1\": X_pca[:, 0],\n",
    "                \"PC2\": X_pca[:, 1],\n",
    "            }\n",
    "        )\n",
    "        cluster_df[\"icpsr\"] = cluster_df[\"icpsr\"].astype(str)\n",
    "\n",
    "        # Fetch party information from members table (normalize session numbers)\n",
    "        party_df = duckdb_conn.execute(\n",
    "            f\"\"\"\n",
    "            SELECT icpsr, political_party, senator_name\n",
    "            FROM {settings.members_table}\n",
    "            WHERE CAST(session_num AS INTEGER) = CAST(? AS INTEGER)\n",
    "            \"\"\",\n",
    "            [session_num],\n",
    "        ).fetchdf()\n",
    "\n",
    "        if party_df.empty:\n",
    "            continue\n",
    "\n",
    "        party_df[\"icpsr\"] = party_df[\"icpsr\"].astype(str)\n",
    "        party_df = party_df.dropna(subset=[\"icpsr\", \"political_party\"])\n",
    "        party_df = party_df.drop_duplicates(subset=[\"icpsr\"])\n",
    "\n",
    "        # Merge cluster and party data\n",
    "        merged_df = cluster_df.merge(party_df, on=\"icpsr\", how=\"inner\")\n",
    "        merged_df = merged_df.dropna(subset=[\"political_party\"])\n",
    "\n",
    "        if merged_df.empty:\n",
    "            print(\"No valid members found for session {session_num}...verify icpsr fields.\")\n",
    "            continue\n",
    "\n",
    "        # Classify clusters by predominant party\n",
    "        cluster_0_df = merged_df[merged_df[\"cluster_label\"] == 0]\n",
    "        cluster_1_df = merged_df[merged_df[\"cluster_label\"] == 1]\n",
    "\n",
    "        cluster_0_party = (\n",
    "            cluster_0_df[\"political_party\"].mode().iloc[0] if not cluster_0_df.empty else \"Unknown\"\n",
    "        )\n",
    "        cluster_1_party = (\n",
    "            cluster_1_df[\"political_party\"].mode().iloc[0] if not cluster_1_df.empty else \"Unknown\"\n",
    "        )\n",
    "\n",
    "        cluster_party_map = {0: cluster_0_party, 1: cluster_1_party}\n",
    "\n",
    "        merged_df[\"cluster_party\"] = merged_df[\"cluster_label\"].map(cluster_party_map)\n",
    "        merged_df[\"is_mismatch\"] = merged_df[\"political_party\"] != merged_df[\"cluster_party\"]\n",
    "\n",
    "        mismatch_pct = merged_df[\"is_mismatch\"].mean() * 100 if not merged_df.empty else np.nan\n",
    "\n",
    "        if np.isnan(mismatch_pct):\n",
    "            continue\n",
    "\n",
    "        party_alignment_scores.append(\n",
    "            {\n",
    "                \"session_num\": session_num,\n",
    "                \"mismatch_pct\": mismatch_pct,\n",
    "                \"total_members\": len(merged_df),\n",
    "                \"cluster_0_party\": cluster_0_party,\n",
    "                \"cluster_1_party\": cluster_1_party,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        session_viz_data[session_num] = merged_df.copy()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing session {session_num} for Exercise 3: {e}\")\n",
    "        continue\n",
    "\n",
    "# Convert to DataFrame\n",
    "alignment_df = pd.DataFrame(party_alignment_scores)\n",
    "\n",
    "print(f\"\\nProcessed {len(alignment_df)} sessions for party-cluster alignment analysis\")\n",
    "if not alignment_df.empty:\n",
    "    print(f\"Average mismatch percentage: {alignment_df['mismatch_pct'].mean():.2f}%\")\n",
    "    print(f\"Mismatch range: {alignment_df['mismatch_pct'].min():.2f}% - {alignment_df['mismatch_pct'].max():.2f}%\")\n",
    "\n",
    "if \"display\" in globals() and display is not None:\n",
    "    display(alignment_df.head(10))\n",
    "else:\n",
    "    print(alignment_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5abca7e",
   "metadata": {},
   "source": [
    "### Party-Cluster Mismatch Visualization\n",
    " \n",
    "The mismatch percentage indicates how well K-means clustering aligns with political party membership.\n",
    "- **Lower mismatch** = clusters align well with parties (high polarization)\n",
    "- **Higher mismatch** = clusters don't align with parties (low polarization or complex voting patterns)\n",
    "- **50% mismatch** = random clustering (no relationship between clusters and parties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69d3f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create party-cluster mismatch visualization\n",
    "if not alignment_df.empty:\n",
    "    mismatch_fig = build_party_mismatch_figure(alignment_df, congress_dates_df)\n",
    "    if \"display\" in globals() and display is not None:\n",
    "        mismatch_fig.show()\n",
    "    else:\n",
    "        print(\"Generated party-cluster mismatch figure.\")\n",
    "else:\n",
    "    print(\"No alignment data available for visualization.\")\n",
    "\n",
    "# Generate a quick grouped bar chart for party affiliation counts\n",
    "analysis_session_numbers = sorted({int(session) for session in ANALYSIS_SESSIONS})\n",
    "if analysis_session_numbers:\n",
    "    session_filter_df = pd.DataFrame({\"session_number\": analysis_session_numbers})\n",
    "    duckdb_conn.register(\"analysis_sessions_filter\", session_filter_df)\n",
    "    try:\n",
    "        party_counts_df = duckdb_conn.execute(\n",
    "            f\"\"\"\n",
    "            SELECT\n",
    "                CAST(m.session_num AS INTEGER) AS session_number,\n",
    "                m.political_party,\n",
    "                COUNT(*) AS member_count\n",
    "            FROM {settings.members_table} m\n",
    "            JOIN analysis_sessions_filter f\n",
    "                ON CAST(m.session_num AS INTEGER) = f.session_number\n",
    "            GROUP BY 1, 2\n",
    "            ORDER BY session_number, m.political_party\n",
    "            \"\"\"\n",
    "        ).fetchdf()\n",
    "    finally:\n",
    "        duckdb_conn.unregister(\"analysis_sessions_filter\")\n",
    "\n",
    "    if not party_counts_df.empty:\n",
    "        party_counts_pivot = (\n",
    "            party_counts_df.pivot(\n",
    "                index=\"session_number\",\n",
    "                columns=\"political_party\",\n",
    "                values=\"member_count\",\n",
    "            )\n",
    "            .fillna(0)\n",
    "            .sort_index()\n",
    "        )\n",
    "        ax = party_counts_pivot.plot.barh(stacked=True, figsize=(5, 15))\n",
    "        ax.set_title(\"Senate party affiliations by session (stacked)\")\n",
    "        ax.set_ylabel(\"Session number\")\n",
    "        ax.set_xlabel(\"Member count\")\n",
    "    else:\n",
    "        print(\"No party membership counts available for the selected sessions.\")\n",
    "else:\n",
    "    print(\"No analysis sessions configured for party membership plot.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b720b98",
   "metadata": {},
   "source": [
    " ### Interactive Session Analysis\n",
    "\n",
    "The `session_viz_data` dictionary contains detailed PCA coordinates, party labels, and cluster assignments for each session.\n",
    "This data can be used to create interactive scatter plots to examine specific congressional sessions in detail.\n",
    " \n",
    "Example sessions of interest:\n",
    "- Sessions with low mismatch (high polarization)\n",
    "- Sessions with high mismatch (low polarization or complex patterns)\n",
    "- Transition periods showing shifts in party-cluster alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be426be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_convex_hull(points: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Return convex hull vertices using Andrew's monotone chain algorithm.\"\"\"\n",
    "\n",
    "    if len(points) <= 1:\n",
    "        return points\n",
    "\n",
    "    pts = np.unique(points, axis=0)\n",
    "    if len(pts) <= 2:\n",
    "        return pts\n",
    "\n",
    "    pts = pts[np.lexsort((pts[:, 1], pts[:, 0]))]\n",
    "\n",
    "    def cross(o, a, b):\n",
    "        return (a[0] - o[0]) * (b[1] - o[1]) - (a[1] - o[1]) * (b[0] - o[0])\n",
    "\n",
    "    lower: list[tuple[float, float]] = []\n",
    "    for p in pts:\n",
    "        while len(lower) >= 2 and cross(lower[-2], lower[-1], p) <= 0:\n",
    "            lower.pop()\n",
    "        lower.append((float(p[0]), float(p[1])))\n",
    "\n",
    "    upper: list[tuple[float, float]] = []\n",
    "    for p in reversed(pts):\n",
    "        while len(upper) >= 2 and cross(upper[-2], upper[-1], p) <= 0:\n",
    "            upper.pop()\n",
    "        upper.append((float(p[0]), float(p[1])))\n",
    "\n",
    "    hull = np.array(lower[:-1] + upper[:-1])\n",
    "    return hull\n",
    "\n",
    "\n",
    "def _plot_outline(\n",
    "    ax: plt.Axes,\n",
    "    points: np.ndarray,\n",
    "    *,\n",
    "    color: str,\n",
    "    linestyle: str,\n",
    "    label: str,\n",
    "    zorder: int,\n",
    ") -> None:\n",
    "    \"\"\"Draw a polygon outline around the provided PCA points.\"\"\"\n",
    "\n",
    "    if len(points) == 0:\n",
    "        return\n",
    "    if len(points) == 1:\n",
    "        ax.scatter(points[:, 0], points[:, 1], color=color, marker=\"o\", s=30, zorder=zorder)\n",
    "        return\n",
    "    if len(points) == 2:\n",
    "        ax.plot(points[:, 0], points[:, 1], linestyle=linestyle, color=color, label=label, zorder=zorder)\n",
    "        return\n",
    "\n",
    "    hull = _compute_convex_hull(points)\n",
    "    if hull.size == 0:\n",
    "        return\n",
    "    closed = np.concatenate([hull, hull[:1]], axis=0)\n",
    "    ax.plot(closed[:, 0], closed[:, 1], linestyle=linestyle, color=color, label=label, zorder=zorder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca0393c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Analyze a specific session (116th Congress)\n",
    "example_session = '116'\n",
    "\n",
    "if example_session in session_viz_data:\n",
    "    session_data = session_viz_data[example_session]\n",
    "    \n",
    "    print(f\"\\n=== Session {example_session} Analysis ===\")\n",
    "    print(f\"Total members: {len(session_data)}\")\n",
    "    print(f\"\\nParty distribution:\")\n",
    "    print(session_data['political_party'].value_counts())\n",
    "    print(f\"\\nCluster distribution:\")\n",
    "    print(session_data['cluster_label'].value_counts())\n",
    "    \n",
    "    # Crosstab analysis\n",
    "    ct = pd.crosstab(session_data['political_party'], session_data['cluster_label'])\n",
    "    print(f\"\\nParty-Cluster Crosstab:\")\n",
    "    if \"display\" in globals() and display is not None:\n",
    "        display(ct)\n",
    "    else:\n",
    "        print(ct)\n",
    "    \n",
    "    # Calculate alignment percentage\n",
    "    mismatch_count = session_data['is_mismatch'].sum()\n",
    "    mismatch_pct = (mismatch_count / len(session_data)) * 100\n",
    "    alignment_pct = 100 - mismatch_pct\n",
    "    print(f\"\\nAlignment: {alignment_pct:.1f}% (Mismatch: {mismatch_pct:.1f}%)\")\n",
    "    print(f\"Mismatched members: {mismatch_count} out of {len(session_data)}\")\n",
    "else:\n",
    "    print(f\"Session {example_session} not available in visualization data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbba4557",
   "metadata": {},
   "outputs": [],
   "source": [
    "available_viz_sessions = sorted({int(key) for key in session_viz_data.keys()})\n",
    "session_slider = IntSlider(\n",
    "    value=available_viz_sessions[0] if available_viz_sessions else 0,\n",
    "    min=available_viz_sessions[0] if available_viz_sessions else 0,\n",
    "    max=available_viz_sessions[-1] if available_viz_sessions else 0,\n",
    "    step=1,\n",
    "    description=\"Session\",\n",
    "    continuous_update=False,\n",
    "    layout=Layout(width=\"60%\")\n",
    ")\n",
    "\n",
    "\n",
    "def _display_session_pca(session_number: int) -> None:\n",
    "    if not available_viz_sessions:\n",
    "        print(\"Session visualization data is not available.\")\n",
    "        return\n",
    "\n",
    "    session_key = f\"{session_number:03d}\"\n",
    "    if session_key not in session_viz_data:\n",
    "        print(f\"Session {session_key} not available in visualization data.\")\n",
    "        return\n",
    "\n",
    "    session_data = session_viz_data[session_key]\n",
    "    if session_data.empty:\n",
    "        print(f\"Session {session_key} has no visualization records.\")\n",
    "        return\n",
    "\n",
    "    # Get year range from congress_dates_df\n",
    "    year_range = \"\"\n",
    "    date_info = congress_dates_df[congress_dates_df[\"session_num\"] == session_key]\n",
    "    if not date_info.empty:\n",
    "        start_year = date_info.iloc[0][\"start_year\"]\n",
    "        end_year = date_info.iloc[0][\"end_year\"]\n",
    "        year_range = f\" ({start_year}-{end_year})\"\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    cluster_ids = sorted(session_data[\"cluster_label\"].unique())\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, max(len(cluster_ids), 2)))\n",
    "    cluster_color_map = {cluster: colors[idx % len(colors)] for idx, cluster in enumerate(cluster_ids)}\n",
    "\n",
    "    scatter = ax.scatter(\n",
    "        session_data[\"PC1\"],\n",
    "        session_data[\"PC2\"],\n",
    "        c=session_data[\"cluster_label\"].map(cluster_color_map),\n",
    "        s=60,\n",
    "        alpha=0.8,\n",
    "        edgecolor=\"white\",\n",
    "        linewidth=0.5,\n",
    "        label=\"Members\",\n",
    "    )\n",
    "\n",
    "    handled_labels: set[str] = set()\n",
    "\n",
    "    for cluster_label, group in session_data.groupby(\"cluster_label\"):\n",
    "        points = group[[\"PC1\", \"PC2\"]].to_numpy()\n",
    "        label = f\"Cluster {cluster_label}\"\n",
    "        color = cluster_color_map.get(cluster_label, \"#333333\")\n",
    "        if label not in handled_labels:\n",
    "            _plot_outline(\n",
    "                ax,\n",
    "                points,\n",
    "                color=color,\n",
    "                linestyle=\"--\",\n",
    "                label=label,\n",
    "                zorder=3,\n",
    "            )\n",
    "            handled_labels.add(label)\n",
    "        else:\n",
    "            _plot_outline(\n",
    "                ax,\n",
    "                points,\n",
    "                color=color,\n",
    "                linestyle=\"--\",\n",
    "                label=\"\",\n",
    "                zorder=3,\n",
    "            )\n",
    "\n",
    "    party_colors = {\"D\": \"#1f77b4\", \"R\": \"#d62728\", \"I\": \"#2ca02c\", \"Other\": \"#7f7f7f\"}\n",
    "    for party, group in session_data.groupby(\"political_party\"):\n",
    "        points = group[[\"PC1\", \"PC2\"]].to_numpy()\n",
    "        color = party_colors.get(party, \"#7f7f7f\")\n",
    "        label = f\"Party {party}\"\n",
    "        linestyle_label = label if label not in handled_labels else \"\"\n",
    "        if len(points) == 0:\n",
    "            continue\n",
    "        _plot_outline(\n",
    "            ax,\n",
    "            points,\n",
    "            color=color,\n",
    "            linestyle=\"-\",\n",
    "            label=linestyle_label,\n",
    "            zorder=4,\n",
    "        )\n",
    "        handled_labels.add(label)\n",
    "\n",
    "    ax.set_title(f\"PCA Cluster Visualization — Session {session_key}{year_range}\")\n",
    "    ax.set_xlabel(\"Principal Component 1\")\n",
    "    ax.set_ylabel(\"Principal Component 2\")\n",
    "    ax.axhline(0, color=\"#cccccc\", linewidth=0.5)\n",
    "    ax.axvline(0, color=\"#cccccc\", linewidth=0.5)\n",
    "    ax.set_aspect(\"equal\")\n",
    "\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    filtered = []\n",
    "    seen = set()\n",
    "    for handle, label in zip(handles, labels):\n",
    "        if not label:\n",
    "            continue\n",
    "        if label in seen:\n",
    "            continue\n",
    "        seen.add(label)\n",
    "        filtered.append((handle, label))\n",
    "    if filtered:\n",
    "        ax.legend(*zip(*filtered), loc=\"best\", frameon=False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.close(fig)\n",
    "    display(fig)\n",
    "\n",
    "\n",
    "@interact(session_number=session_slider)\n",
    "def display_session_clusters(session_number: int):\n",
    "    _display_session_pca(session_number)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810fcf43",
   "metadata": {},
   "source": [
    "Here the outline of each party's and cluster's members are plotted. We should expect to see that mixed-voting (less polarized) senate sessions should have more overlap between the party and cluster outlines while more polarized sessions should have less of no overlap."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3028d0c",
   "metadata": {},
   "source": [
    "## Exercise 4 — Additional Polarization Metrics\n",
    "\n",
    "### Objectives\n",
    "1. **Multiple Cluster Validity Indices**: Calculate Dunn Index, Davies-Bouldin Index, and Calinski-Harabasz Index\n",
    "2. **Party-Based Separation**: Measure how well clusters separate Democrats from Republicans using crosstabs\n",
    "3. **Normalization**: Scale all metrics to 0-1 range for direct comparison\n",
    "4. **Interactive Visualization**: Use ipywidgets to explore metrics across different time ranges\n",
    "\n",
    "### Why Multiple Metrics?\n",
    "Each metric captures different aspects of cluster quality:\n",
    "\n",
    "**Dunn Index** (higher = better separation):\n",
    "- Ratio of minimum inter-cluster distance to maximum intra-cluster distance\n",
    "- Sensitive to outliers and cluster compactness\n",
    "- Range: 0 to ∞ (typically 0-2)\n",
    "\n",
    "**Davies-Bouldin Index** (lower = better separation):\n",
    "- Average similarity between each cluster and its most similar cluster\n",
    "- Considers both cluster scatter and separation\n",
    "- Range: 0 to ∞ (typically 0-3)\n",
    "\n",
    "**Calinski-Harabasz Index** (higher = better separation):\n",
    "- Ratio of between-cluster variance to within-cluster variance\n",
    "- Similar to F-statistic in ANOVA\n",
    "- Range: 0 to ∞ (typically 10-1000+)\n",
    "\n",
    "**Crosstab Separation** (higher = better party-cluster alignment):\n",
    "- Percentage of senators whose cluster matches their party's dominant cluster\n",
    "- Directly measures partisan polarization\n",
    "- Range: 0-100%\n",
    "\n",
    "### Why Normalize?\n",
    "These metrics have vastly different scales (Dunn: 0-2, CH: 10-1000+), making direct comparison impossible.\n",
    "Normalizing to 0-1 allows us to:\n",
    "- Plot all metrics on the same chart\n",
    "- Identify periods where all metrics agree (high confidence)\n",
    "- Spot divergences that might indicate measurement artifacts\n",
    "\n",
    "### Performance Considerations\n",
    "- **Dunn Index**: O(n²) distance calculations, slowest metric \n",
    "- **Davies-Bouldin**: O(n·k) where k=2 clusters, fast \n",
    "- **Calinski-Harabasz**: O(n·k), fast \n",
    "- **Crosstab**: O(n), fastest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0093be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dunn_index(X: np.ndarray, labels: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Dunn Index for clustering quality.\n",
    "    \n",
    "    Dunn Index = min(inter-cluster distance) / max(intra-cluster distance)\n",
    "    Higher values indicate better clustering (well-separated, compact clusters).\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix (n_samples, n_features)\n",
    "        labels: Cluster labels for each sample\n",
    "    \n",
    "    Returns:\n",
    "        Dunn index value\n",
    "    \"\"\"\n",
    "    from scipy.spatial.distance import pdist, squareform\n",
    "    \n",
    "    unique_labels = np.unique(labels)\n",
    "    if len(unique_labels) < 2:\n",
    "        return np.nan\n",
    "    \n",
    "    # Calculate pairwise distances\n",
    "    distances = squareform(pdist(X, metric='euclidean'))\n",
    "    \n",
    "    # Calculate minimum inter-cluster distance\n",
    "    min_inter_cluster = np.inf\n",
    "    for i in range(len(unique_labels)):\n",
    "        for j in range(i + 1, len(unique_labels)):\n",
    "            cluster_i_indices = np.where(labels == unique_labels[i])[0]\n",
    "            cluster_j_indices = np.where(labels == unique_labels[j])[0]\n",
    "            \n",
    "            # Get distances between all pairs of points from different clusters\n",
    "            inter_distances = distances[np.ix_(cluster_i_indices, cluster_j_indices)]\n",
    "            if inter_distances.size > 0:\n",
    "                min_inter_cluster = min(min_inter_cluster, np.min(inter_distances))\n",
    "    \n",
    "    # Calculate maximum intra-cluster distance (diameter)\n",
    "    max_intra_cluster = 0\n",
    "    for label in unique_labels:\n",
    "        cluster_indices = np.where(labels == label)[0]\n",
    "        if len(cluster_indices) > 1:\n",
    "            intra_distances = distances[np.ix_(cluster_indices, cluster_indices)]\n",
    "            max_intra_cluster = max(max_intra_cluster, np.max(intra_distances))\n",
    "    \n",
    "    if max_intra_cluster == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    return min_inter_cluster / max_intra_cluster\n",
    "\n",
    "\n",
    "# %%\n",
    "# Initialize storage for Exercise 4 metrics\n",
    "dunn_scores = []\n",
    "db_scores = []\n",
    "ch_scores = []\n",
    "crosstab_separation_scores = []\n",
    "\n",
    "print(\"\\nComputing additional polarization metrics (Exercise 4)...\")\n",
    "\n",
    "for session_num in tqdm(ANALYSIS_SESSIONS, desc=\"Calculating Cluster Validity Metrics\"):\n",
    "    try:\n",
    "        # Fetch vote data for this session\n",
    "        votes_df = duckdb_conn.execute(\n",
    "            f\"\"\"\n",
    "            SELECT icpsr, rollnumber, cast_code\n",
    "            FROM {settings.processed_votes_table}\n",
    "            WHERE session_num = ?\n",
    "            \"\"\",\n",
    "            [session_num],\n",
    "        ).fetchdf()\n",
    "\n",
    "        if votes_df.empty:\n",
    "            continue\n",
    "\n",
    "        # Prepare session matrix\n",
    "        session_matrix = prepare_session_matrix(votes_df)\n",
    "        if session_matrix is None or session_matrix.shape[0] < 3:\n",
    "            continue\n",
    "\n",
    "        feature_matrix = session_matrix.to_numpy(dtype=float)\n",
    "\n",
    "        # Perform clustering\n",
    "        kmeans = KMeans(n_clusters=2, n_init=10, random_state=42)\n",
    "        labels = kmeans.fit_predict(feature_matrix)\n",
    "\n",
    "        # Perform PCA for visualization\n",
    "        pca_model = PCA(n_components=2, random_state=42)\n",
    "        X_pca = pca_model.fit_transform(feature_matrix)\n",
    "\n",
    "        # Calculate sklearn metrics on PCA-transformed data\n",
    "        try:\n",
    "            db_score = davies_bouldin_score(X_pca, labels)\n",
    "            db_scores.append({\n",
    "                \"session_num\": session_num,\n",
    "                \"db_score\": db_score\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: Could not calculate DB score for session {session_num}: {e}\")\n",
    "\n",
    "        try:\n",
    "            ch_score = calinski_harabasz_score(X_pca, labels)\n",
    "            ch_scores.append({\n",
    "                \"session_num\": session_num,\n",
    "                \"ch_score\": ch_score\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: Could not calculate CH score for session {session_num}: {e}\")\n",
    "\n",
    "        # Calculate Dunn Index\n",
    "        try:\n",
    "            dunn_score = compute_dunn_index(X_pca, labels)\n",
    "            if not np.isnan(dunn_score):\n",
    "                dunn_scores.append({\n",
    "                    \"session_num\": session_num,\n",
    "                    \"dunn_score\": dunn_score\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: Could not calculate Dunn index for session {session_num}: {e}\")\n",
    "\n",
    "        # Calculate Crosstab Separation Metric\n",
    "        # This requires party information from Exercise 3\n",
    "        if session_num in session_viz_data:\n",
    "            merged_df = session_viz_data[session_num]\n",
    "            \n",
    "            # Filter for main parties (D and R)\n",
    "            main_parties_df = merged_df[merged_df[\"political_party\"].isin([\"D\", \"R\"])].copy()\n",
    "            \n",
    "            if not main_parties_df.empty and len(main_parties_df) >= 2:\n",
    "                # Create crosstab\n",
    "                ct = pd.crosstab(\n",
    "                    main_parties_df[\"political_party\"],\n",
    "                    main_parties_df[\"cluster_label\"]\n",
    "                )\n",
    "                \n",
    "                # Ensure both clusters and both parties exist in crosstab\n",
    "                for cluster in [0, 1]:\n",
    "                    if cluster not in ct.columns:\n",
    "                        ct[cluster] = 0\n",
    "                for party in [\"D\", \"R\"]:\n",
    "                    if party not in ct.index:\n",
    "                        ct.loc[party] = 0\n",
    "                \n",
    "                ct = ct.reindex(index=[\"D\", \"R\"], columns=[0, 1], fill_value=0)\n",
    "                \n",
    "                # Calculate separation percentage\n",
    "                # Scenario 1: D in cluster 0, R in cluster 1\n",
    "                scenario_1_correct = ct.loc[\"D\", 0] + ct.loc[\"R\", 1]\n",
    "                # Scenario 2: D in cluster 1, R in cluster 0\n",
    "                scenario_2_correct = ct.loc[\"D\", 1] + ct.loc[\"R\", 0]\n",
    "                \n",
    "                total_members = ct.sum().sum()\n",
    "                if total_members > 0:\n",
    "                    separation_pct = max(scenario_1_correct, scenario_2_correct) / total_members * 100\n",
    "                    crosstab_separation_scores.append({\n",
    "                        \"session_num\": session_num,\n",
    "                        \"separation_pct\": separation_pct,\n",
    "                        \"total_dr_members\": int(total_members),\n",
    "                        \"scenario_1_pct\": (scenario_1_correct / total_members * 100),\n",
    "                        \"scenario_2_pct\": (scenario_2_correct / total_members * 100)\n",
    "                    })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing session {session_num} for Exercise 4: {e}\")\n",
    "        continue\n",
    "\n",
    "# Convert to DataFrames\n",
    "dunn_df = pd.DataFrame(dunn_scores)\n",
    "db_df = pd.DataFrame(db_scores)\n",
    "ch_df = pd.DataFrame(ch_scores)\n",
    "crosstab_df = pd.DataFrame(crosstab_separation_scores)\n",
    "\n",
    "print(f\"\\n=== Exercise 4 Metrics Summary ===\")\n",
    "print(f\"Dunn Index scores calculated: {len(dunn_df)}\")\n",
    "if not dunn_df.empty:\n",
    "    print(f\"  Range: {dunn_df['dunn_score'].min():.4f} - {dunn_df['dunn_score'].max():.4f}\")\n",
    "    print(f\"  Mean: {dunn_df['dunn_score'].mean():.4f}\")\n",
    "\n",
    "print(f\"\\nDavies-Bouldin scores calculated: {len(db_df)}\")\n",
    "if not db_df.empty:\n",
    "    print(f\"  Range: {db_df['db_score'].min():.4f} - {db_df['db_score'].max():.4f}\")\n",
    "    print(f\"  Mean: {db_df['db_score'].mean():.4f}\")\n",
    "\n",
    "print(f\"\\nCalinski-Harabasz scores calculated: {len(ch_df)}\")\n",
    "if not ch_df.empty:\n",
    "    print(f\"  Range: {ch_df['ch_score'].min():.2f} - {ch_df['ch_score'].max():.2f}\")\n",
    "    print(f\"  Mean: {ch_df['ch_score'].mean():.2f}\")\n",
    "\n",
    "print(f\"\\nCrosstab Separation scores calculated: {len(crosstab_df)}\")\n",
    "if not crosstab_df.empty:\n",
    "    print(f\"  Range: {crosstab_df['separation_pct'].min():.2f}% - {crosstab_df['separation_pct'].max():.2f}%\")\n",
    "    print(f\"  Mean: {crosstab_df['separation_pct'].mean():.2f}%\")\n",
    "\n",
    "if \"display\" in globals() and display is not None:\n",
    "    print(\"\\nSample Dunn Index scores:\")\n",
    "    display(dunn_df.head(10))\n",
    "    print(\"\\nSample Davies-Bouldin scores:\")\n",
    "    display(db_df.head(10))\n",
    "    print(\"\\nSample Calinski-Harabasz scores:\")\n",
    "    display(ch_df.head(10))\n",
    "    print(\"\\nSample Crosstab Separation scores:\")\n",
    "    display(crosstab_df.head(10))\n",
    "else:\n",
    "    print(\"\\nSample metrics:\")\n",
    "    print(dunn_df.head(10))\n",
    "    print(db_df.head(10))\n",
    "    print(ch_df.head(10))\n",
    "    print(crosstab_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b067a3",
   "metadata": {},
   "source": [
    "### Exercise 4 Visualization: Time Series of Cluster Validity Metrics\n",
    "\n",
    "Now let's visualize how these metrics evolve over time. We'll create separate plots for each metric\n",
    "since they have different scales and interpretations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f62bd42",
   "metadata": {},
   "source": [
    "Try this now for some of the older data sets, and generate a measure that indicates how separated the parties are. For example, for the 116th congress, once you create the crosstab, based on those results you can say that parties are 100% separated. Using that, ilustrate separation for older congreeses and you could plot this measure across time, to see again how political party based polarization has evolved over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c611cf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive Dunn Index visualization with session range slider\n",
    "if not dunn_df.empty:\n",
    "    # Get available session numbers as integers\n",
    "    dunn_session_ints = sorted([int(s) for s in dunn_df[\"session_num\"].unique()])\n",
    "    \n",
    "    dunn_range_slider = IntRangeSlider(\n",
    "        value=(dunn_session_ints[0], dunn_session_ints[-1]),\n",
    "        min=dunn_session_ints[0],\n",
    "        max=dunn_session_ints[-1],\n",
    "        step=1,\n",
    "        description=\"Sessions\",\n",
    "        continuous_update=False,\n",
    "        layout=Layout(width=\"70%\")\n",
    "    )\n",
    "    \n",
    "    def _render_dunn_window(session_window: tuple[int, int]) -> None:\n",
    "        lower, upper = session_window\n",
    "        selected_sessions = [f\"{num:03d}\" for num in range(lower, upper + 1)]\n",
    "        subset_df = dunn_df[dunn_df[\"session_num\"].isin(selected_sessions)].copy()\n",
    "        if subset_df.empty:\n",
    "            print(\"No Dunn Index data available for the selected session window.\")\n",
    "            return\n",
    "        \n",
    "        subset_df = subset_df.sort_values(\"session_num\")\n",
    "        \n",
    "        # Map session numbers to dates\n",
    "        subset_df = subset_df.merge(congress_dates_df[[\"session_num\", \"start_year\"]], on=\"session_num\", how=\"left\")\n",
    "        \n",
    "        fig_dunn = go.Figure()\n",
    "        fig_dunn.add_trace(go.Scatter(\n",
    "            x=subset_df[\"start_year\"],\n",
    "            y=subset_df[\"dunn_score\"],\n",
    "            mode=\"lines+markers\",\n",
    "            name=\"Dunn Index\",\n",
    "            line=dict(color=\"#1f77b4\", width=2),\n",
    "            marker=dict(size=6),\n",
    "            text=subset_df[\"session_num\"],\n",
    "            hovertemplate=\"<b>Session %{text}</b><br>Year: %{x}<br>Dunn Index: %{y:.4f}<extra></extra>\"\n",
    "        ))\n",
    "        \n",
    "        fig_dunn.update_layout(\n",
    "            title=\"Dunn Index Over Time (Higher = Better Separation)\",\n",
    "            xaxis_title=\"Session Start Year\",\n",
    "            yaxis_title=\"Dunn Index\",\n",
    "            hovermode=\"x unified\",\n",
    "            template=\"plotly_white\"\n",
    "        )\n",
    "        \n",
    "        fig_dunn.show()\n",
    "    \n",
    "    @interact(session_window=dunn_range_slider)\n",
    "    def display_dunn_interactive(session_window: tuple[int, int]):\n",
    "        _render_dunn_window(session_window)\n",
    "else:\n",
    "    print(\"No Dunn Index data available for visualization.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747a01a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Interactive Davies-Bouldin Index visualization with session range slider\n",
    "if not db_df.empty:\n",
    "    # Get available session numbers as integers\n",
    "    db_session_ints = sorted([int(s) for s in db_df[\"session_num\"].unique()])\n",
    "    \n",
    "    db_range_slider = IntRangeSlider(\n",
    "        value=(db_session_ints[0], db_session_ints[-1]),\n",
    "        min=db_session_ints[0],\n",
    "        max=db_session_ints[-1],\n",
    "        step=1,\n",
    "        description=\"Sessions\",\n",
    "        continuous_update=False,\n",
    "        layout=Layout(width=\"70%\")\n",
    "    )\n",
    "    \n",
    "    def _render_db_window(session_window: tuple[int, int]) -> None:\n",
    "        lower, upper = session_window\n",
    "        selected_sessions = [f\"{num:03d}\" for num in range(lower, upper + 1)]\n",
    "        subset_df = db_df[db_df[\"session_num\"].isin(selected_sessions)].copy()\n",
    "        if subset_df.empty:\n",
    "            print(\"No Davies-Bouldin data available for the selected session window.\")\n",
    "            return\n",
    "        \n",
    "        subset_df = subset_df.sort_values(\"session_num\")\n",
    "        \n",
    "        # Map session numbers to dates\n",
    "        subset_df = subset_df.merge(congress_dates_df[[\"session_num\", \"start_year\"]], on=\"session_num\", how=\"left\")\n",
    "        \n",
    "        fig_db = go.Figure()\n",
    "        fig_db.add_trace(go.Scatter(\n",
    "            x=subset_df[\"start_year\"],\n",
    "            y=subset_df[\"db_score\"],\n",
    "            mode=\"lines+markers\",\n",
    "            name=\"Davies-Bouldin Index\",\n",
    "            line=dict(color=\"#d62728\", width=2),\n",
    "            marker=dict(size=6),\n",
    "            text=subset_df[\"session_num\"],\n",
    "            hovertemplate=\"<b>Session %{text}</b><br>Year: %{x}<br>Davies-Bouldin: %{y:.4f}<extra></extra>\"\n",
    "        ))\n",
    "        \n",
    "        fig_db.update_layout(\n",
    "            title=\"Davies-Bouldin Index Over Time (Lower = Better Separation)\",\n",
    "            xaxis_title=\"Session Start Year\",\n",
    "            yaxis_title=\"Davies-Bouldin Index\",\n",
    "            hovermode=\"x unified\",\n",
    "            template=\"plotly_white\"\n",
    "        )\n",
    "        \n",
    "        fig_db.show()\n",
    "    \n",
    "    @interact(session_window=db_range_slider)\n",
    "    def display_db_interactive(session_window: tuple[int, int]):\n",
    "        _render_db_window(session_window)\n",
    "else:\n",
    "    print(\"No Davies-Bouldin data available for visualization.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f02f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Interactive Calinski-Harabasz Index visualization with session range slider\n",
    "if not ch_df.empty:\n",
    "    # Get available session numbers as integers\n",
    "    ch_session_ints = sorted([int(s) for s in ch_df[\"session_num\"].unique()])\n",
    "    \n",
    "    ch_range_slider = IntRangeSlider(\n",
    "        value=(ch_session_ints[0], ch_session_ints[-1]),\n",
    "        min=ch_session_ints[0],\n",
    "        max=ch_session_ints[-1],\n",
    "        step=1,\n",
    "        description=\"Sessions\",\n",
    "        continuous_update=False,\n",
    "        layout=Layout(width=\"70%\")\n",
    "    )\n",
    "    \n",
    "    def _render_ch_window(session_window: tuple[int, int]) -> None:\n",
    "        lower, upper = session_window\n",
    "        selected_sessions = [f\"{num:03d}\" for num in range(lower, upper + 1)]\n",
    "        subset_df = ch_df[ch_df[\"session_num\"].isin(selected_sessions)].copy()\n",
    "        if subset_df.empty:\n",
    "            print(\"No Calinski-Harabasz data available for the selected session window.\")\n",
    "            return\n",
    "        \n",
    "        subset_df = subset_df.sort_values(\"session_num\")\n",
    "        \n",
    "        # Map session numbers to dates\n",
    "        subset_df = subset_df.merge(congress_dates_df[[\"session_num\", \"start_year\"]], on=\"session_num\", how=\"left\")\n",
    "        \n",
    "        fig_ch = go.Figure()\n",
    "        fig_ch.add_trace(go.Scatter(\n",
    "            x=subset_df[\"start_year\"],\n",
    "            y=subset_df[\"ch_score\"],\n",
    "            mode=\"lines+markers\",\n",
    "            name=\"Calinski-Harabasz Index\",\n",
    "            line=dict(color=\"#2ca02c\", width=2),\n",
    "            marker=dict(size=6),\n",
    "            text=subset_df[\"session_num\"],\n",
    "            hovertemplate=\"<b>Session %{text}</b><br>Year: %{x}<br>Calinski-Harabasz: %{y:.2f}<extra></extra>\"\n",
    "        ))\n",
    "        \n",
    "        fig_ch.update_layout(\n",
    "            title=\"Calinski-Harabasz Index Over Time (Higher = Better Separation)\",\n",
    "            xaxis_title=\"Session Start Year\",\n",
    "            yaxis_title=\"Calinski-Harabasz Index\",\n",
    "            hovermode=\"x unified\",\n",
    "            template=\"plotly_white\"\n",
    "        )\n",
    "        \n",
    "        fig_ch.show()\n",
    "    \n",
    "    @interact(session_window=ch_range_slider)\n",
    "    def display_ch_interactive(session_window: tuple[int, int]):\n",
    "        _render_ch_window(session_window)\n",
    "else:\n",
    "    print(\"No Calinski-Harabasz data available for visualization.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fece86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Interactive Crosstab Separation visualization with session range slider\n",
    "if not crosstab_df.empty:\n",
    "    # Get available session numbers as integers\n",
    "    crosstab_session_ints = sorted([int(s) for s in crosstab_df[\"session_num\"].unique()])\n",
    "    \n",
    "    crosstab_range_slider = IntRangeSlider(\n",
    "        value=(crosstab_session_ints[0], crosstab_session_ints[-1]),\n",
    "        min=crosstab_session_ints[0],\n",
    "        max=crosstab_session_ints[-1],\n",
    "        step=1,\n",
    "        description=\"Sessions\",\n",
    "        continuous_update=False,\n",
    "        layout=Layout(width=\"70%\")\n",
    "    )\n",
    "    \n",
    "    def _render_crosstab_window(session_window: tuple[int, int]) -> None:\n",
    "        lower, upper = session_window\n",
    "        selected_sessions = [f\"{num:03d}\" for num in range(lower, upper + 1)]\n",
    "        subset_df = crosstab_df[crosstab_df[\"session_num\"].isin(selected_sessions)].copy()\n",
    "        if subset_df.empty:\n",
    "            print(\"No Crosstab Separation data available for the selected session window.\")\n",
    "            return\n",
    "        \n",
    "        subset_df = subset_df.sort_values(\"session_num\")\n",
    "        \n",
    "        # Map session numbers to dates\n",
    "        subset_df = subset_df.merge(congress_dates_df[[\"session_num\", \"start_year\"]], on=\"session_num\", how=\"left\")\n",
    "        \n",
    "        fig_crosstab = go.Figure()\n",
    "        fig_crosstab.add_trace(go.Scatter(\n",
    "            x=subset_df[\"start_year\"],\n",
    "            y=subset_df[\"separation_pct\"],\n",
    "            mode=\"lines+markers\",\n",
    "            name=\"Party Separation %\",\n",
    "            line=dict(color=\"#ff7f0e\", width=2),\n",
    "            marker=dict(size=6),\n",
    "            customdata=subset_df[[\"session_num\", \"total_dr_members\"]],\n",
    "            hovertemplate=\"<b>Session %{customdata[0]}</b><br>Year: %{x}<br>Separation: %{y:.1f}%<br>D+R Members: %{customdata[1]}<extra></extra>\"\n",
    "        ))\n",
    "        \n",
    "        fig_crosstab.update_layout(\n",
    "            title=\"Crosstab Party Separation Over Time (Higher = More Polarized)\",\n",
    "            xaxis_title=\"Session Start Year\",\n",
    "            yaxis_title=\"Separation Percentage (%)\",\n",
    "            hovermode=\"x unified\",\n",
    "            template=\"plotly_white\"\n",
    "        )\n",
    "        \n",
    "        # Add reference line at 50% (random chance)\n",
    "        fig_crosstab.add_hline(\n",
    "            y=50,\n",
    "            line_dash=\"dash\",\n",
    "            line_color=\"gray\",\n",
    "            annotation_text=\"50% (Random)\",\n",
    "            annotation_position=\"right\"\n",
    "        )\n",
    "        \n",
    "        fig_crosstab.show()\n",
    "    \n",
    "    @interact(session_window=crosstab_range_slider)\n",
    "    def display_crosstab_interactive(session_window: tuple[int, int]):\n",
    "        _render_crosstab_window(session_window)\n",
    "else:\n",
    "    print(\"No Crosstab Separation data available for visualization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901c18c3",
   "metadata": {},
   "source": [
    "### Combined Metric Comparison\n",
    "\n",
    "Let's create a normalized comparison plot to see how different metrics correlate.\n",
    "We'll normalize each metric to 0-1 scale where higher values indicate more polarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66aba221",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (dunn_df.empty and db_df.empty and ch_df.empty and crosstab_df.empty):\n",
    "    # Merge all metrics\n",
    "    combined_df = dunn_df.copy()\n",
    "    if not db_df.empty:\n",
    "        combined_df = combined_df.merge(db_df, on=\"session_num\", how=\"outer\")\n",
    "    if not ch_df.empty:\n",
    "        combined_df = combined_df.merge(ch_df, on=\"session_num\", how=\"outer\")\n",
    "    if not crosstab_df.empty:\n",
    "        combined_df = combined_df.merge(crosstab_df[[\"session_num\", \"separation_pct\"]], on=\"session_num\", how=\"outer\")\n",
    "\n",
    "    combined_df = combined_df.sort_values(\"session_num\")\n",
    "\n",
    "    # Normalize metrics (0-1 scale, higher = more polarized)\n",
    "    if \"dunn_score\" in combined_df.columns:\n",
    "        # Dunn: higher is better (more polarized)\n",
    "        combined_df[\"dunn_normalized\"] = (\n",
    "            (combined_df[\"dunn_score\"] - combined_df[\"dunn_score\"].min()) /\n",
    "            (combined_df[\"dunn_score\"].max() - combined_df[\"dunn_score\"].min())\n",
    "        )\n",
    "\n",
    "    if \"db_score\" in combined_df.columns:\n",
    "        # DB: lower is better, so invert\n",
    "        combined_df[\"db_normalized\"] = 1 - (\n",
    "            (combined_df[\"db_score\"] - combined_df[\"db_score\"].min()) /\n",
    "            (combined_df[\"db_score\"].max() - combined_df[\"db_score\"].min())\n",
    "        )\n",
    "\n",
    "    if \"ch_score\" in combined_df.columns:\n",
    "        # CH: higher is better (more polarized)\n",
    "        combined_df[\"ch_normalized\"] = (\n",
    "            (combined_df[\"ch_score\"] - combined_df[\"ch_score\"].min()) /\n",
    "            (combined_df[\"ch_score\"].max() - combined_df[\"ch_score\"].min())\n",
    "        )\n",
    "\n",
    "    if \"separation_pct\" in combined_df.columns:\n",
    "        # Separation: normalize from 0-100% to 0-1\n",
    "        combined_df[\"separation_normalized\"] = combined_df[\"separation_pct\"] / 100\n",
    "        \n",
    "    # Bring silhouette scores into the combined frame and create normalized aliases for Exercise 6\n",
    "    if 'silhouette_overview_df' in globals() and isinstance(silhouette_overview_df, pd.DataFrame) and not silhouette_overview_df.empty:\n",
    "        combined_df = combined_df.merge(\n",
    "            silhouette_overview_df[[\"session_num\", \"silhouette_score\"]],\n",
    "            on=\"session_num\",\n",
    "            how=\"left\"\n",
    "        )\n",
    "        # Normalize silhouette to 0-1\n",
    "        s_min = combined_df[\"silhouette_score\"].min(skipna=True) if \"silhouette_score\" in combined_df else None\n",
    "        s_max = combined_df[\"silhouette_score\"].max(skipna=True) if \"silhouette_score\" in combined_df else None\n",
    "        if s_min is not None and s_max is not None and pd.notna(s_min) and pd.notna(s_max) and s_max > s_min:\n",
    "            combined_df[\"silhouette_norm\"] = (combined_df[\"silhouette_score\"] - s_min) / (s_max - s_min)\n",
    "        else:\n",
    "            combined_df[\"silhouette_norm\"] = np.nan\n",
    "\n",
    "    # Ensure separation_normalized is available before aliasing\n",
    "    if \"separation_pct\" in combined_df.columns and \"separation_normalized\" not in combined_df.columns:\n",
    "        combined_df[\"separation_normalized\"] = combined_df[\"separation_pct\"] / 100\n",
    "\n",
    "    # Provide alias names expected by the Exercise 6 dashboard\n",
    "    if \"dunn_normalized\" in combined_df.columns:\n",
    "        combined_df[\"dunn_norm\"] = combined_df[\"dunn_normalized\"]\n",
    "    if \"db_normalized\" in combined_df.columns:\n",
    "        combined_df[\"db_norm\"] = combined_df[\"db_normalized\"]\n",
    "    if \"ch_normalized\" in combined_df.columns:\n",
    "        combined_df[\"ch_norm\"] = combined_df[\"ch_normalized\"]\n",
    "    if \"separation_normalized\" in combined_df.columns:\n",
    "        combined_df[\"crosstab_norm\"] = combined_df[\"separation_normalized\"]\n",
    "\n",
    "    # Materialize combined_metrics_df for Exercise 6 if at least one normalized column exists\n",
    "    norm_cols = [c for c in [\"silhouette_norm\", \"dunn_norm\", \"db_norm\", \"ch_norm\", \"crosstab_norm\"] if c in combined_df.columns]\n",
    "    if norm_cols:\n",
    "        cols = [\"session_num\"] + norm_cols\n",
    "        combined_metrics_df = combined_df[cols].sort_values(\"session_num\")\n",
    "        # Ensure expected columns exist for downstream plotting\n",
    "        for _col in [\"silhouette_norm\", \"dunn_norm\", \"db_norm\", \"ch_norm\", \"crosstab_norm\"]:\n",
    "            if _col not in combined_metrics_df.columns:\n",
    "                combined_metrics_df[_col] = np.nan\n",
    "\n",
    "\n",
    "        # Separation: normalize from 0-100% to 0-1\n",
    "        combined_df[\"separation_normalized\"] = combined_df[\"separation_pct\"] / 100\n",
    "\n",
    "    # Create comparison plot\n",
    "    fig_combined = go.Figure()\n",
    "\n",
    "    if \"dunn_normalized\" in combined_df.columns:\n",
    "        fig_combined.add_trace(go.Scatter(\n",
    "            x=combined_df[\"session_num\"],\n",
    "            y=combined_df[\"dunn_normalized\"],\n",
    "            mode=\"lines\",\n",
    "            name=\"Dunn Index\",\n",
    "            line=dict(width=2)\n",
    "        ))\n",
    "\n",
    "    if \"db_normalized\" in combined_df.columns:\n",
    "        fig_combined.add_trace(go.Scatter(\n",
    "            x=combined_df[\"session_num\"],\n",
    "            y=combined_df[\"db_normalized\"],\n",
    "            mode=\"lines\",\n",
    "            name=\"Davies-Bouldin (inverted)\",\n",
    "            line=dict(width=2)\n",
    "        ))\n",
    "\n",
    "    if \"ch_normalized\" in combined_df.columns:\n",
    "        fig_combined.add_trace(go.Scatter(\n",
    "            x=combined_df[\"session_num\"],\n",
    "            y=combined_df[\"ch_normalized\"],\n",
    "            mode=\"lines\",\n",
    "            name=\"Calinski-Harabasz\",\n",
    "            line=dict(width=2)\n",
    "        ))\n",
    "\n",
    "    if \"separation_normalized\" in combined_df.columns:\n",
    "        fig_combined.add_trace(go.Scatter(\n",
    "            x=combined_df[\"session_num\"],\n",
    "            y=combined_df[\"separation_normalized\"],\n",
    "            mode=\"lines\",\n",
    "            name=\"Party Separation\",\n",
    "            line=dict(width=2)\n",
    "        ))\n",
    "\n",
    "    fig_combined.update_layout(\n",
    "        title=\"Normalized Polarization Metrics Comparison (Higher = More Polarized)\",\n",
    "        xaxis_title=\"Congressional Session\",\n",
    "        yaxis_title=\"Normalized Score (0-1)\",\n",
    "        hovermode=\"x unified\",\n",
    "        template=\"plotly_white\",\n",
    "        legend=dict(yanchor=\"bottom\", y=-0.8, xanchor=\"right\", x=0.75)\n",
    "    )\n",
    "\n",
    "    fig_combined.show()\n",
    "else:\n",
    "    print(\"Not enough metric data available for combined comparison.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752dcf44",
   "metadata": {},
   "source": [
    "## Exercise 5 — Intra-Party Cohesion via Vector Similarity\n",
    "\n",
    "### Objectives\n",
    "1. **Measure Intra-Party Cohesion**: Calculate how similar each senator's voting pattern is to their party peers\n",
    "2. **N-Dimensional PCA**: Explore how PCA dimensionality (2D-50D) affects similarity calculations\n",
    "3. **DuckDB Vector Storage**: Store PCA vectors as fixed-length arrays for efficient similarity queries\n",
    "4. **Interactive Visualizations**: Build two interactive tools:\n",
    "   - Time-series plot of party cohesion with PCA dimensionality control\n",
    "   - Senator similarity explorer with scatter plots\n",
    "5. **Performance Optimization**: Use DuckDB's `array_cosine_similarity()` function for fast vectorized comparisons\n",
    "\n",
    "### Why Intra-Party Cohesion Matters\n",
    "Previous exercises measured **inter-party** polarization (how different Democrats and Republicans are).\n",
    "This exercise measures **intra-party** cohesion (how similar members within each party are):\n",
    "- **High cohesion** (similarity → 1.0): Party members vote together consistently\n",
    "- **Low cohesion** (similarity → 0.0): Party members frequently disagree\n",
    "**Interpretation:**\n",
    "- **High polarization** = High inter-party separation + High intra-party cohesion\n",
    "- **Low polarization** = Low inter-party separation + Low intra-party cohesion\n",
    "\n",
    "### Why N-Dimensional PCA?\n",
    "**2D PCA** (used in Exercise 3 scatter plots):\n",
    "- Good for visualization (humans can see 2D plots)\n",
    "- Captures less variance in voting patterns\n",
    "- May miss important voting dimensions\n",
    "**20D-50D PCA** (used for similarity calculations):\n",
    "- Captures more variance\n",
    "- More accurate similarity measurements\n",
    "- Better reflects true voting behavior\n",
    "**Trade-off**: We use 2D PCA for scatter plots (visualization) and N-D PCA for similarity calculations (accuracy).\n",
    "\n",
    "### DuckDB Performance Benefits\n",
    "**Why store PCA vectors in DuckDB?**\n",
    "1. **Efficient Storage**: Fixed-length ARRAY type (`FLOAT[20]`) is more compact than JSON or text\n",
    "2. **Fast Similarity Queries**: DuckDB's `array_cosine_similarity()` is vectorized and runs in C++\n",
    "3. **Indexed Lookups**: Query a single senator's vector in <1ms vs. scanning a pandas DataFrame\n",
    "4. **Persistent Cache**: Vectors persist across notebook restarts, no need to recompute PCA\n",
    "**Performance comparison** (for 100 sessions, ~8,000 senators):\n",
    "- **Pandas approach**: Store vectors in DataFrame, use sklearn's `cosine_similarity()` \n",
    "- **DuckDB approach**: Store vectors in table, use `array_cosine_similarity()` → ~50-100ms per query\n",
    "\n",
    "### Addition:\n",
    "- **Interactive PCA Dimensionality Control**: Use an IntSlider to dynamically adjust PCA dimensions\n",
    "- This allows exploration of how dimensionality affects similarity calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdde6c62",
   "metadata": {},
   "source": [
    "Go further by _making a measure_ of **how likely it is that two individuals of the same political party are in the same cluster**, and you could plot this measure across time, to see again how political party based polarization has evolved over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5919f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pca_vectors_ndim(n_components: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute N-dimensional PCA vectors for all senators across all sessions.\n",
    "\n",
    "    Args:\n",
    "        n_components: Number of PCA components to compute\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with columns: session_num, icpsr, senator_name, political_party, pca_vector\n",
    "    \"\"\"\n",
    "    from sklearn.decomposition import PCA\n",
    "\n",
    "    vector_records = []\n",
    "\n",
    "    print(f\"\\nComputing {n_components}-dimensional PCA vectors...\")\n",
    "\n",
    "    for session_num in tqdm(ANALYSIS_SESSIONS, desc=f\"Computing {n_components}D PCA\"):\n",
    "        try:\n",
    "            # Fetch vote data for this session\n",
    "            votes_df = duckdb_conn.execute(\n",
    "                f\"\"\"\n",
    "                SELECT icpsr, rollnumber, cast_code\n",
    "                FROM {settings.processed_votes_table}\n",
    "                WHERE session_num = ?\n",
    "                \"\"\",\n",
    "                [session_num],\n",
    "            ).fetchdf()\n",
    "\n",
    "            if votes_df.empty:\n",
    "                continue\n",
    "\n",
    "            # Prepare session matrix\n",
    "            session_matrix = prepare_session_matrix(votes_df)\n",
    "            if session_matrix is None or session_matrix.empty:\n",
    "                continue\n",
    "\n",
    "            # Compute PCA with n_components\n",
    "            feature_matrix = session_matrix.to_numpy(dtype=float)\n",
    "\n",
    "            # Adjust n_components if it exceeds available dimensions\n",
    "            actual_n_components = min(n_components, feature_matrix.shape[1], feature_matrix.shape[0])\n",
    "\n",
    "            if actual_n_components < 2:\n",
    "                continue\n",
    "\n",
    "            pca_model = PCA(n_components=actual_n_components, random_state=42)\n",
    "            pca_coords = pca_model.fit_transform(feature_matrix)\n",
    "\n",
    "            # Fetch party information\n",
    "            party_df = duckdb_conn.execute(\n",
    "                f\"\"\"\n",
    "                SELECT icpsr, political_party, senator_name\n",
    "                FROM {settings.members_table}\n",
    "                WHERE CAST(session_num AS INTEGER) = CAST(? AS INTEGER)\n",
    "                \"\"\",\n",
    "                [session_num],\n",
    "            ).fetchdf()\n",
    "\n",
    "            if party_df.empty:\n",
    "                continue\n",
    "\n",
    "            party_df[\"icpsr\"] = party_df[\"icpsr\"].astype(str)\n",
    "            party_df = party_df.drop_duplicates(subset=[\"icpsr\"])\n",
    "\n",
    "            # Create DataFrame with PCA coordinates and icpsr\n",
    "            pca_df = pd.DataFrame(pca_coords)\n",
    "            pca_df[\"icpsr\"] = session_matrix.index.astype(str)\n",
    "\n",
    "            # Merge PCA coords with party info\n",
    "            party_df[\"icpsr\"] = party_df[\"icpsr\"].astype(str)\n",
    "            merged_df = pca_df.merge(party_df, on=\"icpsr\", how=\"inner\")\n",
    "\n",
    "            # Filter for main parties (D and R)\n",
    "            merged_df = merged_df[merged_df[\"political_party\"].isin([\"D\", \"R\"])]\n",
    "\n",
    "            if merged_df.empty:\n",
    "                continue\n",
    "\n",
    "            # Store vectors - get PCA columns (all numeric columns except metadata)\n",
    "            pca_columns = [col for col in merged_df.columns if isinstance(col, int)]\n",
    "\n",
    "            for _, row in merged_df.iterrows():\n",
    "                vector_records.append({\n",
    "                    \"session_num\": session_num,\n",
    "                    \"icpsr\": row[\"icpsr\"],\n",
    "                    \"senator_name\": row[\"senator_name\"],\n",
    "                    \"political_party\": row[\"political_party\"],\n",
    "                    \"pca_vector\": row[pca_columns].tolist()\n",
    "                })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing session {session_num}: {e}\")\n",
    "            continue\n",
    "\n",
    "    return pd.DataFrame(vector_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e590e59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recalculate_cohesion_scores(vectors_df: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Calculate intra-party cohesion scores using DuckDB array_cosine_similarity.\n",
    "\n",
    "    Args:\n",
    "        vectors_df: DataFrame with senator PCA vectors\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (senator_similarity_df, cohesion_stats_df)\n",
    "    \"\"\"\n",
    "    party_cohesion_stats = []\n",
    "    senator_similarity_records = []\n",
    "\n",
    "    print(\"\\nCalculating intra-party cohesion scores...\")\n",
    "\n",
    "    # First, store vectors in DuckDB\n",
    "    duckdb_conn.execute(\"DROP TABLE IF EXISTS senator_vectors\")\n",
    "    duckdb_conn.register(\"vectors_temp\", vectors_df)\n",
    "\n",
    "    # Determine vector length from first record\n",
    "    if not vectors_df.empty:\n",
    "        vector_len = len(vectors_df.iloc[0][\"pca_vector\"])\n",
    "\n",
    "        # Create table with appropriate array size\n",
    "        duckdb_conn.execute(f\"\"\"\n",
    "            CREATE TABLE senator_vectors AS\n",
    "            SELECT\n",
    "                session_num,\n",
    "                icpsr,\n",
    "                senator_name,\n",
    "                political_party,\n",
    "                pca_vector::FLOAT[{vector_len}] as pca_vector\n",
    "            FROM vectors_temp\n",
    "        \"\"\")\n",
    "\n",
    "    # Get 2D PCA coordinates from session_viz_data for visualization\n",
    "    session_2d_coords = {}\n",
    "    for session_num in ANALYSIS_SESSIONS:\n",
    "        if session_num in session_viz_data:\n",
    "            viz_df = session_viz_data[session_num]\n",
    "            for _, row in viz_df.iterrows():\n",
    "                key = (session_num, str(row[\"icpsr\"]))\n",
    "                session_2d_coords[key] = (row[\"PC1\"], row[\"PC2\"])\n",
    "\n",
    "    for session_num in tqdm(ANALYSIS_SESSIONS, desc=\"Computing Party Cohesion\"):\n",
    "        try:\n",
    "            # Calculate pairwise similarities within each party using DuckDB\n",
    "            for party in [\"D\", \"R\"]:\n",
    "                # Get all senators from this party in this session\n",
    "                party_senators = duckdb_conn.execute(\"\"\"\n",
    "                    SELECT\n",
    "                        icpsr,\n",
    "                        senator_name,\n",
    "                        political_party,\n",
    "                        pca_vector\n",
    "                    FROM senator_vectors\n",
    "                    WHERE session_num = ? AND political_party = ?\n",
    "                \"\"\", [session_num, party]).fetchdf()\n",
    "\n",
    "                if len(party_senators) < 2:\n",
    "                    continue\n",
    "\n",
    "                # Calculate mean similarity for each senator to their party peers\n",
    "                for idx, senator in party_senators.iterrows():\n",
    "                    target_icpsr = senator[\"icpsr\"]\n",
    "                    target_vector = senator[\"pca_vector\"]\n",
    "\n",
    "                    # Calculate similarity to all other party members (excluding self)\n",
    "                    similarities = duckdb_conn.execute(f\"\"\"\n",
    "                        SELECT array_cosine_similarity(CAST(? AS FLOAT[{vector_len}]), pca_vector) as similarity\n",
    "                        FROM senator_vectors\n",
    "                        WHERE session_num = ?\n",
    "                          AND political_party = ?\n",
    "                          AND icpsr != ?\n",
    "                    \"\"\", [target_vector, session_num, party, target_icpsr]).fetchdf()\n",
    "\n",
    "                    if not similarities.empty and len(similarities) > 0:\n",
    "                        mean_similarity = similarities[\"similarity\"].mean()\n",
    "\n",
    "                        # Get 2D coordinates for visualization from session_viz_data\n",
    "                        coord_key = (session_num, str(target_icpsr))\n",
    "                        pc1, pc2 = session_2d_coords.get(coord_key, (np.nan, np.nan))\n",
    "\n",
    "                        senator_similarity_records.append({\n",
    "                            \"session_num\": session_num,\n",
    "                            \"icpsr\": target_icpsr,\n",
    "                            \"senator_name\": senator[\"senator_name\"],\n",
    "                            \"political_party\": party,\n",
    "                            \"pc1\": pc1,\n",
    "                            \"pc2\": pc2,\n",
    "                            \"party_cohesion_score\": mean_similarity\n",
    "                        })\n",
    "\n",
    "            # Calculate aggregate statistics for this session\n",
    "            session_similarity_df = pd.DataFrame([\n",
    "                r for r in senator_similarity_records\n",
    "                if r[\"session_num\"] == session_num\n",
    "            ])\n",
    "\n",
    "            if not session_similarity_df.empty:\n",
    "                d_scores = session_similarity_df[\n",
    "                    session_similarity_df[\"political_party\"] == \"D\"\n",
    "                ][\"party_cohesion_score\"]\n",
    "                r_scores = session_similarity_df[\n",
    "                    session_similarity_df[\"political_party\"] == \"R\"\n",
    "                ][\"party_cohesion_score\"]\n",
    "\n",
    "                party_cohesion_stats.append({\n",
    "                    \"session_num\": session_num,\n",
    "                    \"d_mean_cohesion\": d_scores.mean() if not d_scores.empty else np.nan,\n",
    "                    \"d_median_cohesion\": d_scores.median() if not d_scores.empty else np.nan,\n",
    "                    \"d_std_cohesion\": d_scores.std() if not d_scores.empty else np.nan,\n",
    "                    \"d_count\": len(d_scores) if not d_scores.empty else 0,\n",
    "                    \"r_mean_cohesion\": r_scores.mean() if not r_scores.empty else np.nan,\n",
    "                    \"r_median_cohesion\": r_scores.median() if not r_scores.empty else np.nan,\n",
    "                    \"r_std_cohesion\": r_scores.std() if not r_scores.empty else np.nan,\n",
    "                    \"r_count\": len(r_scores) if not r_scores.empty else 0,\n",
    "                })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing session {session_num}: {e}\")\n",
    "            continue\n",
    "\n",
    "    senator_similarity_df = pd.DataFrame(senator_similarity_records)\n",
    "    cohesion_stats_df = pd.DataFrame(party_cohesion_stats)\n",
    "\n",
    "    return senator_similarity_df, cohesion_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8e1195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize global variables for Exercise 5\n",
    "# These will be populated by Visualization 1's PCA dimensionality control\n",
    "\n",
    "current_pca_dims = 20\n",
    "senator_similarity_df = pd.DataFrame()\n",
    "cohesion_stats_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e16ea86",
   "metadata": {},
   "source": [
    "### Exercise 5 Visualization 1: Intra-Party Cohesion Over Time (with PCA Dimensionality Control)\n",
    "This interactive visualization allows you to:\n",
    "1. **Adjust PCA dimensionality** (2D-50D) to explore how it affects similarity calculations\n",
    "2. **View intra-party cohesion trends over time** for Democrats and Republicans\n",
    "3. **Select session ranges** to focus on specific time periods\n",
    "**How to use:**\n",
    "- Adjust the \"PCA Dims\" slider to select dimensionality (2-50)\n",
    "- Click \"Recompute Cohesion\" to recalculate with new dimensionality\n",
    "- Use the \"Sessions\" slider to zoom into specific time periods\n",
    "- Observe how cohesion trends change with different PCA dimensions\n",
    "**Interpretation:**\n",
    "- Values closer to 1.0 indicate high intra-party cohesion (senators vote similarly)\n",
    "- Values closer to 0.0 indicate low intra-party cohesion (more diverse voting within party)\n",
    "- Higher PCA dimensions capture more variance → more accurate similarity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5762009d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import IntSlider, Button, HBox, VBox, Output, Label\n",
    "\n",
    "# Create PCA dimensionality control widgets\n",
    "pca_dim_slider = IntSlider(\n",
    "    value=20,\n",
    "    min=2,\n",
    "    max=50,\n",
    "    step=1,\n",
    "    description=\"PCA Dims:\",\n",
    "    continuous_update=False,\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "recompute_button = Button(\n",
    "    description=\"Recompute Cohesion\",\n",
    "    button_style='primary',\n",
    "    tooltip='Click to recompute PCA vectors and cohesion scores with selected dimensionality',\n",
    "    icon='refresh'\n",
    ")\n",
    "\n",
    "# Create output widgets\n",
    "recompute_status_output = Output()\n",
    "cohesion_plot_output = Output()\n",
    "\n",
    "# Create session range slider (will be populated after initial computation)\n",
    "cohesion_range_slider = IntRangeSlider(\n",
    "    value=(87, 119),\n",
    "    min=87,\n",
    "    max=119,\n",
    "    step=1,\n",
    "    description=\"Sessions:\",\n",
    "    continuous_update=False,\n",
    "    layout=Layout(width=\"70%\")\n",
    ")\n",
    "\n",
    "def render_cohesion_plot():\n",
    "    \"\"\"Render the cohesion time-series plot with current data\"\"\"\n",
    "    global cohesion_stats_df, current_pca_dims\n",
    "\n",
    "    with cohesion_plot_output:\n",
    "        cohesion_plot_output.clear_output(wait=True)\n",
    "\n",
    "        if cohesion_stats_df.empty:\n",
    "            print(\"No cohesion data available. Click 'Recompute Cohesion' to generate data.\")\n",
    "            return\n",
    "\n",
    "        # Get session window from slider\n",
    "        lower, upper = cohesion_range_slider.value\n",
    "        selected_sessions = [f\"{num:03d}\" for num in range(lower, upper + 1)]\n",
    "        subset_df = cohesion_stats_df[cohesion_stats_df[\"session_num\"].isin(selected_sessions)].copy()\n",
    "\n",
    "        if subset_df.empty:\n",
    "            print(\"No cohesion data available for the selected session window.\")\n",
    "            return\n",
    "\n",
    "        subset_df = subset_df.sort_values(\"session_num\")\n",
    "\n",
    "        # Map session numbers to dates\n",
    "        subset_df = subset_df.merge(congress_dates_df[[\"session_num\", \"start_year\"]], on=\"session_num\", how=\"left\")\n",
    "\n",
    "        fig_cohesion = go.Figure()\n",
    "\n",
    "        # Add Democrat cohesion line\n",
    "        fig_cohesion.add_trace(go.Scatter(\n",
    "            x=subset_df[\"start_year\"],\n",
    "            y=subset_df[\"d_mean_cohesion\"],\n",
    "            mode=\"lines+markers\",\n",
    "            name=\"Democrat Cohesion\",\n",
    "            line=dict(color=\"#1f77b4\", width=2),\n",
    "            marker=dict(size=6),\n",
    "            text=subset_df[\"session_num\"],\n",
    "            customdata=subset_df[[\"d_count\", \"d_std_cohesion\"]],\n",
    "            hovertemplate=\"<b>Session %{text}</b><br>Year: %{x}<br>Mean Cohesion: %{y:.4f}<br>Count: %{customdata[0]}<br>Std Dev: %{customdata[1]:.4f}<extra></extra>\"\n",
    "        ))\n",
    "\n",
    "        # Add Republican cohesion line\n",
    "        fig_cohesion.add_trace(go.Scatter(\n",
    "            x=subset_df[\"start_year\"],\n",
    "            y=subset_df[\"r_mean_cohesion\"],\n",
    "            mode=\"lines+markers\",\n",
    "            name=\"Republican Cohesion\",\n",
    "            line=dict(color=\"#d62728\", width=2),\n",
    "            marker=dict(size=6),\n",
    "            text=subset_df[\"session_num\"],\n",
    "            customdata=subset_df[[\"r_count\", \"r_std_cohesion\"]],\n",
    "            hovertemplate=\"<b>Session %{text}</b><br>Year: %{x}<br>Mean Cohesion: %{y:.4f}<br>Count: %{customdata[0]}<br>Std Dev: %{customdata[1]:.4f}<extra></extra>\"\n",
    "        ))\n",
    "\n",
    "        fig_cohesion.update_layout(\n",
    "            title=f\"Intra-Party Cohesion Over Time ({current_pca_dims}D PCA)<br><sub>Higher = More Similar Voting Within Party</sub>\",\n",
    "            xaxis_title=\"Session Start Year\",\n",
    "            yaxis_title=\"Mean Cosine Similarity (Party Cohesion)\",\n",
    "            hovermode=\"x unified\",\n",
    "            template=\"plotly_white\",\n",
    "            legend=dict(yanchor=\"top\", y=0.99, xanchor=\"left\", x=0.01)\n",
    "        )\n",
    "\n",
    "        fig_cohesion.show()\n",
    "\n",
    "def on_recompute_clicked(b):\n",
    "    \"\"\"Callback for recompute button\"\"\"\n",
    "    global current_pca_dims, senator_similarity_df, cohesion_stats_df\n",
    "\n",
    "    with recompute_status_output:\n",
    "        recompute_status_output.clear_output()\n",
    "\n",
    "        n_components = pca_dim_slider.value\n",
    "        current_pca_dims = n_components\n",
    "\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Recomputing with {n_components}-dimensional PCA...\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        # Compute PCA vectors\n",
    "        vectors_df = compute_pca_vectors_ndim(n_components)\n",
    "\n",
    "        if vectors_df.empty:\n",
    "            print(\"❌ No vectors computed. Check data availability.\")\n",
    "            return\n",
    "\n",
    "        print(f\"✅ Computed {len(vectors_df)} PCA vectors\")\n",
    "\n",
    "        # Recalculate cohesion scores\n",
    "        senator_similarity_df, cohesion_stats_df = recalculate_cohesion_scores(vectors_df)\n",
    "\n",
    "        print(f\"\\n=== Exercise 5 Summary ({n_components}D PCA) ===\")\n",
    "        print(f\"Senator similarity records: {len(senator_similarity_df)}\")\n",
    "        print(f\"Sessions with cohesion stats: {len(cohesion_stats_df)}\")\n",
    "\n",
    "        if not cohesion_stats_df.empty:\n",
    "            print(f\"\\nDemocrat mean cohesion range: {cohesion_stats_df['d_mean_cohesion'].min():.4f} - {cohesion_stats_df['d_mean_cohesion'].max():.4f}\")\n",
    "            print(f\"Republican mean cohesion range: {cohesion_stats_df['r_mean_cohesion'].min():.4f} - {cohesion_stats_df['r_mean_cohesion'].max():.4f}\")\n",
    "\n",
    "            # Update session range slider bounds\n",
    "            cohesion_session_ints = sorted([int(s) for s in cohesion_stats_df[\"session_num\"].unique()])\n",
    "            cohesion_range_slider.min = cohesion_session_ints[0]\n",
    "            cohesion_range_slider.max = cohesion_session_ints[-1]\n",
    "            cohesion_range_slider.value = (cohesion_session_ints[0], cohesion_session_ints[-1])\n",
    "\n",
    "        # Store in DuckDB\n",
    "        if not senator_similarity_df.empty:\n",
    "            duckdb_conn.execute(\"DROP TABLE IF EXISTS senator_similarity\")\n",
    "            duckdb_conn.execute(\"\"\"\n",
    "                CREATE TABLE senator_similarity (\n",
    "                    session_num VARCHAR,\n",
    "                    icpsr VARCHAR,\n",
    "                    senator_name VARCHAR,\n",
    "                    political_party VARCHAR,\n",
    "                    pc1 DOUBLE,\n",
    "                    pc2 DOUBLE,\n",
    "                    party_cohesion_score DOUBLE,\n",
    "                    PRIMARY KEY (session_num, icpsr)\n",
    "                )\n",
    "            \"\"\")\n",
    "            duckdb_conn.register(\"senator_similarity_temp\", senator_similarity_df)\n",
    "            duckdb_conn.execute(\"\"\"\n",
    "                INSERT INTO senator_similarity\n",
    "                SELECT * FROM senator_similarity_temp\n",
    "            \"\"\")\n",
    "            print(f\"\\n✅ Stored {len(senator_similarity_df)} senator similarity records in DuckDB.\")\n",
    "\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"✅ Recomputation complete! Plot updated below.\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "    # Update the plot with new data\n",
    "    render_cohesion_plot()\n",
    "\n",
    "def on_session_slider_change(change):\n",
    "    \"\"\"Callback for session range slider\"\"\"\n",
    "    render_cohesion_plot()\n",
    "\n",
    "# Wire up callbacks\n",
    "recompute_button.on_click(on_recompute_clicked)\n",
    "cohesion_range_slider.observe(on_session_slider_change, names='value')\n",
    "display(VBox([pca_dim_slider, recompute_button, recompute_status_output, cohesion_plot_output]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94739cb",
   "metadata": {},
   "source": [
    "### Exercise 5 Visualization 2: Interactive Senator Similarity Explorer\n",
    "This interactive tool allows you to:\n",
    "1. Select a congressional session\n",
    "2. Choose a specific senator\n",
    "3. View a scatter plot showing:\n",
    "   - The selected senator (highlighted)\n",
    "   - Other senators color-coded by similarity to the selected senator\n",
    "   - Party boundaries for context\n",
    "4. See a table of the top-k most similar senators\n",
    "**Use Cases:**\n",
    "- Identify senators who vote similarly across party lines\n",
    "- Find the most/least typical members of each party\n",
    "- Explore bipartisan coalitions or party outliers\n",
    "**Important Notes:**\n",
    "- **PCA Dimensionality**: Controlled in Visualization 1 above (currently using {current_pca_dims}D PCA)\n",
    "- **Similarity scores**: Computed using the N-dimensional PCA vectors for accuracy\n",
    "- **Scatter plot positions**: Use 2D PCA from Exercise 3 for visualization\n",
    "- This separation allows accurate similarity calculations while maintaining interpretable visualizations. \n",
    "\n",
    "\n",
    "*Tip:* Try different PCA dimensions in Visualization 1 to see how similarity scores change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c330152",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not senator_similarity_df.empty and session_viz_data:\n",
    "    print(\"\\n=== Interactive Senator Similarity Explorer ===\")\n",
    "    print(f\"Using {current_pca_dims}D PCA for similarity calculations (set in Visualization 1)\")\n",
    "    print(\"Select a session and senator to explore voting similarity patterns.\\n\")\n",
    "\n",
    "    # Get available sessions\n",
    "    available_sessions = sorted([int(s) for s in senator_similarity_df[\"session_num\"].unique()])\n",
    "\n",
    "    # Create session dropdown\n",
    "    session_dropdown = Dropdown(\n",
    "        options=[(f\"Session {s} ({congress_dates_df[congress_dates_df['session_num']==f'{s:03d}']['start_year'].iloc[0] if not congress_dates_df[congress_dates_df['session_num']==f'{s:03d}'].empty else s})\", s)\n",
    "                 for s in available_sessions],\n",
    "        value=available_sessions[-1] if available_sessions else None,\n",
    "        description=\"Session:\",\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "\n",
    "    # Create senator dropdown (will be populated based on session)\n",
    "    senator_dropdown = Dropdown(\n",
    "        options=[],\n",
    "        description=\"Senator:\",\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "\n",
    "    # Create output widget for plot\n",
    "    plot_output = Output()\n",
    "\n",
    "    # Create output widget for table\n",
    "    table_output = Output()\n",
    "\n",
    "    def update_senator_dropdown(change):\n",
    "        \"\"\"Update senator dropdown when session changes\"\"\"\n",
    "        session_num = f\"{change['new']:03d}\"\n",
    "\n",
    "        if session_num in session_viz_data:\n",
    "            session_data = session_viz_data[session_num]\n",
    "            # Get senators with their names\n",
    "            senators = session_data[[\"icpsr\", \"senator_name\", \"political_party\"]].drop_duplicates()\n",
    "            senators = senators.sort_values(\"senator_name\")\n",
    "\n",
    "            senator_dropdown.options = [\n",
    "                (f\"{row['senator_name']} ({row['political_party']})\", row['icpsr'])\n",
    "                for _, row in senators.iterrows()\n",
    "            ]\n",
    "\n",
    "            if len(senator_dropdown.options) > 0:\n",
    "                senator_dropdown.value = senator_dropdown.options[0][1]\n",
    "\n",
    "    def calculate_senator_similarities(session_num: str, target_icpsr: str):\n",
    "        \"\"\"Calculate similarity between target senator and all others in session using DuckDB\"\"\"\n",
    "        if session_num not in session_viz_data:\n",
    "            return None\n",
    "\n",
    "        session_data = session_viz_data[session_num].copy()\n",
    "\n",
    "        # Get target senator's vector from DuckDB\n",
    "        target_result = duckdb_conn.execute(\"\"\"\n",
    "            SELECT pca_vector\n",
    "            FROM senator_vectors\n",
    "            WHERE session_num = ? AND icpsr = ?\n",
    "        \"\"\", [session_num, target_icpsr]).fetchone()\n",
    "\n",
    "        if not target_result:\n",
    "            return None\n",
    "\n",
    "        target_vector = target_result[0]\n",
    "\n",
    "        # Determine vector length from the fetched target_vector\n",
    "        vector_len = len(target_vector) if target_vector is not None else None\n",
    "\n",
    "        # Calculate similarity to all senators in this session using DuckDB\n",
    "        similarities_df = duckdb_conn.execute(f\"\"\"\n",
    "            SELECT\n",
    "                icpsr,\n",
    "                array_cosine_similarity(CAST(? AS FLOAT[{vector_len}]), pca_vector) as similarity\n",
    "            FROM senator_vectors\n",
    "            WHERE session_num = ?\n",
    "        \"\"\", [target_vector, session_num]).fetchdf()\n",
    "\n",
    "        # Merge similarities with session data\n",
    "        session_data = session_data.merge(\n",
    "            similarities_df,\n",
    "            on=\"icpsr\",\n",
    "            how=\"left\"\n",
    "        )\n",
    "        session_data = session_data.rename(columns={\"similarity\": \"similarity_to_target\"})\n",
    "\n",
    "        return session_data\n",
    "\n",
    "    def render_similarity_plot(session_num: str, target_icpsr: str):\n",
    "        \"\"\"Render scatter plot with similarity color coding\"\"\"\n",
    "        session_data = calculate_senator_similarities(session_num, target_icpsr)\n",
    "\n",
    "        if session_data is None or session_data.empty:\n",
    "            print(f\"No data available for session {session_num}\")\n",
    "            return\n",
    "\n",
    "        # Get target senator info\n",
    "        target_row = session_data[session_data[\"icpsr\"] == target_icpsr].iloc[0]\n",
    "        target_name = target_row[\"senator_name\"]\n",
    "        target_party = target_row[\"political_party\"]\n",
    "\n",
    "        # Create figure\n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "        # Plot all senators with similarity color coding\n",
    "        scatter = ax.scatter(\n",
    "            session_data[\"PC1\"],\n",
    "            session_data[\"PC2\"],\n",
    "            c=session_data[\"similarity_to_target\"],\n",
    "            cmap=\"RdYlGn\",  # Red (low similarity) to Green (high similarity)\n",
    "            s=100,\n",
    "            alpha=0.6,\n",
    "            edgecolor=\"black\",\n",
    "            linewidth=0.5,\n",
    "            vmin=0,\n",
    "            vmax=1\n",
    "        )\n",
    "\n",
    "        # Highlight target senator\n",
    "        ax.scatter(\n",
    "            target_row[\"PC1\"],\n",
    "            target_row[\"PC2\"],\n",
    "            s=400,\n",
    "            c=\"gold\",\n",
    "            marker=\"*\",\n",
    "            edgecolor=\"black\",\n",
    "            linewidth=2,\n",
    "            zorder=10,\n",
    "            label=f\"Selected: {target_name}\"\n",
    "        )\n",
    "\n",
    "        # Add colorbar\n",
    "        cbar = plt.colorbar(scatter, ax=ax)\n",
    "        cbar.set_label(\"Cosine Similarity to Selected Senator\", rotation=270, labelpad=20)\n",
    "\n",
    "        # Get year range for title\n",
    "        year_info = congress_dates_df[congress_dates_df[\"session_num\"] == session_num]\n",
    "        year_str = \"\"\n",
    "        if not year_info.empty:\n",
    "            start_year = year_info.iloc[0][\"start_year\"]\n",
    "            end_year = year_info.iloc[0][\"end_year\"]\n",
    "            year_str = f\" ({start_year}-{end_year})\"\n",
    "\n",
    "        ax.set_title(f\"Senator Similarity Analysis — Session {session_num}{year_str} ({current_pca_dims}D PCA)\\nSelected: {target_name} ({target_party})\")\n",
    "        ax.set_xlabel(\"Principal Component 1 (2D projection for visualization)\")\n",
    "        ax.set_ylabel(\"Principal Component 2 (2D projection for visualization)\")\n",
    "        ax.axhline(0, color=\"#cccccc\", linewidth=0.5)\n",
    "        ax.axvline(0, color=\"#cccccc\", linewidth=0.5)\n",
    "        ax.set_aspect(\"equal\")\n",
    "        ax.legend(loc=\"best\", frameon=True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def render_similarity_table(session_num: str, target_icpsr: str, top_k: int = 10):\n",
    "        \"\"\"Render table of most similar senators using DuckDB\"\"\"\n",
    "        # Use DuckDB to efficiently get top-k most similar senators\n",
    "        top_similar_df = duckdb_conn.execute(\"\"\"\n",
    "            WITH target AS (\n",
    "                SELECT pca_vector\n",
    "                FROM senator_vectors\n",
    "                WHERE session_num = ? AND icpsr = ?\n",
    "            )\n",
    "            SELECT\n",
    "                sv.senator_name,\n",
    "                sv.political_party,\n",
    "                array_cosine_similarity((SELECT pca_vector FROM target), sv.pca_vector) as similarity_score\n",
    "            FROM senator_vectors sv\n",
    "            WHERE sv.session_num = ?\n",
    "              AND sv.icpsr != ?\n",
    "            ORDER BY similarity_score DESC\n",
    "            LIMIT ?\n",
    "        \"\"\", [session_num, target_icpsr, session_num, target_icpsr, top_k]).fetchdf()\n",
    "\n",
    "        if top_similar_df.empty:\n",
    "            print(f\"No similarity data available for session {session_num}\")\n",
    "            return\n",
    "\n",
    "        # Format for display\n",
    "        display_df = top_similar_df.copy()\n",
    "        display_df.columns = [\"Senator Name\", \"Party\", \"Similarity Score\"]\n",
    "        display_df = display_df.reset_index(drop=True)\n",
    "        display_df.index = display_df.index + 1  # Start index at 1\n",
    "\n",
    "        print(f\"\\nTop {top_k} Most Similar Senators:\")\n",
    "        if \"display\" in globals() and display is not None:\n",
    "            display(display_df)\n",
    "        else:\n",
    "            print(display_df.to_string())\n",
    "\n",
    "    def on_selection_change(change):\n",
    "        \"\"\"Handle changes to session or senator selection\"\"\"\n",
    "        with plot_output:\n",
    "            plot_output.clear_output(wait=True)\n",
    "            session_num = f\"{session_dropdown.value:03d}\"\n",
    "            target_icpsr = senator_dropdown.value\n",
    "\n",
    "            if target_icpsr:\n",
    "                render_similarity_plot(session_num, target_icpsr)\n",
    "\n",
    "        with table_output:\n",
    "            table_output.clear_output(wait=True)\n",
    "            session_num = f\"{session_dropdown.value:03d}\"\n",
    "            target_icpsr = senator_dropdown.value\n",
    "\n",
    "            if target_icpsr:\n",
    "                render_similarity_table(session_num, target_icpsr, top_k=15)\n",
    "\n",
    "    # Connect event handlers\n",
    "    session_dropdown.observe(update_senator_dropdown, names='value')\n",
    "    session_dropdown.observe(on_selection_change, names='value')\n",
    "    senator_dropdown.observe(on_selection_change, names='value')\n",
    "\n",
    "    # Initialize senator dropdown\n",
    "    update_senator_dropdown({'new': session_dropdown.value})\n",
    "\n",
    "    # Create UI layout\n",
    "    controls = HBox([session_dropdown, senator_dropdown])\n",
    "    ui = VBox([controls, plot_output, table_output])\n",
    "\n",
    "    # Display UI\n",
    "    display(ui)\n",
    "\n",
    "    # Trigger initial render\n",
    "    on_selection_change(None)\n",
    "else:\n",
    "    print(\"No senator similarity data available for interactive explorer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3881304",
   "metadata": {},
   "source": [
    "## Exercise 6: Summary Visualization Dashboard\n",
    "This exercise presents a 2x2 grid of key polarization metrics from Exercises 2-5, providing a comprehensive overview of Senate polarization trends.\n",
    "**Dashboard Layout:**\n",
    "- **Top-Left**: Silhouette Score with Significant Shifts (Exercise 2)\n",
    "- **Top-Right**: Party-Cluster Alignment Mismatch (Exercise 3)\n",
    "- **Bottom-Left**: Combined Normalized Polarization Metrics (Exercise 4)\n",
    "- **Bottom-Right**: Intra-Party Cohesion Over Time (Exercise 5)\n",
    "\n",
    "We should note that we present the interactive visualizations in the exercises above to satisfy\n",
    "the requirement for \"a couple more visualizations that represent some measurement that could be used as an indicator of polarization.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c08377",
   "metadata": {},
   "source": [
    "Create a grid for your visualizations (silhouette and party-cluster similarity). Add a couple more of visualizations that represent some measurement that could be used as an indicator of polarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ab1da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "# Prepare data for all four plots\n",
    "print(\"\\n=== Creating Exercise 6 Summary Dashboard ===\")\n",
    "\n",
    "# Check data availability\n",
    "has_silhouette = not silhouette_df.empty if 'silhouette_df' in globals() else False\n",
    "has_alignment = not alignment_df.empty if 'alignment_df' in globals() else False\n",
    "has_combined = not combined_metrics_df.empty if 'combined_metrics_df' in globals() else False\n",
    "has_cohesion = not cohesion_stats_df.empty if 'cohesion_stats_df' in globals() else False\n",
    "\n",
    "if not all([has_silhouette, has_alignment, has_combined, has_cohesion]):\n",
    "    print(\"⚠️ Warning: Some data is missing. Dashboard may be incomplete.\")\n",
    "    print(f\"  Silhouette data: {'✓' if has_silhouette else '✗'}\")\n",
    "    print(f\"  Alignment data: {'✓' if has_alignment else '✗'}\")\n",
    "    print(f\"  Combined metrics: {'✓' if has_combined else '✗'}\")\n",
    "    print(f\"  Cohesion data: {'✓' if has_cohesion else '✗'}\")\n",
    "\n",
    "# Create figure with 2x2 grid\n",
    "fig = plt.figure(figsize=(20, 12))\n",
    "gs = GridSpec(2, 2, figure=fig, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Plot 1: Silhouette Score with Significant Shifts (Top-Left)\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "if has_silhouette:\n",
    "    # Merge with congress dates for x-axis\n",
    "    plot_df = silhouette_df.merge(congress_dates_df[[\"session_num\", \"start_year\"]], on=\"session_num\", how=\"left\")\n",
    "    plot_df = plot_df.sort_values(\"start_year\")\n",
    "\n",
    "    # Calculate rolling average and shifts\n",
    "    plot_df['rolling_avg'] = plot_df['silhouette_score'].rolling(window=4, center=True, min_periods=1).mean()\n",
    "    plot_df['delta'] = plot_df['silhouette_score'] - plot_df['rolling_avg']\n",
    "    delta_std = plot_df['delta'].std()\n",
    "    plot_df['significant_shift'] = plot_df['delta'].abs() > (1.0 * delta_std)\n",
    "\n",
    "    # Plot\n",
    "    ax1.plot(plot_df['start_year'], plot_df['silhouette_score'], 'o-', color='#1f77b4', linewidth=2, markersize=4, label='Silhouette Score')\n",
    "    ax1.plot(plot_df['start_year'], plot_df['rolling_avg'], '--', color='gray', linewidth=1.5, label='Rolling Avg (4 sessions)')\n",
    "\n",
    "    # Highlight significant shifts\n",
    "    shift_df = plot_df[plot_df['significant_shift']]\n",
    "    if not shift_df.empty:\n",
    "        ax1.scatter(shift_df['start_year'], shift_df['silhouette_score'],\n",
    "                   color='red', s=100, marker='o', facecolors='none', linewidths=2,\n",
    "                   label='Significant Shift', zorder=5)\n",
    "\n",
    "    ax1.set_xlabel('Session Start Year', fontsize=11)\n",
    "    ax1.set_ylabel('Silhouette Score', fontsize=11)\n",
    "    ax1.set_title('Exercise 2: Silhouette Score with Significant Shifts\\n(Higher = Better Cluster Separation)', fontsize=12, fontweight='bold')\n",
    "    ax1.legend(loc='best', fontsize=9)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "else:\n",
    "    ax1.text(0.5, 0.5, 'Silhouette data not available', ha='center', va='center', transform=ax1.transAxes)\n",
    "    ax1.set_title('Exercise 2: Silhouette Score', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Plot 2: Party-Cluster Alignment Mismatch (Top-Right)\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "if has_alignment:\n",
    "    plot_df = alignment_df.merge(congress_dates_df[[\"session_num\", \"start_year\"]], on=\"session_num\", how=\"left\")\n",
    "    plot_df = plot_df.sort_values(\"start_year\")\n",
    "\n",
    "    ax2.plot(plot_df['start_year'], plot_df['mismatch_pct'], 'o-', color='#ff7f0e', linewidth=2, markersize=4)\n",
    "    ax2.axhline(y=50, color='red', linestyle='--', linewidth=1, alpha=0.5, label='50% (Random)')\n",
    "    ax2.set_xlabel('Session Start Year', fontsize=11)\n",
    "    ax2.set_ylabel('Mismatch Percentage (%)', fontsize=11)\n",
    "    ax2.set_title('Exercise 3: Party-Cluster Alignment Mismatch\\n(Lower = Higher Polarization)', fontsize=12, fontweight='bold')\n",
    "    ax2.legend(loc='best', fontsize=9)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "else:\n",
    "    ax2.text(0.5, 0.5, 'Alignment data not available', ha='center', va='center', transform=ax2.transAxes)\n",
    "    ax2.set_title('Exercise 3: Party-Cluster Alignment', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Plot 3: Combined Normalized Metrics (Bottom-Left)\n",
    "ax3 = fig.add_subplot(gs[1, 0])\n",
    "if has_combined:\n",
    "    plot_df = combined_metrics_df.merge(congress_dates_df[[\"session_num\", \"start_year\"]], on=\"session_num\", how=\"left\")\n",
    "    plot_df = plot_df.sort_values(\"start_year\")\n",
    "\n",
    "    ax3.plot(plot_df['start_year'], plot_df['silhouette_norm'], 'o-', label='Silhouette', linewidth=2, markersize=3)\n",
    "    ax3.plot(plot_df['start_year'], plot_df['dunn_norm'], 's-', label='Dunn Index', linewidth=2, markersize=3)\n",
    "    ax3.plot(plot_df['start_year'], plot_df['db_norm'], '^-', label='Davies-Bouldin', linewidth=2, markersize=3)\n",
    "    ax3.plot(plot_df['start_year'], plot_df['ch_norm'], 'd-', label='Calinski-Harabasz', linewidth=2, markersize=3)\n",
    "    ax3.plot(plot_df['start_year'], plot_df['crosstab_norm'], 'v-', label='Crosstab Sep.', linewidth=2, markersize=3)\n",
    "\n",
    "    ax3.set_xlabel('Session Start Year', fontsize=11)\n",
    "    ax3.set_ylabel('Normalized Score (0-1)', fontsize=11)\n",
    "    ax3.set_title('Exercise 4: Combined Normalized Polarization Metrics\\n(All Scaled to 0-1 Range)', fontsize=12, fontweight='bold')\n",
    "    ax3.legend(loc='best', fontsize=8, ncol=2)\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "else:\n",
    "    ax3.text(0.5, 0.5, 'Combined metrics not available', ha='center', va='center', transform=ax3.transAxes)\n",
    "    ax3.set_title('Exercise 4: Combined Metrics', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Plot 4: Intra-Party Cohesion (Bottom-Right)\n",
    "ax4 = fig.add_subplot(gs[1, 1])\n",
    "if has_cohesion:\n",
    "    plot_df = cohesion_stats_df.merge(congress_dates_df[[\"session_num\", \"start_year\"]], on=\"session_num\", how=\"left\")\n",
    "    plot_df = plot_df.sort_values(\"start_year\")\n",
    "\n",
    "    ax4.plot(plot_df['start_year'], plot_df['d_mean_cohesion'], 'o-', color='#1f77b4', linewidth=2, markersize=4, label='Democrat Cohesion')\n",
    "    ax4.plot(plot_df['start_year'], plot_df['r_mean_cohesion'], 's-', color='#d62728', linewidth=2, markersize=4, label='Republican Cohesion')\n",
    "\n",
    "    ax4.set_xlabel('Session Start Year', fontsize=11)\n",
    "    ax4.set_ylabel('Mean Cosine Similarity', fontsize=11)\n",
    "    ax4.set_title(f'Exercise 5: Intra-Party Cohesion Over Time ({current_pca_dims}D PCA)\\n(Higher = More Similar Voting Within Party)', fontsize=12, fontweight='bold')\n",
    "    ax4.legend(loc='best', fontsize=9)\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "else:\n",
    "    ax4.text(0.5, 0.5, 'Cohesion data not available', ha='center', va='center', transform=ax4.transAxes)\n",
    "    ax4.set_title('Exercise 5: Intra-Party Cohesion', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Add overall title\n",
    "# get beginning and end sessions from ANALYSIS_SESSIONS\n",
    "start_sess, end_sess = min(ANALYSIS_SESSIONS), max(ANALYSIS_SESSIONS)\n",
    "fig.suptitle(f\"Senate Polarization Analysis Dashboard ({start_sess}th-{end_sess}th Congress)\",\n",
    "             fontsize=16, fontweight='bold', y=0.995)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✅ Exercise 6 dashboard created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc31770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete all raw .csv files in senate_dataset dir so project dir is ready for compression and submission\n",
    "import os\n",
    "import glob\n",
    "raw_data_path = settings.votes_dir\n",
    "# remove with glob pattern\n",
    "raw_files = glob.glob(os.path.join(raw_data_path, \"*.csv\"))\n",
    "for file in raw_files:\n",
    "    os.remove(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02afd4e4",
   "metadata": {},
   "source": [
    "## References\n",
    "### Implementation Planning\n",
    "- **Google Gemini Conversation** (Implementation Plan Draft): https://gemini.google.com/share/ead9be9470dd\n",
    "### Code Development Tools\n",
    "- **Github Copilot Agent**: https://code.visualstudio.com/docs/copilot/copilot-coding-agent\n",
    "- **Augment Code Agent**: https://docs.augmentcode.com/using-augment/agent\n",
    "\n",
    "These AI coding assistants helped speed up code writing, debugging, and documentation. \n",
    "These were leveraged particularly for the interactive plotting features and DuckDB ingestion.\n",
    "\n",
    "### Data Sources\n",
    "- **VoteView Data Portal**: https://voteview.com/data\n",
    "- **Senate Vote CSVs (Pattern)**: https://voteview.com/static/data/out/votes/S<NUM>_votes.csv\n",
    "- **All Members Metadata**: https://voteview.com/static/data/out/members/HSall_members.csv\n",
    "### DuckDB Documentation & Resources\n",
    "- **Multiple CSV Files Overview**: https://duckdb.org/docs/stable/data/multiple_files/overview#csv\n",
    "- **Python CSV Ingestion**: https://duckdb.org/docs/stable/clients/python/data_img/data_ingestion#csv-files\n",
    "- **CSV Reading Tips**: https://duckdb.org/docs/stable/data/csv/tips\n",
    "- **Handling Faulty CSVs**: https://duckdb.org/docs/stable/data/csv/reading_faulty_csv_files\n",
    "- **DuckDB Tricks (Blog)**: https://duckdb.org/2024/08/19/duckdb-tricks-part-1\n",
    "- **Taming Wild CSVs (Blog)**: https://motherduck.com/blog/taming-wild-csvs-with-duckdb-data-engineering/\n",
    "- **Persisting CSVs (Blog)**: https://motherduck.com/blog/csv-files-persist-duckdb-solution/\n",
    "- **Video: Taming Wild CSVs**: https://www.youtube.com/watch?v=pHeVP92O9zc\n",
    "- **DataFrame Glossary**: https://motherduck.com/glossary/DataFrame/\n",
    "- **Array Processing**: https://duckdb.org/docs/stable/sql/functions/array#array_cosine_similarityarray1-array2\n",
    "- **Vector Similarity Search**: https://duckdb.org/2024/05/03/vector-similarity-search-vss\n",
    "- **DuckDB VSS Extension**: https://github.com/duckdb/duckdb-vss\n",
    "- **MotherDuck Vector Search Blog**: https://motherduck.com/blog/search-using-duckdb-part-1/\n",
    "- **HuggingFace DuckDB Vector Similarity**: https://huggingface.co/docs/hub/en/datasets-duckdb-vector-similarity-search\n",
    "### Python Libraries & Techniques\n",
    "- **defaultdict Tutorial**: https://realpython.com/python-defaultdict/\n",
    "- **Handling KeyError**: https://www.datacamp.com/tutorial/python-keyerror\n",
    "- **Dunn Index Implementation Repo**: https://github.com/jqmviegas/jqm_cvi/tree/master\n",
    "### Polarization Metrics\n",
    "**Dunn Index:**\n",
    "- GeeksforGeeks: https://www.geeksforgeeks.org/machine-learning/dunn-index-and-db-index-cluster-validity-indices-set-1/\n",
    "- Theory PDF: https://github.com/jqmviegas/jqm_cvi/blob/master/theory.pdf\n",
    "- Wikipedia: https://en.wikipedia.org/wiki/Dunn_index\n",
    "\n",
    "**Davies-Bouldin Index:**\n",
    "- GeeksforGeeks: https://www.geeksforgeeks.org/machine-learning/davies-bouldin-index/\n",
    "- Wikipedia: https://en.wikipedia.org/wiki/Davies%E2%80%93Bouldin_index\n",
    "- scikit-learn Docs: https://scikit-learn.org/stable/modules/clustering.html#davies-bouldin-index\n",
    "- scikit-learn API: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.davies_bouldin_score.html\n",
    "\n",
    "**Calinski-Harabasz Index:**\n",
    "- Towards Data Science: https://towardsdatascience.com/calinski-harabasz-index-for-k-means-clustering-evaluation-using-python-4fefeeb2988e/\n",
    "- Wikipedia: https://en.wikipedia.org/wiki/Calinski%E2%80%93Harabasz_index\n",
    "- scikit-learn Docs: https://scikit-learn.org/stable/modules/clustering.html#calinski-harabasz-index\n",
    "- scikit-learn API: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.calinski_harabasz_score.html\n",
    "\n",
    "### Plotly & Dash\n",
    "- **Plotly Getting Started**: https://plotly.com/python/getting-started/\n",
    "- **Plotly v3 Notebook Example**: https://plotly.com/python/v3/ipython-notebooks/baltimore-vital-signs/\n",
    "- **GeeksforGeeks Plotly Tutorial**: https://www.geeksforgeeks.org/data-visualization/using-plotly-for-interactive-data-visualization-in-python/\n",
    "- **Dash Tutorial**: https://dash.plotly.com/tutorial\n",
    "- **Dash in Jupyter**: https://dash.plotly.com/dash-in-jupyter\n",
    "- **Jupyter Support Update (GitHub)**: https://github.com/plotly/jupyter-dash?tab=readme-ov-file#notice-as-of-dash-v211-jupyter-support-is-built-into-the-main-dash-package\n",
    "- **Plotly Figure Structure**: https://plotly.com/python/figure-structure/\n",
    "- **Creating & Updating Figures**: https://plotly.com/python/creating-and-updating-figures/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effd209a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project1_polarization (3.14.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
