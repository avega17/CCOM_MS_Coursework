{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " PV Segmentation with PyTorch Lightning & segmentation-models-pytorch\n",
    "\n",
    "This notebook provides our initial approach for training and evaluating deep learning models baselines for solar photovoltaic (PV) panel segmentation from satellite imagery. It leverages the power and flexibility of `PyTorch Lightning` for streamlined training and `segmentation-models-pytorch` (SMP) for easy access to a wide variety of cutting-edge model architectures.\n",
    "\n",
    "**Key Goals:**\n",
    "1.  *Data Preparation:* Set up a PyTorch `Lightning DataModule` to efficiently load and preprocess image patches and their corresponding masks.\n",
    "2.  *Model Definition:* Define a reusable `PyTorch LightningModule` that can accommodate various segmentation architectures from SMP. \n",
    "3.  *Training:* Execute training loops, leveraging PyTorch Lightning's features for hardware acceleration (including Apple Silicon *MPS*), logging, and checkpointing with wandb.\n",
    "4.  *Evaluation:* Assess model performance using relevant segmentation metrics and visualize predictions, ground truth, and sample from lower-res STAC imagery using `cubo`.\n",
    "\n",
    "**Assumptions:**\n",
    "*   Image patches and their corresponding binary masks are assumed to be pre-prepared and stored in specified directories.\n",
    "*   The notebook is designed to be adaptable for different datasets, such as those derived from Maxar imagery with YOLO labels converted to masks, or other datasets providing pixel-coordinate labels.\n",
    "\n",
    "This notebook will demonstrate the advantages of using PyTorch Lightning for organizing code and simplifying complex training workflows, and how `segmentation-models-pytorch` allows for rapid experimentation with different model backbones and architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Research Problem & Motivation\n",
    "\n",
    "### The Challenge of PV Segmentation\n",
    "\n",
    "As the world transitions to renewable energy sources, solar photovoltaic (PV) installations are growing exponentially worldwide. Accurate mapping and monitoring of these installations is crucial for energy planning, grid management, carbon accounting, and sustainable development. However, traditional methods of tracking PV installations rely on incomplete permit data, manual surveys, or voluntary reporting—all of which present significant gaps in coverage and accuracy.\n",
    "\n",
    "Satellite imagery offers a promising solution for automated detection of PV installations at regional and global scales. Yet, several challenges make this a non-trivial computer vision problem:\n",
    "\n",
    "1. **Multi-scale challenge**: PV installations vary dramatically in size, from small residential rooftop panels (a few m²) to utility-scale solar farms (several km²)\n",
    "2. **Visual variability**: PV panels appear differently depending on panel type, orientation, age, viewing angle, and illumination conditions\n",
    "3. **Resolution trade-offs**: As demonstrated in Clark et al.'s study (2023), detection performance is strongly affected by image resolution, creating a compromise between coverage area and detection accuracy\n",
    "4. **Class imbalance**: PV installations typically occupy a small fraction of any given geographic area, creating extreme class imbalance in training data\n",
    "5. **Data scarcity**: High-quality labeled datasets for training are limited and geographically biased toward certain regions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Research Questions\n",
    "\n",
    "This project addresses the following key questions:\n",
    "\n",
    "1. **How can we leverage state-of-the-art deep learning architectures to improve PV segmentation accuracy across diverse geographic regions including those with sparse data?**\n",
    "\n",
    "2. **Can PyTorch Lightning's framework enable more efficient experimentation across multiple model architectures to identify optimal approaches for this domain-specific problem?**\n",
    "\n",
    "3. **What combination of data augmentation strategies, model architectures, loss functions, and training approaches best addresses the unique challenges of PV segmentation?**\n",
    "\n",
    "4. **How can we optimize models to work effectively across different spatial resolutions while maximizing the area that can be covered in operational settings?**\n",
    "\n",
    "### Global Significance\n",
    "\n",
    "Accurate mapping of solar PV installations has far-reaching implications:\n",
    "\n",
    "- **Energy transition monitoring**: Tracking actual deployment rates of solar PV against climate targets\n",
    "- **Grid integration**: Supporting power system planning by precisely locating distributed energy resources\n",
    "- **Environmental impact assessment**: Understanding land use changes and habitat effects of renewable energy development\n",
    "- **Socioeconomic analysis**: Studying adoption patterns across different communities to inform equitable energy transition policies\n",
    "- **Sustainable Development Goals**: Contributing directly to SDG 7 (Affordable and Clean Energy) and SDG 13 (Climate Action)\n",
    "\n",
    "In their seminal work with very high resolution (VHR) satellite imagery *(< 1 meter/pixel*), Cecilia Clark and Fabio Pacifici (2023) demonstrated that resolution significantly impacts detection performance, with our employer, Maxar Intelligence's, [\"HD Technology\" product](https://blog.maxar.com/tech-and-tradecraft/2022/maxars-hd-technology-provides-measurable-improvements-in-machine-learning-applications) (proprietary upscaling algorithm capable of simulating 15.5cm GSD) delivering substantially better results than native resolution (31cm) imagery. This **resolution-performance trade-off** informs our approach to developing models that can work effectively across varying image resolutions. They summarize the challenges succinctly below: \n",
    "\n",
    "**\"Residential solar panels are considered small, weak targets even in VHR satellite imagery due to the average number of pixels per object, variation among  \n",
    "objects, and complex context**. *Existing satellite imagery datasets often include large-scale, or non-residential, solar panel annotations* due to resolution  \n",
    "of the imagery and therefore ability to detect small objects. There are available datasets of VHR imagery to support accurate detection\n",
    "of small-scale and residential installations, but **the imagery is generally sourced from aerial platforms.**\" \n",
    "\n",
    "By developing improved segmentation techniques, this research contributes to the broader goal of creating comprehensive, accurate, and timely inventories of global PV installations—a critical capability for managing the ongoing energy transition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure style=\"text-align: center\">\n",
    "<img src=\"report/assets/figures/Munich_2021-06-18_WV03_HD_16x9.jpg\" style=\"width:70%; height:auto;\">\n",
    "<figcaption align = \"center\"> 31cm native resolution vs simulated \"15.5\"cm spatial resolution </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Power of PyTorch Lightning\n",
    "\n",
    "PyTorch Lightning is a lightweight PyTorch wrapper that significantly simplifies the process of training deep learning models. It provides a structured framework that abstracts away much of the boilerplate code typically associated with PyTorch training loops, allowing researchers and developers to focus more on the model architecture and data.\n",
    "\n",
    "**Key Advantages of PyTorch Lightning:**\n",
    "\n",
    "*   **Reduced Boilerplate:** Lightning handles the engineering aspects of training, such as the training loop, validation loop, and test loop. This means you write less code for common tasks.\n",
    "*   **Organized Code:** It promotes a clean and organized code structure through its core components: `LightningModule` and `LightningDataModule`.\n",
    "    *   The `LightningModule` encapsulates all model-related code (architecture, optimizers, training steps, validation steps, etc.).\n",
    "    *   The `LightningDataModule` handles all data-related operations (data loading, transformations, splitting, batching).\n",
    "*   **Simplified Training & Iteration:** With the boilerplate handled, iterating on different model architectures or hyperparameters becomes much faster and more straightforward.\n",
    "*   **Hardware Agnostic:** Lightning makes it easy to train models on CPUs, GPUs (single or multiple), and TPUs with minimal code changes. You can specify the `accelerator` (e.g., *\"gpu\"*, **\"mps\"**, *\"tpu\"*, \"cpu\") and `devices` (e.g., number of GPUs) directly in the `Trainer`.\n",
    "*   **Scalability:** It seamlessly supports distributed training (multi-GPU, multi-node) and mixed-precision training (`precision='16-mixed'`), which are crucial for training large models or large datasets.\n",
    "*   **Reproducibility:** By organizing code and managing training details, Lightning helps in creating more reproducible experiments.\n",
    "*   **Callbacks & Loggers:** It has a rich ecosystem of callbacks (for checkpointing, early stopping, learning rate monitoring, etc.) and loggers (TensorBoard, CSVLogger, etc.) that integrate easily into the training process.\n",
    "\n",
    "As highlighted in the [PyTorch Lightning tutorial by DataCamp](https://www.datacamp.com/tutorial/pytorch-lightning-tutorial), its broad yet useful abstractions allow for quick training and iteration on multiple model architectures and facilitate scaling to multi-GPU or cloud environments. This notebook will leverage these features to efficiently train our PV segmentation models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure style=\"text-align: center\">\n",
    "<img src=\"report/assets/figures/xkcd_python.png\" style=\"width:60%; height:auto;\">\n",
    "<figcaption align = \"center\"> Illustration of what modern Python DL workflows can feel like</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% --- 2. Configuration ---\n",
    "\n",
    "# --- Data Parameters ---\n",
    "# These paths point to pre-prepared image patches and their corresponding masks\n",
    "# For initial testing with datasets like Maxar's (Clark et al.) or Jiang et al.,\n",
    "# ensure you have converted their pixel-coordinate labels into raster mask images.\n",
    "MASK_DIR = Path('data/maxar_sample_masks_native/') # <<< --- UPDATE (e.g., where you save generated masks)\n",
    "IMAGE_PATCH_DIR = Path('data/maxar_sample_chips_native/') # <<< --- UPDATE (e.g., where Maxar image chips are)\n",
    "# LABEL_FILE_PATH is not directly used by DataModule if image/mask paths are directly globbed,\n",
    "# but can be useful for cross-referencing or generating file lists.\n",
    "# LABEL_FILE_PATH = 'path/to/your/pv_labels.gpkg'\n",
    "\n",
    "PATCH_SIZE_PIXELS = 256 # Should match your prepared image/mask chip size\n",
    "NUM_WORKERS = os.cpu_count() // 2 if os.cpu_count() else 0 # For DataLoaders\n",
    "\n",
    "# --- Model & Training Hyperparameters (Common) ---\n",
    "IN_CHANNELS = 3 # RGB. Change to 4 if using RGB+NIR\n",
    "NUM_CLASSES = 1 # Binary segmentation (PV vs background)\n",
    "TARGET_ACTIVATION = 'sigmoid' # Output activation for the model\n",
    "\n",
    "LEARNING_RATE = 1e-4\n",
    "BATCH_SIZE = 8 # Adjust based on GPU/MPS memory\n",
    "NUM_EPOCHS = 5 # Start with very few epochs for initial weekend testing\n",
    "VAL_SPLIT_RATIO = 0.2\n",
    "\n",
    "# --- MPS/GPU Configuration ---\n",
    "# Check for MPS availability (Apple Silicon GPU)\n",
    "if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    ACCELERATOR = \"mps\"\n",
    "    DEVICES = 1\n",
    "    PRECISION_TRAINER = \"32\" # MPS generally prefers 32-bit for stability, though 16-mixed might work for some ops\n",
    "    print(\"MPS (Apple Silicon GPU) backend is available and will be used.\")\n",
    "elif torch.cuda.is_available():\n",
    "    ACCELERATOR = \"gpu\"\n",
    "    DEVICES = 1 # Or specify number of GPUs [0, 1] or \"auto\"\n",
    "    PRECISION_TRAINER = '16-mixed'\n",
    "    print(\"CUDA GPU is available and will be used.\")\n",
    "else:\n",
    "    ACCELERATOR = \"cpu\"\n",
    "    DEVICES = 1\n",
    "    PRECISION_TRAINER = \"32\"\n",
    "    print(\"No GPU or MPS found. CPU will be used (training will be slow).\")\n",
    "\n",
    "\n",
    "# --- Define Architectures and Encoders to Test ---\n",
    "# Prioritizing lighter versions for initial testing\n",
    "MODEL_CONFIGURATIONS = {\n",
    "    \"UnetPlusPlus_ResNet18\": {\"arch\": \"UnetPlusPlus\", \"encoder\": \"resnet18\"},\n",
    "    \"DeepLabV3Plus_MobileNetV2\": {\"arch\": \"DeepLabV3Plus\", \"encoder\": \"mobilenet_v2\"},\n",
    "    \"FPN_EfficientNetB0\": {\"arch\": \"FPN\", \"encoder\": \"efficientnet-b0\"},\n",
    "    \"MAnet_ResNet18\": {\"arch\": \"MAnet\", \"encoder\": \"resnet18\"},\n",
    "    # DPT and Segformer are generally heavier, can be uncommented later\n",
    "    # \"DPT_Hybrid_Tiny\": {\"arch\": \"DPT\", \"encoder\": \"vit_tiny_patch16_224\"}, # Check SMP for exact ViT encoder names for DPT\n",
    "    # \"Segformer_MiTB0\": {\"arch\": \"Segformer\", \"encoder\": \"mit_b0\"},\n",
    "}\n",
    "ENCODER_WEIGHTS = 'imagenet'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Composability of the PyTorch Ecosystem for Data Handling\n",
    "\n",
    "The PyTorch ecosystem offers a highly composable and flexible set of tools for data loading and preprocessing, which are essential for any deep learning pipeline. Key components include `Dataset`, `DataLoader`, and various transformation libraries.\n",
    "\n",
    "**Core Components:**\n",
    "\n",
    "*   **`torch.utils.data.Dataset`:** This is an abstract class representing a dataset. To create a custom dataset, you typically inherit from `Dataset` and override two methods:\n",
    "    *   `__len__(self)`: Should return the size of the dataset.\n",
    "    *   `__getitem__(self, idx)`: Should return the sample (e.g., an image and its corresponding mask) at the given index `idx`. This is where you load and preprocess individual data points.\n",
    "\n",
    "*   **`torch.utils.data.DataLoader`:** This utility wraps an iterable around the `Dataset` to enable easy access to the samples. It handles many crucial aspects of data loading efficiently:\n",
    "    *   **Batching:** Groups multiple samples into batches.\n",
    "    *   **Shuffling:** Randomly shuffles the data at every epoch to prevent model bias.\n",
    "    *   **Parallel Loading:** Uses multiple worker processes (`num_workers`) to load data in parallel, which can significantly speed up training by preventing the GPU from waiting for data.\n",
    "    *   **Memory Pinning (`pin_memory`):** When using GPUs, setting `pin_memory=True` can speed up data transfer from CPU to GPU memory.\n",
    "\n",
    "*   **Transformation Libraries (e.g., `torchvision.transforms`, `albumentations`):**\n",
    "    *   **`torchvision.transforms`:** Provides common image transformations (resizing, cropping, normalization, conversion to tensor, etc.). These are often composed together using `transforms.Compose`.\n",
    "    *   **`albumentations`:** A powerful library specifically designed for image augmentation. It offers a wide variety of augmentations (flips, rotations, color adjustments, noise, blurs, etc.) and is highly optimized for performance. It integrates well with PyTorch and other frameworks.\n",
    "\n",
    "**Composability in Action:**\n",
    "\n",
    "These components are designed to work together seamlessly. A typical workflow involves:\n",
    "1.  Creating a custom `Dataset` class to load and apply initial transformations to individual image-mask pairs.\n",
    "2.  Wrapping this `Dataset` instance with a `DataLoader` to manage batching, shuffling, and parallel loading.\n",
    "3.  The `DataLoader` then provides an iterator that yields batches of data (images and masks) ready to be fed into the model during training or evaluation.\n",
    "\n",
    "This modular approach, as generally seen in the PyTorch world and highlighted in guides like the [PyTorch Segmentation Models practical guide](https://medium.com/@heyamit10/pytorch-segmentation-models-a-practical-guide-5bf973a32e30), makes the data pipeline flexible, maintainable, and efficient. In this notebook, we use `PVSegmentationDataset` (a custom `Dataset`) and `PVSegmentationDataModule` (which internally uses `DataLoader` and `transforms`) to manage our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PV Segmentation with Torch Lightning & SMP\n",
    "\n",
    "This notebook provides a framework for training and evaluating deep learning models for solar photovoltaic (PV) panel segmentation from satellite imagery. It leverages the power and flexibility of PyTorch Lightning for streamlined training and `segmentation-models-pytorch` (SMP) for easy access to a wide variety of cutting-edge model architectures.\n",
    "\n",
    "This notebook will demonstrate the benefits of using PyTorch Lightning for organizing code and simplifying complex training workflows, and leveraging `segmentation-models-pytorch` (SMP) allows for rapid experimentation with different model backbones and architectures.\n",
    "\n",
    "## Datasets Overview\n",
    "\n",
    "The training and evaluation of PV segmentation models rely on diverse, publicly available datasets. Many of these datasets are located in [Zenodo](https://zenodo.org/), a general-purpose open-access repository developed under the European OpenAIRE program and operated by CERN. Others are hosted in figshare, a web-based platform for sharing research data and other types of content. The rest are hosted in GitHub repositories or other open-access data platforms.\n",
    "\n",
    "The dataset labels are available in a variety of formats, including CSV, GeoJSON, GeoPackage, ESRI shapefiles, raw raster masks, and GeoParquet. For this notebook, we assume that these datasets have been preprocessed into image patches and corresponding *raster* segmentation masks.\n",
    "\n",
    "Here is a list of some prominent Solar Panel dataset publications, their first authors, DOI links, and approximate number of labels, which can be sources for preparing data for this notebook:\n",
    "\n",
    "-   **\"Distributed solar photovoltaic array location and extent dataset for remote sensing object identification\"** - K. Bradbury, 2016 | [paper DOI](https://doi.org/10.1038/sdata.2016.106) | [dataset DOI](https://doi.org/10.6084/m9.figshare.3385780.v4) | polygon annotations for 19,433 PV modules in 4 cities in California, USA\n",
    "-   **\"A solar panel dataset of very high resolution satellite imagery to support the Sustainable Development Goals\"** - C. Clark et al, 2023 | [paper DOI](https://doi.org/10.1038/s41597-023-02539-8) | [dataset DOI](https://doi.org/10.6084/m9.figshare.22081091.v3) | 2,542 object labels (per spatial resolution)\n",
    "-   \"A harmonised, high-coverage, open dataset of solar photovoltaic installations in the UK\" - D. Stowell et al, 2020** | [paper DOI](https://doi.org/10.1038/s41597-020-00739-0) | [dataset DOI](https://zenodo.org/records/4059881) | 265,418 data points (over 255,000 are stand-alone installations, 1067 solar farms, and rest are subcomponents within solar farms)\n",
    "-   \"Georectified polygon database of ground-mounted large-scale solar photovoltaic sites in the United States\" - K. Sydny, 2023 | [paper DOI](https://doi.org/10.1038/s41597-023-02644-8) | [dataset DOI](https://www.sciencebase.gov/catalog/item/6671c479d34e84915adb7536) | 4186 data points\n",
    "-   \"Vectorized solar photovoltaic installation dataset across China in 2015 and 2020\" - J. Liu et al, 2024 | [paper DOI](https://doi.org/10.1038/s41597-024-04356-z) | [dataset link](https://github.com/qingfengxitu/ChinaPV) | 3,356 PV labels (inspect quality!)\n",
    "-   *\"Multi-resolution dataset for photovoltaic panel segmentation from satellite and aerial imagery\"* - H. Jiang, 2021 | [paper DOI](https://doi.org/10.5194/essd-13-5389-2021) | [dataset DOI](https://doi.org/10.5281/zenodo.5171712) | 3,716 samples of PV data points\n",
    "- **\"A crowdsourced dataset of aerial images with annotated solar photovoltaic arrays and installation metadata\"** - G. Kasmi, 2023 | [paper DOI](https://doi.org/10.1038/s41597-023-01951-4) | [dataset DOI](https://doi.org/10.5281/zenodo.6865878) | > 28K points of PV installations; 13K+ segmentation masks for PV arrays; metadata for 8K+ installations\n",
    "-   \"An Artificial Intelligence Dataset for Solar Energy Locations in India\" - A. Ortiz, 2022 | [paper DOI](https://doi.org/10.1038/s41597-022-01499-9) | [dataset link 1](https://researchlabwuopendata.blob.core.windows.net/solar-farms/solar_farms_india_2021.geojson) or [dataset link 2](https://raw.githubusercontent.com/microsoft/solar-farms-mapping/refs/heads/main/data/solar_farms_india_2021_merged_simplified.geojson) | 117 geo-referenced points of solar installations across India\n",
    "- **\"GloSoFarID: Global multispectral dataset for Solar Farm IDentification in satellite imagery\"** - Z. Yang, 2024** | [paper DOI](https://doi.org/10.48550/arXiv.2404.05180) | [dataset DOI](https://github.com/yzyly1992/GloSoFarID/tree/main/data_coordinates) | 6,793 PV samples across 3 years (double counting of samples)\n",
    "-   \"A global inventory of photovoltaic solar energy generating units\" - L. Kruitwagen et al, 2021 | [paper DOI](https://doi.org/10.1038/s41586-021-03957-7) | [dataset DOI](https://doi.org/10.5281/zenodo.5005867) | 50,426 for training, cross-validation, and testing; 68,661 predicted polygon labels\n",
    "-   \"Harmonised global datasets of wind and solar farm locations and power\" - S. Dunnett et al, 2020 | [paper DOI](https://doi.org/10.1038/s41597-020-0469-8) | [dataset DOI](https://doi.org/10.6084/m9.figshare.11310269.v6) | 35272 PV installations\n",
    "\n",
    "**Key Goals for this Notebook:**\n",
    "1.  **Data Preparation:** Set up a PyTorch Lightning DataModule to efficiently load and preprocess image patches and their corresponding masks derived from the datasets listed above (or similar).\n",
    "2.  **Model Definition:** Define a reusable PyTorch LightningModule that can accommodate various segmentation architectures from `segmentation-models-pytorch`.\n",
    "3.  **Training:** Execute training loops, leveraging PyTorch Lightning's features for hardware acceleration (including Apple Silicon MPS), logging, and checkpointing.\n",
    "4.  **Evaluation:** Assess model performance using relevant metrics and visualize predictions.\n",
    "\n",
    "**Assumptions for this Notebook:**\n",
    "*   Image patches and their corresponding binary masks are assumed to be pre-prepared (e.g., using the `fetch-pv-datasets-ESDA.ipynb` notebook or similar methods) and stored in specified directories.\n",
    "*   The notebook is designed to be adaptable for different datasets, such as those derived from Maxar imagery with YOLO labels converted to masks, or other datasets providing pixel-coordinate labels that have been rasterized to masks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# %% --- 1. Setup & Imports ---\n",
    "# Standard libraries\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "# Geospatial libraries (primarily for data prep, less so for core training loop if data is pre-processed)\n",
    "# import geopandas as gpd # Keep if your label_gpkg_path is used to derive file lists\n",
    "import xarray as xr\n",
    "import rasterio # For reading image patches\n",
    "# import pystac_client\n",
    "# from shapely.geometry import Point, box\n",
    "\n",
    "# ML/DL libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "import segmentation_models_pytorch as smp\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger, CSVLogger\n",
    "from torchmetrics import MetricCollection\n",
    "from torchmetrics.classification import BinaryJaccardIndex, BinaryAccuracy, BinaryPrecision, BinaryRecall, BinaryF1Score\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Suppress specific warnings ---\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"rasterio\")\n",
    "warnings.filterwarnings(\"ignore\", message=\".* Shapely GEOS version .*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% --- 3. PyTorch Lightning DataModule ---\n",
    "\n",
    "class PVSegmentationDataset(Dataset):\n",
    "    def __init__(self, image_paths, mask_paths, transform=None, mask_transform=None,\n",
    "                 target_size=(PATCH_SIZE_PIXELS, PATCH_SIZE_PIXELS), in_channels=IN_CHANNELS):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.transform = transform\n",
    "        self.mask_transform = mask_transform\n",
    "        self.target_size = target_size\n",
    "        self.in_channels = in_channels\n",
    "        assert len(self.image_paths) == len(self.mask_paths), \"Mismatch between number of images and masks\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        mask_path = self.mask_paths[idx]\n",
    "\n",
    "        try:\n",
    "            # Load image using PIL, then convert to numpy for consistency if needed by rasterio-like processing\n",
    "            # Or directly use PIL if images are standard formats like PNG/JPG\n",
    "            # If images are GeoTIFFs:\n",
    "            if str(img_path).lower().endswith(('.tif', '.tiff')):\n",
    "                with rasterio.open(img_path) as src:\n",
    "                    # Read specified channels\n",
    "                    num_bands_to_read = self.in_channels\n",
    "                    # Ensure we don't try to read more bands than available\n",
    "                    if src.count < num_bands_to_read:\n",
    "                        print(f\"Warning: Image {img_path.name} has {src.count} bands, but {num_bands_to_read} were requested. Reading available bands.\")\n",
    "                        num_bands_to_read = src.count\n",
    "\n",
    "                    image_data = src.read(list(range(1, num_bands_to_read + 1))) # Bands are 1-indexed\n",
    "\n",
    "                    # If fewer channels read than expected, pad with zeros (or handle differently)\n",
    "                    if image_data.shape[0] < self.in_channels:\n",
    "                        padding = np.zeros((self.in_channels - image_data.shape[0], src.height, src.width), dtype=image_data.dtype)\n",
    "                        image_data = np.concatenate((image_data, padding), axis=0)\n",
    "\n",
    "                    # Convert to HWC for PIL\n",
    "                    image = np.moveaxis(image_data, 0, -1).astype(np.uint8) # Assuming 8-bit after scaling\n",
    "                    image_pil = Image.fromarray(image)\n",
    "\n",
    "\n",
    "            else: # Assume PNG, JPG etc.\n",
    "                image_pil = Image.open(img_path)\n",
    "                if self.in_channels == 3 and image_pil.mode != 'RGB':\n",
    "                    image_pil = image_pil.convert('RGB')\n",
    "                elif self.in_channels == 4 and image_pil.mode != 'RGBA': # Example for RGBA\n",
    "                    image_pil = image_pil.convert('RGBA')\n",
    "                elif self.in_channels == 1 and image_pil.mode != 'L':\n",
    "                    image_pil = image_pil.convert('L')\n",
    "\n",
    "\n",
    "            if image_pil.size != self.target_size:\n",
    "                image_pil = image_pil.resize(self.target_size, Image.BILINEAR)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}. Returning zeros.\")\n",
    "            # Create zero image with correct number of channels\n",
    "            zero_data = np.zeros((self.target_size[1], self.target_size[0], self.in_channels), dtype=np.uint8)\n",
    "            image_pil = Image.fromarray(zero_data)\n",
    "\n",
    "\n",
    "        try: # Load Mask\n",
    "            mask = Image.open(mask_path).convert('L') # Grayscale\n",
    "            if mask.size != self.target_size:\n",
    "                mask = mask.resize(self.target_size, Image.NEAREST) # Use NEAREST for masks\n",
    "            mask_np = np.array(mask)\n",
    "            mask_np = (mask_np > 0).astype(np.float32) # Ensure binary 0 or 1\n",
    "            # mask_np = np.expand_dims(mask_np, axis=-1) # H, W, C (C=1) # Not needed if ToTensor adds channel\n",
    "            mask_pil = Image.fromarray(mask_np, mode='F') # Mode 'F' for float32\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading mask {mask_path}: {e}. Returning zeros.\")\n",
    "            mask_pil = Image.fromarray(np.zeros(self.target_size, dtype=np.float32), mode='F')\n",
    "\n",
    "\n",
    "        if self.transform: image_tensor = self.transform(image_pil)\n",
    "        else: image_tensor = transforms.ToTensor()(image_pil)\n",
    "\n",
    "        if self.mask_transform: mask_tensor = self.mask_transform(mask_pil)\n",
    "        else: mask_tensor = transforms.ToTensor()(mask_pil) # ToTensor on (H,W) PIL gives (1,H,W)\n",
    "\n",
    "        return image_tensor, mask_tensor\n",
    "\n",
    "\n",
    "class PVSegmentationDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, image_dir: str, mask_dir: str,\n",
    "                 batch_size: int = 32, num_workers: int = 0,\n",
    "                 val_split_ratio: float = 0.2, seed: int = 42,\n",
    "                 patch_size: int = PATCH_SIZE_PIXELS, in_channels: int = IN_CHANNELS): # Use global defaults\n",
    "        super().__init__()\n",
    "        self.image_dir = Path(image_dir)\n",
    "        self.mask_dir = Path(mask_dir)\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.val_split_ratio = val_split_ratio\n",
    "        self.seed = seed\n",
    "        self.patch_size = (patch_size, patch_size) # Target size as tuple\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        self.imagenet_mean = [0.485, 0.456, 0.406]\n",
    "        self.imagenet_std = [0.229, 0.224, 0.225]\n",
    "        if self.in_channels == 4:\n",
    "            self.imagenet_mean.append(0.406) # Placeholder for NIR mean\n",
    "            self.imagenet_std.append(0.225)  # Placeholder for NIR std\n",
    "        elif self.in_channels == 1: # For grayscale\n",
    "            self.imagenet_mean = [0.449] # Approx ImageNet grayscale mean\n",
    "            self.imagenet_std = [0.226]  # Approx ImageNet grayscale std\n",
    "\n",
    "\n",
    "        self.train_transform = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomVerticalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=self.imagenet_mean, std=self.imagenet_std)\n",
    "        ])\n",
    "        self.val_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=self.imagenet_mean, std=self.imagenet_std)\n",
    "        ])\n",
    "        self.mask_transform = transforms.Compose([\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def prepare_data(self):\n",
    "        if not self.image_dir.exists() or not self.mask_dir.exists():\n",
    "            raise FileNotFoundError(f\"Image dir ({self.image_dir}) or Mask dir ({self.mask_dir}) not found.\")\n",
    "        print(f\"Data located in {self.image_dir} and {self.mask_dir}\")\n",
    "\n",
    "    def setup(self, stage: str = None):\n",
    "        img_extensions = ['*.tif', '*.png', '*.jpg', '*.jpeg']\n",
    "        all_image_paths = []\n",
    "        for ext in img_extensions:\n",
    "            all_image_paths.extend(list(self.image_dir.glob(ext)))\n",
    "        all_image_paths = sorted([p for p in all_image_paths if p.is_file() and p.stat().st_size > 100]) # Basic check\n",
    "\n",
    "        all_mask_paths_final = []\n",
    "        valid_image_paths_final = []\n",
    "        missing_masks_count = 0\n",
    "\n",
    "        for img_path in all_image_paths:\n",
    "            mask_found = False\n",
    "            for mask_ext in ['.png', '.tif', '.jpg', '.jpeg']: # Check common mask extensions\n",
    "                potential_mask_path = self.mask_dir / (img_path.stem + mask_ext)\n",
    "                if potential_mask_path.exists() and potential_mask_path.is_file() and potential_mask_path.stat().st_size > 0:\n",
    "                    all_mask_paths_final.append(potential_mask_path)\n",
    "                    valid_image_paths_final.append(img_path)\n",
    "                    mask_found = True\n",
    "                    break\n",
    "            if not mask_found:\n",
    "                missing_masks_count += 1\n",
    "\n",
    "        if missing_masks_count > 0:\n",
    "             print(f\"Warning: Skipped {missing_masks_count} images due to missing or invalid masks.\")\n",
    "        if not valid_image_paths_final:\n",
    "            raise ValueError(\"No valid image/mask pairs found. Check data directories and file names/extensions.\")\n",
    "\n",
    "        dataset_size = len(valid_image_paths_final)\n",
    "        val_size = int(dataset_size * self.val_split_ratio)\n",
    "        train_size = dataset_size - val_size\n",
    "\n",
    "        indices = list(range(dataset_size))\n",
    "        if train_size > 0 and val_size > 0 :\n",
    "            train_indices, val_indices = random_split(indices, [train_size, val_size],\n",
    "                                                  generator=torch.Generator().manual_seed(self.seed))\n",
    "        elif train_size > 0 : # Use all for training if val_size is 0\n",
    "            print(\"Warning: Validation split resulted in 0 validation samples. Using all data for training.\")\n",
    "            train_indices = indices\n",
    "            val_indices = []\n",
    "        else:\n",
    "            raise ValueError(f\"Train size is {train_size} and val size is {val_size}. Cannot create datasets.\")\n",
    "\n",
    "\n",
    "        train_img_p = [valid_image_paths_final[i] for i in train_indices]\n",
    "        train_msk_p = [all_mask_paths_final[i] for i in train_indices]\n",
    "        val_img_p = [valid_image_paths_final[i] for i in val_indices] if val_indices else []\n",
    "        val_msk_p = [all_mask_paths_final[i] for i in val_indices] if val_indices else []\n",
    "\n",
    "\n",
    "        if stage == 'fit' or stage is None:\n",
    "            self.train_dataset = PVSegmentationDataset(train_img_p, train_msk_p,\n",
    "                                                       transform=self.train_transform,\n",
    "                                                       mask_transform=self.mask_transform,\n",
    "                                                       target_size=self.patch_size, in_channels=self.in_channels)\n",
    "            if val_img_p: # Only create val_dataset if there are validation samples\n",
    "                self.val_dataset = PVSegmentationDataset(val_img_p, val_msk_p,\n",
    "                                                        transform=self.val_transform,\n",
    "                                                        mask_transform=self.mask_transform,\n",
    "                                                        target_size=self.patch_size, in_channels=self.in_channels)\n",
    "                print(f\"Setup complete. Train: {len(self.train_dataset)}, Val: {len(self.val_dataset)}\")\n",
    "            else:\n",
    "                self.val_dataset = None # Explicitly set to None\n",
    "                print(f\"Setup complete. Train: {len(self.train_dataset)}, Val: No validation set.\")\n",
    "\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        if hasattr(self, 'train_dataset') and self.train_dataset:\n",
    "            return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True,\n",
    "                              num_workers=self.num_workers, pin_memory=(ACCELERATOR != \"cpu\"), persistent_workers=self.num_workers > 0)\n",
    "        return None\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        if hasattr(self, 'val_dataset') and self.val_dataset:\n",
    "            return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False,\n",
    "                              num_workers=self.num_workers, pin_memory=(ACCELERATOR != \"cpu\"), persistent_workers=self.num_workers > 0)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `segmentation-models-pytorch` (SMP): A Rich Toolkit for Segmentation\n",
    "\n",
    "`segmentation-models-pytorch` (SMP) is a Python library built on PyTorch that provides a high-level API for image segmentation tasks. It simplifies the implementation of various state-of-the-art segmentation architectures and allows for easy integration of pre-trained encoders.\n",
    "\n",
    "**Key Features of SMP:**\n",
    "\n",
    "*   **Variety of Architectures:** SMP offers a collection of popular and effective segmentation architectures, including:\n",
    "    *   Unet\n",
    "    *   Unet++\n",
    "    *   MAnet\n",
    "    *   Linknet\n",
    "    *   FPN (Feature Pyramid Network)\n",
    "    *   PSPNet (Pyramid Scene Parsing Network)\n",
    "    *   DeepLabV3 / DeepLabV3+\n",
    "    *   PAN (Pyramid Attention Network)\n",
    "    *   DPT (Dense Prediction Transformer)\n",
    "    *   SegFormer\n",
    "*   **Pre-trained Encoders:** One of the most powerful features of SMP is its seamless integration with a vast number of pre-trained encoders (backbones). This is largely facilitated by its use of libraries like `timm` (PyTorch Image Models by Ross Wightman).\n",
    "    *   This allows you to use encoders like ResNets (resnet18, resnet34, resnet50, etc.), EfficientNets (efficientnet-b0 to b7), MobileNets, ViTs (Vision Transformers), and many others, often with weights pre-trained on ImageNet.\n",
    "    *   Using pre-trained encoders can significantly speed up convergence and improve performance, especially when working with limited datasets.\n",
    "*   **Ease of Use:** Creating a segmentation model is typically a one-liner: `smp.Unet(encoder_name='resnet34', encoder_weights='imagenet', in_channels=3, classes=1)`.\n",
    "*   **Flexibility:** You can easily switch between different architectures and encoders to experiment and find the best combination for your specific task.\n",
    "*   **Loss Functions and Metrics:** SMP also includes common loss functions (e.g., DiceLoss, JaccardLoss, FocalLoss) and metrics relevant to segmentation.\n",
    "\n",
    "As mentioned in the [PyTorch Segmentation Models practical guide](https://medium.com/@heyamit10/pytorch-segmentation-models-a-practical-guide-5bf973a32e30), SMP's strength lies in providing ready-to-use segmentation models with a wide choice of decoders and a huge variety of pre-trained encoders from `timm`. This composability allows for rapid prototyping and benchmarking of different approaches.\n",
    "\n",
    "In this notebook, we define a `PVSegmentationTask` (a `LightningModule`) that utilizes `smp.create_model` to dynamically build segmentation models based on the configurations specified, allowing us to easily test different architectures and encoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% --- 4. PyTorch Lightning Module ---\n",
    "# PVSegmentationTask class definition (largely the same as previous version)\n",
    "class PVSegmentationTask(pl.LightningModule):\n",
    "    def __init__(self, model_arch: str, encoder_name: str, encoder_weights: str,\n",
    "                 in_channels: int, num_classes: int, activation: str,\n",
    "                 learning_rate: float = 1e-4, loss_weights: tuple = (0.5, 0.5)):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.model = smp.create_model(\n",
    "            arch=self.hparams.model_arch,\n",
    "            encoder_name=self.hparams.encoder_name,\n",
    "            encoder_weights=self.hparams.encoder_weights,\n",
    "            in_channels=self.hparams.in_channels,\n",
    "            classes=self.hparams.num_classes,\n",
    "            activation=self.hparams.activation\n",
    "        )\n",
    "        self.dice_loss = DiceLoss(mode='binary', from_logits=(self.hparams.activation is None))\n",
    "        self.bce_loss = SoftBCEWithLogitsLoss()\n",
    "\n",
    "        metrics_args = {\"task\": \"binary\", \"threshold\": 0.5} # num_classes not needed for binary task with single output\n",
    "        metrics_collection = MetricCollection({\n",
    "            'iou': BinaryJaccardIndex(**metrics_args), 'f1': BinaryF1Score(**metrics_args),\n",
    "            'accuracy': BinaryAccuracy(**metrics_args), 'precision': BinaryPrecision(**metrics_args),\n",
    "            'recall': BinaryRecall(**metrics_args),\n",
    "        })\n",
    "        self.train_metrics = metrics_collection.clone(prefix='train_')\n",
    "        self.val_metrics = metrics_collection.clone(prefix='val_')\n",
    "\n",
    "    def forward(self, x): return self.model(x)\n",
    "\n",
    "    def _calculate_loss(self, y_pred, y_true):\n",
    "        y_true = y_true.float()\n",
    "        if self.hparams.activation is None:\n",
    "             bce = self.bce_loss(y_pred, y_true); dice = self.dice_loss(y_pred, y_true)\n",
    "        else:\n",
    "             epsilon = 1e-7; y_pred_clamped = torch.clamp(y_pred, epsilon, 1.0 - epsilon)\n",
    "             logits = torch.log(y_pred_clamped / (1.0 - y_pred_clamped)); bce = self.bce_loss(logits, y_true)\n",
    "             dice = self.dice_loss(y_pred, y_true)\n",
    "        return self.hparams.loss_weights[0] * bce + self.hparams.loss_weights[1] * dice\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y_true = batch; y_pred = self(x); loss = self._calculate_loss(y_pred, y_true)\n",
    "        self.train_metrics.update(y_pred, y_true.int())\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        self.log_dict(self.train_metrics.compute(), logger=True); self.train_metrics.reset()\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y_true = batch; y_pred = self(x); loss = self._calculate_loss(y_pred, y_true)\n",
    "        self.val_metrics.update(y_pred, y_true.int())\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        if not self.trainer.sanity_checking: # Skip logging during sanity check\n",
    "             metrics = self.val_metrics.compute()\n",
    "             self.log_dict(metrics, logger=True)\n",
    "             self.val_metrics.reset()\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=3, verbose=True)\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": {\"scheduler\": scheduler, \"monitor\": \"val_iou\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streamlined Training with PyTorch Lightning Trainer\n",
    "\n",
    "Once the `LightningDataModule` (for data handling) and the `LightningModule` (for the model, optimizers, and training/validation logic) are defined, PyTorch Lightning makes the actual training process remarkably straightforward using its `Trainer` class.\n",
    "\n",
    "**The `Trainer` Class:**\n",
    "\n",
    "The `Trainer` automates most of the training loop, including:\n",
    "*   Iterating over epochs and batches.\n",
    "*   Calling the appropriate methods in your `LightningModule` (e.g., `training_step`, `validation_step`).\n",
    "*   Performing optimizer steps and learning rate scheduler adjustments.\n",
    "*   Moving data to the correct device (CPU/GPU/TPU).\n",
    "*   Handling distributed training and mixed-precision if configured.\n",
    "\n",
    "**Key `Trainer` Arguments Used in this Notebook:**\n",
    "\n",
    "*   `accelerator`: Specifies the hardware to use (e.g., \"mps\", \"gpu\", \"cpu\").\n",
    "*   `devices`: Specifies the number of devices or specific device IDs.\n",
    "*   `max_epochs`: The maximum number of epochs to train for.\n",
    "*   `logger`: Accepts one or more logger instances (e.g., `TensorBoardLogger`, `CSVLogger`) to record metrics and hyperparameters.\n",
    "*   `callbacks`: A list of callback objects that can customize the training behavior at various points. Common callbacks include:\n",
    "    *   `ModelCheckpoint`: Saves the model periodically, often based on a monitored metric (e.g., best validation IoU).\n",
    "    *   `LearningRateMonitor`: Logs the learning rate at each epoch or step.\n",
    "    *   `EarlyStopping`: Stops training if a monitored metric stops improving for a certain number of epochs (patience).\n",
    "*   `precision`: Configures training precision (e.g., \"32\" for full precision, \"16-mixed\" for mixed-precision training).\n",
    "\n",
    "**Initiating Training:**\n",
    "\n",
    "Training is typically started with a single line: `trainer.fit(model, datamodule=data_module)`.\n",
    "\n",
    "PyTorch Lightning's `Trainer` abstracts away the complexities of the training loop, allowing you to focus on the core components of your deep learning model and experiment more rapidly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "\n",
    "# %% --- 5. Training Execution ---\n",
    "data_module = PVSegmentationDataModule(\n",
    "    image_dir=str(IMAGE_PATCH_DIR), mask_dir=str(MASK_DIR),\n",
    "    batch_size=BATCH_SIZE, num_workers=NUM_WORKERS,\n",
    "    val_split_ratio=VAL_SPLIT_RATIO, patch_size=PATCH_SIZE_PIXELS, in_channels=IN_CHANNELS\n",
    ")\n",
    "try:\n",
    "    data_module.prepare_data()\n",
    "    data_module.setup(stage='fit')\n",
    "\n",
    "    # Visualize a sample batch from DataModule\n",
    "    train_dl = data_module.train_dataloader()\n",
    "    if train_dl and len(train_dl) > 0 :\n",
    "        print(\"Visualizing a sample batch from DataModule's train_dataloader...\")\n",
    "        images, masks = next(iter(train_dl))\n",
    "        def show_dm_batch(image_tensor, mask_tensor, num_samples=4, mean_val=None, std_val=None, in_channels=IN_CHANNELS):\n",
    "            images_np = image_tensor[:num_samples].cpu().numpy()\n",
    "            masks_np = mask_tensor[:num_samples].cpu().numpy()\n",
    "            fig, axes = plt.subplots(num_samples, 2, figsize=(8, num_samples * 4))\n",
    "            if num_samples == 1: axes = np.array([axes])\n",
    "            for i in range(num_samples):\n",
    "                img = images_np[i].transpose(1, 2, 0)\n",
    "                if mean_val and std_val:\n",
    "                    m, s = np.array(mean_val), np.array(std_val)\n",
    "                    img = s * img + m\n",
    "                img = np.clip(img, 0, 1)\n",
    "                if img.shape[-1] == 1: img = img.squeeze(-1) # For grayscale display\n",
    "\n",
    "                mask = masks_np[i].squeeze()\n",
    "                axes[i, 0].imshow(img, cmap='gray' if img.ndim==2 else None); axes[i, 0].set_title(\"Image\"); axes[i, 0].axis('off')\n",
    "                axes[i, 1].imshow(mask, cmap='gray'); axes[i, 1].set_title(\"Mask\"); axes[i, 1].axis('off')\n",
    "            plt.tight_layout(); plt.show()\n",
    "        show_dm_batch(images, masks, mean_val=data_module.imagenet_mean, std_val=data_module.imagenet_std)\n",
    "    else:\n",
    "        print(\"Train dataloader from DataModule is empty or not available.\")\n",
    "        data_module = None # Prevent training if data is not loaded\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error setting up DataModule or visualizing batch: {e}\")\n",
    "    data_module = None\n",
    "\n",
    "if data_module:\n",
    "    for config_name, params in MODEL_CONFIGURATIONS.items():\n",
    "        print(f\"\\n--- Training Model: {config_name} ---\")\n",
    "        lightning_model = PVSegmentationTask(\n",
    "            model_arch=params['arch'], encoder_name=params['encoder'], encoder_weights=ENCODER_WEIGHTS,\n",
    "            in_channels=IN_CHANNELS, num_classes=NUM_CLASSES, activation=TARGET_ACTIVATION,\n",
    "            learning_rate=LEARNING_RATE\n",
    "        )\n",
    "        tb_logger = TensorBoardLogger(\"tb_logs\", name=config_name)\n",
    "        csv_logger = CSVLogger(\"csv_logs\", name=config_name)\n",
    "        checkpoint_cb = pl.callbacks.ModelCheckpoint(\n",
    "            dirpath=f\"checkpoints/{config_name}\", filename=\"{epoch}-{val_iou:.4f}\",\n",
    "            monitor=\"val_iou\", mode=\"max\", save_top_k=1,\n",
    "        )\n",
    "        lr_monitor_cb = pl.callbacks.LearningRateMonitor(logging_interval='epoch')\n",
    "        early_stop_cb = pl.callbacks.EarlyStopping(monitor=\"val_iou\", patience=5, verbose=True, mode=\"max\")\n",
    "\n",
    "        trainer = pl.Trainer(\n",
    "            accelerator=ACCELERATOR, devices=DEVICES, max_epochs=NUM_EPOCHS,\n",
    "            logger=[tb_logger, csv_logger], callbacks=[checkpoint_cb, lr_monitor_cb, early_stop_cb],\n",
    "            precision=PRECISION_TRAINER,\n",
    "            # strategy=\"ddp_find_unused_parameters_true\" if ACCELERATOR==\"gpu\" and DEVICES > 1 else \"auto\"\n",
    "        )\n",
    "        try:\n",
    "            print(f\"Starting training for {config_name} with {ACCELERATOR}...\")\n",
    "            trainer.fit(lightning_model, datamodule=data_module)\n",
    "            print(f\"Training finished for {config_name}.\")\n",
    "            if checkpoint_cb.best_model_path:\n",
    "                print(f\"Best model for {config_name} saved at: {checkpoint_cb.best_model_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during training of {config_name}: {e}\")\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "else:\n",
    "    print(\"DataModule not initialized. Skipping training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation and Prediction Visualization\n",
    "\n",
    "After training, it's crucial to evaluate the model's performance on unseen data (typically the validation or a separate test set) and visualize its predictions to gain qualitative insights. This section demonstrates a basic approach to:\n",
    "\n",
    "1.  **Loading the Best Model:** PyTorch Lightning's `ModelCheckpoint` callback saves the best performing model based on a monitored metric. We load this checkpoint for evaluation.\n",
    "2.  **Making Predictions:** The loaded model is used to make predictions on a batch of data from the validation set.\n",
    "3.  **Visualizing Results:** The original images, ground truth masks, and the model's predicted masks are displayed side-by-side for comparison.\n",
    "\n",
    "This allows for a visual assessment of how well the model is segmenting the PV panels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% --- 6. Evaluation & Visualization (Example) ---\n",
    "if data_module and MODEL_CONFIGURATIONS:\n",
    "    first_config_name = list(MODEL_CONFIGURATIONS.keys())[0]\n",
    "    # Determine device for evaluation (mps, cuda, or cpu)\n",
    "    if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "        DEVICE_EVAL = torch.device(\"mps\")\n",
    "    elif torch.cuda.is_available():\n",
    "        DEVICE_EVAL = torch.device(\"cuda\")\n",
    "    else:\n",
    "        DEVICE_EVAL = torch.device(\"cpu\")\n",
    "    print(f\"Using device for evaluation: {DEVICE_EVAL}\")\n",
    "\n",
    "    checkpoint_dir = Path(f\"checkpoints/{first_config_name}/\")\n",
    "    if checkpoint_dir.exists():\n",
    "        ckpt_files = sorted(list(checkpoint_dir.glob(\"*.ckpt\")), key=os.path.getmtime, reverse=True)\n",
    "        if ckpt_files:\n",
    "            best_model_path_eval = ckpt_files[0] # Load the most recently saved best model\n",
    "            print(f\"\\n--- Evaluating and Visualizing: {first_config_name} from {best_model_path_eval} ---\")\n",
    "            try:\n",
    "                eval_model = PVSegmentationTask.load_from_checkpoint(best_model_path_eval, map_location=DEVICE_EVAL)\n",
    "                eval_model.to(DEVICE_EVAL) # Ensure model is on the correct device\n",
    "                eval_model.eval()\n",
    "                val_loader_eval = data_module.val_dataloader()\n",
    "                if val_loader_eval and len(val_loader_eval) > 0:\n",
    "                    images_eval, masks_gt_eval = next(iter(val_loader_eval))\n",
    "                    images_eval = images_eval.to(DEVICE_EVAL) # Move images to device\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        masks_pred_eval = eval_model(images_eval).cpu() # Move predictions to CPU for numpy/PIL\n",
    "\n",
    "                    unnorm_transform = transforms.Compose([\n",
    "                        transforms.Normalize(mean=[0.]*IN_CHANNELS, std=[1/s for s in data_module.imagenet_std]),\n",
    "                        transforms.Normalize(mean=[-m for m in data_module.imagenet_mean], std=[1.]*IN_CHANNELS),\n",
    "                        transforms.ToPILImage()\n",
    "                    ])\n",
    "                    masks_gt_pil = [transforms.ToPILImage()(m.cpu()) for m in masks_gt_eval]\n",
    "                    if eval_model.hparams.activation is None: masks_pred_eval = torch.sigmoid(masks_pred_eval)\n",
    "                    masks_pred_binary_pil = [transforms.ToPILImage()((m > 0.5).float().cpu()) for m in masks_pred_eval]\n",
    "\n",
    "                    num_to_show = min(4, images_eval.size(0))\n",
    "                    fig, axes = plt.subplots(num_to_show, 3, figsize=(12, num_to_show * 4))\n",
    "                    if num_to_show == 1: axes = np.array([axes]) # Ensure axes is always 2D for consistent indexing\n",
    "                    for i in range(num_to_show):\n",
    "                        img_pil = unnorm_transform(images_eval[i].cpu()) # Move image to CPU before unnorm\n",
    "                        axes[i, 0].imshow(img_pil); axes[i, 0].set_title(\"Image\"); axes[i, 0].axis('off')\n",
    "                        axes[i, 1].imshow(masks_gt_pil[i], cmap='gray'); axes[i, 1].set_title(\"Ground Truth\"); axes[i, 1].axis('off')\n",
    "                        axes[i, 2].imshow(masks_pred_binary_pil[i], cmap='gray'); axes[i, 2].set_title(\"Prediction\"); axes[i, 2].axis('off')\n",
    "                    plt.tight_layout(); plt.show()\n",
    "                else: print(\"Validation dataloader not available or empty for visualization.\")\n",
    "            except Exception as e: print(f\"Error during eval/viz of {first_config_name}: {e}\"); import traceback; traceback.print_exc()\n",
    "        else: print(f\"No checkpoint file found in {checkpoint_dir} for {first_config_name}.\")\n",
    "    else: print(f\"Checkpoint directory not found for {first_config_name}. Train first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps and Further Experimentation\n",
    "\n",
    "This notebook provides a foundational framework for PV panel segmentation. Here are some potential next steps and areas for further experimentation:\n",
    "\n",
    "*   **Full Training Runs:** Increase `NUM_EPOCHS` for more comprehensive training.\n",
    "*   **Hyperparameter Tuning:** Experiment with different learning rates, batch sizes, optimizer settings, and loss function weights.\n",
    "*   **Explore More Architectures/Encoders:** Leverage the flexibility of `segmentation-models-pytorch` to try other model configurations available in `MODEL_CONFIGURATIONS` or add new ones.\n",
    "*   **Data Augmentation:** Implement more sophisticated data augmentation techniques using `albumentations` within the `PVSegmentationDataset` or `PVSegmentationDataModule` to improve model generalization.\n",
    "*   **Advanced Loss Functions:** Explore other loss functions or combinations suitable for imbalanced segmentation tasks.\n",
    "*   **Test Set Evaluation:** Create a dedicated test set (if not already done) and evaluate the final model on it for an unbiased performance measure.\n",
    "*   **Post-processing:** Implement post-processing steps (e.g., removing small predicted regions, morphological operations) to potentially improve segmentation quality.\n",
    "*   **Larger Datasets:** Train on larger and more diverse datasets if available.\n",
    "*   **Cross-Validation:** Implement k-fold cross-validation for more robust performance estimation.\n",
    "\n",
    "Review the logs generated in the `tb_logs/` (TensorBoard) and `csv_logs/` directories, and inspect the saved model checkpoints in the `checkpoints/` directory to monitor training progress and select the best models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Visualization Slideshow\n",
    "\n",
    "The cell below creates an interactive slideshow to display multiple screenshots without cluttering the notebook. This is particularly useful for showing a series of visualizations from the NYT article about clean energy or for comparing model results.\n",
    "\n",
    "The slideshow has navigation buttons and automatically loads all image files from a specified directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% --- 7. Next Steps ---\n",
    "print(\"\\nNotebook execution finished.\")\n",
    "print(\"Next steps: Review logs in 'tb_logs/' and 'csv_logs/'. Check 'checkpoints/' for saved models.\")\n",
    "print(\"Consider increasing NUM_EPOCHS for full training runs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% --- 9. Interactive Visualization Slideshow ---\n",
    "\n",
    "from IPython.display import HTML, Image, display\n",
    "import os\n",
    "from pathlib import Path\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "def create_slideshow(image_dir=\"report/assets/visualizations\", height=500):\n",
    "    \"\"\"Create an interactive slideshow from images in the specified directory.\n",
    "    \n",
    "    Args:\n",
    "        image_dir: Path to directory containing the screenshots/images\n",
    "        height: Height in pixels for the display area\n",
    "    \n",
    "    Returns:\n",
    "        Interactive widget displaying the slideshow\n",
    "    \"\"\"\n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(image_dir, exist_ok=True)\n",
    "    \n",
    "    # Find all image files\n",
    "    image_extensions = [\"jpg\", \"jpeg\", \"png\", \"gif\"]\n",
    "    image_files = []\n",
    "    for ext in image_extensions:\n",
    "        image_files.extend(glob.glob(os.path.join(image_dir, f\"*.{ext}\")))\n",
    "        image_files.extend(glob.glob(os.path.join(image_dir, f\"*.{ext.upper()}\")))  # Include uppercase extensions\n",
    "    \n",
    "    image_files = sorted(image_files)  # Sort alphabetically\n",
    "    \n",
    "    if not image_files:\n",
    "        print(f\"No images found in {image_dir}\")\n",
    "        print(f\"Please add your screenshots to the {image_dir} directory\")\n",
    "        print(f\"Supported formats: {', '.join(image_extensions)}\")\n",
    "        return None\n",
    "    \n",
    "    # Create widgets\n",
    "    slider = widgets.IntSlider(\n",
    "        value=0,\n",
    "        min=0, \n",
    "        max=len(image_files)-1, \n",
    "        step=1,\n",
    "        description='Image:',\n",
    "        continuous_update=False,\n",
    "        layout=widgets.Layout(width='50%')\n",
    "    )\n",
    "    \n",
    "    prev_button = widgets.Button(\n",
    "        description='Previous',\n",
    "        disabled=False,\n",
    "        button_style='', \n",
    "        tooltip='Previous image',\n",
    "        icon='arrow-left'\n",
    "    )\n",
    "    \n",
    "    next_button = widgets.Button(\n",
    "        description='Next',\n",
    "        disabled=False,\n",
    "        button_style='', \n",
    "        tooltip='Next image',\n",
    "        icon='arrow-right'\n",
    "    )\n",
    "    \n",
    "    image_widget = widgets.Image(\n",
    "        layout=widgets.Layout(height=f\"{height}px\"),\n",
    "    )\n",
    "    \n",
    "    title_widget = widgets.HTML(\n",
    "        layout=widgets.Layout(height='auto')\n",
    "    )\n",
    "\n",
    "    # Define callback functions\n",
    "    def on_slider_change(change):\n",
    "        index = change['new']\n",
    "        display_image(index)\n",
    "    \n",
    "    def on_prev_button_click(b):\n",
    "        if slider.value > 0:\n",
    "            slider.value -= 1\n",
    "    \n",
    "    def on_next_button_click(b):\n",
    "        if slider.value < len(image_files) - 1:\n",
    "            slider.value += 1\n",
    "    \n",
    "    def display_image(index):\n",
    "        filename = image_files[index]\n",
    "        with open(filename, 'rb') as f:\n",
    "            image_widget.value = f.read()\n",
    "        \n",
    "        base_filename = os.path.basename(filename)\n",
    "        title_widget.value = f\"<div style='text-align: center; font-weight: bold;'>{base_filename} ({index + 1}/{len(image_files)})</div>\"\n",
    "    \n",
    "    # Attach callbacks to widgets\n",
    "    slider.observe(on_slider_change, names='value')\n",
    "    prev_button.on_click(on_prev_button_click)\n",
    "    next_button.on_click(on_next_button_click)\n",
    "    \n",
    "    # Display initial image\n",
    "    display_image(0)\n",
    "    \n",
    "    # Arrange widgets\n",
    "    button_box = widgets.HBox([prev_button, next_button])\n",
    "    main_box = widgets.VBox([title_widget, image_widget, slider, button_box])\n",
    "    \n",
    "    return main_box\n",
    "\n",
    "# Create and display the slideshow widget\n",
    "slideshow = create_slideshow()\n",
    "\n",
    "if slideshow:\n",
    "    display(slideshow)\n",
    "else:\n",
    "    # Create directory structure if it doesn't exist\n",
    "    viz_dir = \"report/assets/visualizations\"\n",
    "    os.makedirs(viz_dir, exist_ok=True)\n",
    "    \n",
    "    # Display placeholder image with instructions\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.text(0.5, 0.5, f\"Add your screenshots to:\\n{os.path.abspath(viz_dir)}\", \n",
    "             ha='center', va='center', fontsize=16, wrap=True)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Interactive Slideshow - Setup Instructions\", fontsize=18)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nTo use the slideshow:\")\n",
    "    print(f\"1. Add your screenshot images to: {os.path.abspath(viz_dir)}\")\n",
    "    print(\"2. Make sure they're in jpg, jpeg, png, or gif format\")\n",
    "    print(\"3. Re-run this cell to see the interactive slideshow\")\n",
    "    print(\"\\nTip: You can change the images directory by modifying the argument to create_slideshow()\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% --- 8. Export to CoreML and Quantization ---\n",
    "\n",
    "# Only run this cell after successful training and evaluation\n",
    "\n",
    "if 'eval_model' not in locals() or eval_model is None:\n",
    "    print(\"No model available for export. Please run the evaluation cell first.\")\n",
    "else:\n",
    "    print(\"Preparing model for export to CoreML...\")\n",
    "    \n",
    "    try:\n",
    "        import coremltools as ct\n",
    "        from coremltools.models.neural_network import quantization_utils\n",
    "        \n",
    "        # Move model to CPU for export\n",
    "        eval_model.to('cpu')\n",
    "        eval_model.eval()\n",
    "        \n",
    "        # Define input shape\n",
    "        example_input = torch.rand(1, IN_CHANNELS, PATCH_SIZE_PIXELS, PATCH_SIZE_PIXELS)\n",
    "        \n",
    "        # First, export to TorchScript format\n",
    "        scripted_model = torch.jit.trace(eval_model, example_input)\n",
    "        \n",
    "        # Convert to CoreML\n",
    "        print(\"Converting model to CoreML format...\")\n",
    "        mlmodel = ct.convert(\n",
    "            scripted_model,\n",
    "            inputs=[ct.TensorType(name=\"input\", shape=example_input.shape)],\n",
    "            convert_to=\"mlprogram\",  # Use the newer ML Program format for better performance\n",
    "            # compute_units=\"ALL\"  # Can be \"ALL\", \"CPU_ONLY\", \"CPU_AND_GPU\", \"CPU_AND_NE\" (Neural Engine)\n",
    "        )\n",
    "        \n",
    "        # Set model metadata\n",
    "        mlmodel.short_description = \"PV Segmentation using PyTorch Lightning and SMP\"\n",
    "        mlmodel.input_description['input'] = \"Input image (RGB or multi-channel)\"\n",
    "        mlmodel.output_description['output'] = \"Segmentation mask for PV panels\"\n",
    "        \n",
    "        # Save the model\n",
    "        output_path = Path(\"exported_models\") / f\"{first_config_name}_coreml.mlpackage\"\n",
    "        os.makedirs(output_path.parent, exist_ok=True)\n",
    "        mlmodel.save(str(output_path))\n",
    "        print(f\"Model successfully exported to: {output_path}\")\n",
    "        \n",
    "        # ---------- QUANTIZATION OPTIONS (COMMENTED) ----------\n",
    "        \n",
    "        # # FP16 Quantization\n",
    "        # print(\"\\nCreating FP16 quantized model...\")\n",
    "        # mlmodel_fp16 = quantization_utils.quantize_weights(mlmodel, nbits=16)\n",
    "        # mlmodel_fp16.save(str(output_path).replace(\".mlpackage\", \"_fp16.mlpackage\"))\n",
    "        # print(\"FP16 model saved.\")\n",
    "        \n",
    "        # # INT8 Quantization (more aggressive compression, may affect accuracy)\n",
    "        # # print(\"\\nCreating INT8 quantized model...\")\n",
    "        # # mlmodel_int8 = quantization_utils.quantize_weights(mlmodel, nbits=8)\n",
    "        # # mlmodel_int8.save(str(output_path).replace(\".mlpackage\", \"_int8.mlpackage\"))\n",
    "        # # print(\"INT8 model saved.\")\n",
    "        \n",
    "        # # For even more advanced quantization with calibration:\n",
    "        # # from coremltools.optimize.coreml import create_quantized_model\n",
    "        # # mlmodel_quantized = create_quantized_model(\n",
    "        # #    mlmodel,\n",
    "        # #    calibration_data=create_image_generator(calibration_images),\n",
    "        # #    multiarray_dtype='int8'\n",
    "        # # )\n",
    "        \n",
    "    except ImportError as e:\n",
    "        print(f\"CoreML export failed: {e}\")\n",
    "        print(\"Please install coremltools using: pip install coremltools\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during CoreML export: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "eo-pv-cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
