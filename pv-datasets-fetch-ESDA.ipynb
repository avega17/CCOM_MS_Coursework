{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73f645e0",
   "metadata": {},
   "source": [
    "# Fetch Open Datasets of PV locations\n",
    "\n",
    "Many of these datasets are located in [Zenodo](https://zenodo.org/), a general-purpose open-access repository developed under the European OpenAIRE program and operated by CERN. Others are hosted in figshare, a web-based platform for sharing research data and other types of content. The rest are hosted in GitHub repositories or other open-access platforms.The datasets are available in various formats, including CSV, GeoJSON, and shapefiles, and raster masks. We'll be using open-source Python libraries to download and process them into properly georeferenced geoparquet files that will serve as a base for our [OLAP](https://www.datacamp.com/blog/oltp-vs-olap) database files that we'll manage with [dbt-core+duckdb](https://motherduck.com/blog/motherduck-duckdb-dbt/) (see these short videos on [this stack](https://www.youtube.com/watch?v=asxGh2TrNyI) and background on [MotherDuck](https://www.youtube.com/watch?v=OuCY7_DzCTA) for local AND cloud geospatial data processing with the same tools and codebase).\n",
    "\n",
    "Here we list the dataset titles alongside their first author, DOI links, and their number of labels:\n",
    "- **\"Distributed solar photovoltaic array location and extent dataset for remote sensing object identification\"** - K. Bradbury, 2016 | [paper DOI](https://doi.org/10.1038/sdata.2016.106) | [dataset DOI](https://doi.org/10.6084/m9.figshare.3385780.v4) | polygon annotations for 19,433 PV modules in 4 cities in California, USA\n",
    "- \"A solar panel dataset of very high resolution satellite imagery to support the Sustainable Development Goals\" - C. Clark et al, 2023 | [paper DOI](https://doi.org/10.1038/s41597-023-02539-8) | [dataset DOI](https://doi.org/10.6084/m9.figshare.22081091.v3) | 2,542 object labels (per spatial resolution)\n",
    "- \"A harmonised, high-coverage, open dataset of solar photovoltaic installations in the UK\" - D. Stowell et al, 2020 | [paper DOI](https://doi.org/10.1038/s41597-020-00739-0) | [dataset DOI](https://zenodo.org/records/4059881) | 265,418 data points (over 255,000 are stand-alone installations, 1067 solar farms, and rest are subcomponents within solar farms)\n",
    "- \"Georectified polygon database of ground-mounted large-scale solar photovoltaic sites in the United States\" - K. Sydny, 2023 | [paper DOI](https://doi.org/10.1038/s41597-023-02644-8) | [dataset DOI](https://www.sciencebase.gov/catalog/item/6671c479d34e84915adb7536) | 4186 data points (Note: these correspond to PV _facilities_ rather than individual panel arrays or objects and need filtering of duplicates with other datasets and further processing to extract the PV arrays in the facility)\n",
    "- \"Vectorized solar photovoltaic installation dataset across China in 2015 and 2020\" - J. Liu et al, 2024 | [paper DOI](https://doi.org/10.1038/s41597-024-04356-z) | [dataset link](https://github.com/qingfengxitu/ChinaPV) | 3,356 PV labels (inspect quality!)\n",
    "- \"Multi-resolution dataset for photovoltaic panel segmentation from satellite and aerial imagery\" - H. Jiang, 2021 | [paper DOI](https://doi.org/10.5194/essd-13-5389-2021) | [dataset DOI](https://doi.org/10.5281/zenodo.5171712) | 3,716 samples of PV data points\n",
    "- \"A crowdsourced dataset of aerial images with annotated solar photovoltaic arrays and installation metadata\" - G. Kasmi, 2023 | [paper DOI](https://doi.org/10.1038/s41597-023-01951-4) | [dataset DOI](https://doi.org/10.5281/zenodo.6865878) | > 28K points of PV installations; 13K+ segmentation masks for PV arrays; metadata for 8K+ installations\n",
    "- **\"An Artificial Intelligence Dataset for Solar Energy Locations in India\"** - A. Ortiz, 2022 | [paper DOI](https://doi.org/10.1038/s41597-022-01499-9) | [dataset link 1](https://researchlabwuopendata.blob.core.windows.net/solar-farms/solar_farms_india_2021.geojson) or [dataset link 2](https://raw.githubusercontent.com/microsoft/solar-farms-mapping/refs/heads/main/data/solar_farms_india_2021_merged_simplified.geojson) | 117 geo-referenced points of solar installations across India\n",
    "- \"GloSoFarID: Global multispectral dataset for Solar Farm IDentification in satellite imagery\" - Z. Yang, 2024** | [paper DOI](https://doi.org/10.48550/arXiv.2404.05180) | [dataset DOI](https://github.com/yzyly1992/GloSoFarID/tree/main/data_coordinates) | 6,793 PV samples across 3 years (double counting of samples)\n",
    "- **\"A global inventory of photovoltaic solar energy generating units\" - L. Kruitwagen et al, 2021** | [paper DOI](https://doi.org/10.1038/s41586-021-03957-7) | [dataset DOI](https://doi.org/10.5281/zenodo.5005867) | 50,426 for training, cross-validation, and testing; 68,661 predicted polygon labels \n",
    "- **\"Harmonised global datasets of wind and solar farm locations and power\" - S. Dunnett et al, 2020** | [paper DOI](https://doi.org/10.1038/s41597-020-0469-8) | [dataset DOI](https://doi.org/10.6084/m9.figshare.11310269.v6) | 35272 PV installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8564ba95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import Layout\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from branca.colormap import linear\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "import shapely\n",
    "import pygeohash\n",
    "import folium\n",
    "import lonboard\n",
    "import pydeck as pdk\n",
    "# import openeo \n",
    "# import pystac_client\n",
    "\n",
    "# import easystac\n",
    "# import cubo\n",
    "\n",
    "import duckdb as dd \n",
    "import datahugger\n",
    "import sciencebasepy\n",
    "from seedir import seedir\n",
    "\n",
    "# python libraries\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import urllib.parse\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import tempfile\n",
    "import shutil\n",
    "import pprint as pp\n",
    "import time\n",
    "import re\n",
    "from zipfile import ZipFile\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04686807",
   "metadata": {},
   "source": [
    "# Dataset Metadata and Preparation\n",
    "\n",
    "We will use datahugger, sciencebasepy, and osf-client to manage official dataset records published in open access scientific data repositories.\n",
    "We also implement some ad-hoc functions to download some data assets hosted in GitHub repositories. \n",
    "We will also use geopandas, rasterio, and pyproj to process the datasets into georeferenced geoparquet files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3aad185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dict of metadata for datasets\n",
    "# this will be used for interactive widget and managing downloads\n",
    "\n",
    "# for maxar dataset\n",
    "# Catalogue ID 1040050029DC8C00; use to find geospatial extent coords\n",
    "# The geocoordinates for each solar panel object may be determined using the native resolution labels (found in the labels_native directory). \n",
    "# The center and width values for each object, along with the relative location information provided by the naming convention for each label, \n",
    "# may be used to determine the pixel coordinates for each object in the full, corresponding native resolution tile. \n",
    "# The pixel coordinates may be translated to geocoordinates using the EPSG:32633 coordinate system and the following geotransform for each tile:\n",
    "\n",
    "# Tile 1: (307670.04, 0.31, 0.0, 5434427.100000001, 0.0, -0.31)\n",
    "# Tile 2: (312749.07999999996, 0.31, 0.0, 5403952.860000001, 0.0, -0.31)\n",
    "# Tile 3: (312749.07999999996, 0.31, 0.0, 5363320.540000001, 0.0, -0.31)\n",
    "# see here on gdal format geotransform: https://gdal.org/en/stable/tutorials/geotransforms_tut.html\n",
    "\n",
    "# look into adding dataset crs or projection to metadata dict\n",
    "# note that most of these details are hardcoded and difficult to parse ahead of time\n",
    "# load environment variables\n",
    "load_dotenv()\n",
    "DATASET_DIR = Path(os.getenv('DATA_PATH'))\n",
    "dataset_metadata = {\n",
    "    'deu_maxar_vhr_2023': {\n",
    "        'doi': '10.6084/m9.figshare.22081091.v3',\n",
    "        'repo': 'figshare',\n",
    "        'compression': 'zip',\n",
    "        'label_fmt': 'yolo_fmt_txt',\n",
    "        'has_imgs': False,\n",
    "        'label_count': 2542 # solar panel objects (ie not individual panels)\n",
    "    },\n",
    "    'uk_crowdsourced_pv_2020': {\n",
    "        'doi': '10.5281/zenodo.4059881',\n",
    "        'repo': 'zenodo',\n",
    "        'compression': None,\n",
    "        'label_fmt': 'geojson',\n",
    "        'geom_type': {'features': ['Point', 'Polygon', 'MultiPolygon']},\n",
    "        'crs': None, # default to WGS84 when processing\n",
    "        'has_imgs': False,\n",
    "        'label_count': 265418\n",
    "    },\n",
    "    # note for report later: Maxar Technologies (MT) was primarily used to determine the extent of solar arrays\n",
    "    'usa_eia_large_scale_pv_2023': {\n",
    "        'doi': '10.5281/zenodo.8038684',\n",
    "        'repo': 'sciencebase',\n",
    "        'compression': 'zip',\n",
    "        'label_fmt': 'shp',\n",
    "        'has_imgs': False,\n",
    "        'label_count': 4186\n",
    "    },\n",
    "    'chn_med_res_pv_2024': {\n",
    "        # using github files since zenodo shapefiles fail to load in QGIS\n",
    "        'doi': 'https://github.com/qingfengxitu/ChinaPV/tree/main',\n",
    "        'repo': 'github',\n",
    "        'compression': None,\n",
    "        'label_fmt': 'shp',\n",
    "        'has_imgs': False,\n",
    "        'label_count': 3356\n",
    "    },\n",
    "    'usa_cali_usgs_pv_2016': {\n",
    "        'doi': '10.6084/m9.figshare.3385780.v4',\n",
    "        'repo': 'figshare',\n",
    "        'compression': None,\n",
    "        'label_fmt': 'geojson',\n",
    "        'crs': 'NAD83',\n",
    "        'geom_type': {'features': 'Polygon'},\n",
    "        'has_imgs': False,\n",
    "        'label_count': 19433\n",
    "    },\n",
    "    'chn_jiangsu_vhr_pv_2021': {\n",
    "        'doi': '10.5281/zenodo.5171712',\n",
    "        'repo': 'zenodo',\n",
    "        'compression': 'zip',\n",
    "        # look into geotransform details for processing these labels\n",
    "        'label_fmt': 'pixel_mask',\n",
    "        'has_imgs': True,\n",
    "        'label_count': 3716\n",
    "    },\n",
    "    'ind_pv_solar_farms_2022': {\n",
    "        'doi': 'https://raw.githubusercontent.com/microsoft/solar-farms-mapping/refs/heads/main/data/solar_farms_india_2021_merged_simplified.geojson',\n",
    "        'repo': 'github',\n",
    "        'compression': None,\n",
    "        'label_fmt': 'geojson',\n",
    "        'geom_type': {'features': 'MultiPolygon'}, \n",
    "        'crs': 'WGS84',\n",
    "        'has_imgs': False,\n",
    "        'label_count': 117\n",
    "    },\n",
    "    'fra_west_eur_pv_installations_2023': {\n",
    "        'doi': '10.5281/zenodo.6865878',\n",
    "        'repo': 'zenodo',\n",
    "        'compression': 'zip',\n",
    "        'label_fmt': 'json',\n",
    "        'geom_type': {'Polygon': ['Point']},\n",
    "        'crs': None, \n",
    "        'has_imgs': True, \n",
    "        'label_count': (13303, 7686)\n",
    "    },\n",
    "    'global_pv_inventory_sent2_spot_2021': {\n",
    "        'doi': '10.5281/zenodo.5005867',\n",
    "        'repo': 'zenodo',\n",
    "        'compression': None,\n",
    "        'label_fmt': 'geojson',\n",
    "        'geom_type': ['Polygon'],\n",
    "        'crs': 'WGS84',\n",
    "        'has_imgs': False,\n",
    "        'label_count': (50426, 68661) # (training+cv+test, human-verified predictions)\n",
    "    },\n",
    "    'global_pv_inventory_sent2_2024': {\n",
    "        'doi': 'https://github.com/yzyly1992/GloSoFarID/tree/main/data_coordinates',\n",
    "        'repo': 'github',\n",
    "        'compression': None,\n",
    "        'label_fmt': 'json',\n",
    "        'crs': None, # default to WGS84 when processing\n",
    "        'geom_type': ['Point'], # normal json with no geometry attribute\n",
    "        'has_imgs': True, \n",
    "        'label_count': 6793\n",
    "    },\n",
    "    'global_harmonized_large_solar_farms_2020': {\n",
    "        'doi': '10.6084/m9.figshare.11310269.v6',\n",
    "        'repo': 'figshare',\n",
    "        'compression': 'zip', # entire dataset is zipped\n",
    "        'label_fmt': 'gpkg',\n",
    "        'geom_type': ['Polygon'], # only in geopackage; all other formats are coords of centroid points\n",
    "        'crs': 'WGS84', # also offers file in Eckert IV projection\n",
    "        'has_imgs': False,\n",
    "        'label_count': 35272\n",
    "\n",
    "    }\n",
    "\n",
    "}\n",
    "\n",
    "dataset_choices = [\n",
    "    'global_harmonized_large_solar_farms_2020',\n",
    "    # 'global_pv_inventory_sent2_2024',\n",
    "    'global_pv_inventory_sent2_spot_2021',\n",
    "    # 'fra_west_eur_pv_installations_2023',\n",
    "    'ind_pv_solar_farms_2022',\n",
    "    'usa_cali_usgs_pv_2016',\n",
    "    # 'chn_med_res_pv_2024',\n",
    "    # 'usa_eia_large_scale_pv_2023',\n",
    "    # 'uk_crowdsourced_pv_2020',\n",
    "    # 'deu_maxar_vhr_2023'   \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6162a17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store selected datasets\n",
    "# mostly gen by github copilot with Claude 3.7 model\n",
    "selected_datasets = dataset_choices.copy()\n",
    "\n",
    "def format_dataset_info(dataset):\n",
    "    \"\"\"Create a formatted HTML table for dataset metadata\"\"\"\n",
    "    metadata = dataset_metadata[dataset]\n",
    "    \n",
    "    # Create table with metadata\n",
    "    html = f\"\"\"\n",
    "    <style>\n",
    "    .dataset-table {{\n",
    "        border-collapse: collapse;\n",
    "        width: 30%;\n",
    "        margin: 20px auto;\n",
    "        font-family: Arial, sans-serif;\n",
    "    }}\n",
    "    .dataset-table th, .dataset-table td {{\n",
    "        border: 1px solid #ddd;\n",
    "        padding: 8px;\n",
    "        text-align: left;\n",
    "    }}\n",
    "    .dataset-table th {{\n",
    "        background-color: #f2f2f2;\n",
    "        font-weight: bold;\n",
    "    }}\n",
    "    </style>\n",
    "    <table class=\"dataset-table\">\n",
    "        <tr><th>Metadata</th><th>Value</th></tr>\n",
    "        <tr><td>DOI/URL</td><td>{metadata['doi']}</td></tr>\n",
    "        <tr><td>Repository</td><td>{metadata['repo']}</td></tr>\n",
    "        <tr><td>Compression</td><td>{metadata['compression'] or 'None'}</td></tr>\n",
    "        <tr><td>Label Format</td><td>{metadata['label_fmt']}</td></tr>\n",
    "        <tr><td>Has Images</td><td>{metadata['has_imgs']}</td></tr>\n",
    "        <tr><td>Label Count</td><td>{metadata.get('label_count', 'Unknown')}</td></tr>\n",
    "    </table>\n",
    "    \"\"\"\n",
    "    return html\n",
    "\n",
    "# Create an accordion to display selected datasets with centered layout\n",
    "dataset_accordion = widgets.Accordion(\n",
    "    children=[widgets.HTML(format_dataset_info(ds)) for ds in selected_datasets],\n",
    "    layout=Layout(width='50%', margin='0 auto')\n",
    ")\n",
    "for i, ds in enumerate(selected_datasets):\n",
    "    dataset_accordion.set_title(i, ds)\n",
    "\n",
    "# Define a function to add or remove datasets\n",
    "def manage_datasets(action, dataset=None):\n",
    "    global selected_datasets, dataset_accordion\n",
    "    \n",
    "    if action == 'add' and dataset and dataset not in selected_datasets:\n",
    "        selected_datasets.append(dataset)\n",
    "    elif action == 'remove' and dataset and dataset in selected_datasets:\n",
    "        selected_datasets.remove(dataset)\n",
    "    \n",
    "    # Update the accordion with current selections\n",
    "    dataset_accordion.children = [widgets.HTML(format_dataset_info(ds)) for ds in selected_datasets]\n",
    "    for i, ds in enumerate(selected_datasets):\n",
    "        dataset_accordion.set_title(i, ds)\n",
    "    \n",
    "    f\"Currently selected datasets: {len(selected_datasets)}\"\n",
    "\n",
    "# Create dropdown for available datasets\n",
    "dataset_dropdown = widgets.Dropdown(\n",
    "    options=list(dataset_metadata.keys()),\n",
    "    description='Dataset:',\n",
    "    disabled=False,\n",
    "    layout=Layout(width='70%', margin='20 20 auto 20 20')\n",
    ")\n",
    "\n",
    "# Create buttons for actions\n",
    "add_button = widgets.Button(description=\"Add Dataset\", button_style='success')\n",
    "remove_button = widgets.Button(description=\"Remove Dataset\", button_style='danger')\n",
    "\n",
    "# Define button click handlers\n",
    "def on_add_clicked(b):\n",
    "    manage_datasets('add', dataset_dropdown.value)\n",
    "\n",
    "def on_remove_clicked(b):\n",
    "    manage_datasets('remove', dataset_dropdown.value)\n",
    "\n",
    "# Link buttons to handlers\n",
    "add_button.on_click(on_add_clicked)\n",
    "remove_button.on_click(on_remove_clicked)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f44fde4",
   "metadata": {},
   "source": [
    "## Dataset Selection Interface\n",
    "#### Use the dropdown and buttons below to customize which solar panel datasets will be fetched and processed.\n",
    "- Select a dataset from the dropdown:\n",
    "    - Click \"Add Dataset\" to include it in processing\n",
    "    - Click \"Remove Dataset\" to exclude it\n",
    "- View metadata table in the selected dataset's dropdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb943c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the widgets\n",
    "display(widgets.HBox([dataset_dropdown, add_button, remove_button]))\n",
    "display(dataset_accordion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a78984",
   "metadata": {},
   "source": [
    "# Fetching and Organizing datasets for later-preprocessing\n",
    "\n",
    "We will use [datahugger](https://j535d165.github.io/datahugger/) to fetch datasets hosted in Zenodo, figshare, and GitHub. \n",
    "\n",
    "We will sciencebase for the dataset hosted in the USGS ScienceBase Catalog.\n",
    "We will pre-process and convert datasets into geojson, if not already formatted, and manage these using [geopandas](https://geopandas.org/). These will be further processed into geoparquet files for use in duckdb tables used to manage and later consolidate the datasets with dbt.  \n",
    "- The datasets will be stored in the `data/` directory\n",
    "    - the geoparquet files will be stored in the `data/geoparquet/` directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f56a89",
   "metadata": {},
   "source": [
    "#### Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86cf8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# move to utility functions later\n",
    "# def fetch_github_repo_files(dataset_name, \n",
    "\n",
    "\n",
    "# use the metadata to fetch the dataset files using datahugger\n",
    "def fetch_dataset_files(dataset_name, max_mb=100, force=False):\n",
    "    metadata = dataset_metadata[dataset_name]\n",
    "    doi = metadata['doi']\n",
    "    repo = metadata['repo']\n",
    "    compression = metadata['compression']\n",
    "    label_fmt = metadata['label_fmt']\n",
    "    # convert to bytes\n",
    "    max_dl = max_mb * 1024 * 1024\n",
    "    dataset_dir = os.path.join(os.getenv('DATA_PATH'), 'raw', 'labels', dataset_name)\n",
    "    geofile_regex = r'^(.*\\.(geojson|json|shp|zip|csv|gpkg))$'\n",
    "    dst = os.path.join(os.getcwd(), dataset_dir)\n",
    "    dst_p = Path(dst)\n",
    "\n",
    "    # if re-fetching, remove existing files to avoid duplicates\n",
    "    if force and dst_p.exists():\n",
    "        shutil.rmtree(dst_p)\n",
    "        print(f\"Removed existing dataset directory: {os.path.relpath(dst_p)}\")\n",
    "\n",
    "    # prettyprint metadata and dst info\n",
    "    # pp.pprint(metadata)\n",
    "    # print(f\"Destination: {dataset_dir}\")\n",
    "    # print(f\"Max download size: {max_mb} MB\")\n",
    "    # print(f\"Force Download: {force}\")\n",
    "\n",
    "    dataset_tree = {}\n",
    "\n",
    "    # TODO: move different repo handling to separate functions\n",
    "\n",
    "    # use datahugger to fetch files from most repos\n",
    "    if repo in ['figshare', 'zenodo']:\n",
    "\n",
    "        # handle unzipping manually below so we get progress bar for download\n",
    "        ds_tree = datahugger.get(doi, dst, max_file_size=max_dl, force_download=force, unzip=False)\n",
    "        # compare files to be fetched (after filtering on max file size) with existing files  \n",
    "        files_to_fetch = [f['name'] for f in ds_tree.dataset.files if f['size'] <= max_dl]\n",
    "        ds_files = [os.path.join(root, fname) for root, dirs, files in os.walk(dst_p) for fname in files if re.match(geofile_regex, fname)]\n",
    "        # flag for avoiding extracting zip when already extracted\n",
    "        # is_unzipped = all(f in ds_files for f in files_to_fetch) and len(ds_files) > 1\n",
    "        # TODO: handle .zip files that consist of a redundant copy of the entire dataset\n",
    "        if metadata['compression'] == 'zip' and any(f.endswith('.zip') for f in ds_files):\n",
    "            print(f\"Dataset metadata for {dataset_name} indicates handling of one or more downloaded zip files.\")\n",
    "            # check if the zip file was fetched and directly extract if it's the only file in the dataset\n",
    "            extracted_files = []\n",
    "            if len(ds_files) <= 2 and ds_files[0].endswith('.zip'):\n",
    "                zip_file = dst_p / ds_files[0]\n",
    "                # print(f\"Found single zip file for dataset: {zip_file}\")\n",
    "                # extract the zip file and delete it \n",
    "                with ZipFile(zip_file, 'r') as zip_ref:\n",
    "                    extracted_files = zip_ref.namelist()\n",
    "                    zip_ref.extractall(dst)\n",
    "                \n",
    "                # remove the zip file\n",
    "                # try:\n",
    "                #     os.remove(zip_file)\n",
    "                #     print(f\"Removed {os.path.relpath(zip_file)} after extraction\")\n",
    "                # except Exception as e:\n",
    "                #     print(f\"Error removing {zip_file}: {e}\")\n",
    "                # check if zip file consisted of a single dir and move contents up one level\n",
    "                top_level_dir = dst_p / extracted_files[0]\n",
    "                if top_level_dir.is_dir():\n",
    "                    # move only first level dirs and files to our dataset dir\n",
    "                    for item in top_level_dir.iterdir():\n",
    "                        if item.name.endswith('.zip'):\n",
    "                            continue\n",
    "                        # don't copy if already exists and is non-empty\n",
    "                        # TODO: add non-empty check\n",
    "                        elif os.path.exists(dst_p / item.name):\n",
    "                            print(f\"Skipping {item} as it already exists in {os.path.relpath(dst)}\")\n",
    "                            continue\n",
    "                        elif item.parent == top_level_dir and re.match(geofile_regex, item.name):\n",
    "                            print(f\"Moving {item} to {os.path.relpath(dst)}\")\n",
    "                            shutil.move(item, dst)\n",
    "                    # remove the top level dir\n",
    "                    shutil.rmtree(top_level_dir)\n",
    "\n",
    "                ds_files = [os.path.join(root, fname) for root, dirs, files in os.walk(dst_p) for fname in files if re.match(geofile_regex, fname)]\n",
    "                print(f\"Moved items from {os.path.relpath(top_level_dir)} to:\\n{os.path.relpath(dst_p)}\")\n",
    "                print(f\"After extraction and moving, we have {len(ds_files)} files in {os.path.relpath(dst)}:\\n{ds_files}\")\n",
    "\n",
    "            elif len(ds_files) > 2:\n",
    "                # multiple files in addition to the zip file; handle on case by case basis\n",
    "                print(f\"Multiple files found in {dst_p}:\\n{os.listdir(dst_p)}\")\n",
    "        # no further processing needed; get file list directly from datahugger\n",
    "        else: \n",
    "            ds_files = [os.path.join(root, fname) for root, dirs, files in os.walk(dst_p) for fname in files if re.match(geofile_regex, fname)]\n",
    "\n",
    "\n",
    "        dataset_tree = {\n",
    "            'dataset': dataset_name,\n",
    "            'output_dir': ds_tree.output_folder,\n",
    "            'files': ds_files,\n",
    "            'fs_tree': seedir(dst_p, depthlimit=5, printout=False, regex=True, include_files=geofile_regex)\n",
    "        }\n",
    "\n",
    "    elif repo == 'github':\n",
    "        # Handle GitHub repositories using git partial cloning of repo \n",
    "        \n",
    "        # Create destination directory if it doesn't exist\n",
    "        os.makedirs(dst, exist_ok=True)\n",
    "        # Parse the GitHub URL\n",
    "        # [user, repo, tree, branch, rest of path]\n",
    "        parts = doi.replace('https://github.com/', '').split('/')\n",
    "        repo_path = f\"{parts[0]}/{parts[1]}\"\n",
    "        \n",
    "        # Extract branch and path\n",
    "        branch = 'main'  # Default branch\n",
    "        path = ''\n",
    "        \n",
    "        # check if local path exists and contains expected files\n",
    "        if os.path.exists(dst) and any(os.path.splitext(fname)[1] in ['.geojson', '.json', '.shp', '.zip'] for fname in os.listdir(dst)) and not force:  \n",
    "            print(f\"Destination path for {dataset_name}'s repo already exists and contains expected files.\")\n",
    "            # print in bold\n",
    "            print(f\"\\033[1mSkipping Download!\\033[0m\")\n",
    "            # fetch dataset dir info from Pathlib and tree from seedir \n",
    "            tree = seedir(dst_p, depthlimit=5, printout=False, regex=True, include_files=geofile_regex)\n",
    "            # get list of files in Path object that satisfy regex\n",
    "            ds_files = [os.path.join(root, fname) for root, dirs, files in os.walk(dst_p) for fname in files if re.match(geofile_regex, fname)]\n",
    "            dataset_tree = {\n",
    "                'dataset': dataset_name,\n",
    "                'output_dir': dst,\n",
    "                'files': ds_files,\n",
    "                'fs_tree': tree\n",
    "            }\n",
    "\n",
    "        # Check if it's a folder/repository or a single file\n",
    "        elif '/blob/' not in doi and 'raw.githubusercontent.com' not in doi:\n",
    "            try:\n",
    "                if 'tree' in parts:\n",
    "                    tree_index = parts.index('tree')\n",
    "                    branch = parts[tree_index + 1]\n",
    "                    path = '/'.join(parts[tree_index + 2:]) if len(parts) > tree_index + 2 else ''\n",
    "                \n",
    "                # Create a temporary directory for the sparse checkout\n",
    "                with tempfile.TemporaryDirectory() as temp_dir:\n",
    "                    # Initialize the git repository and set up sparse checkout\n",
    "                    commands = [f\"git clone --filter=blob:limit={max_mb}m --depth 1 https://github.com/{repo_path}.git {dataset_name}\"]\n",
    "                    # print(f\"Running commands: {commands}\")\n",
    "                    # Execute git commands\n",
    "                    for cmd in commands:\n",
    "                        \n",
    "                        process = subprocess.run(cmd, shell=True, cwd=temp_dir, \n",
    "                                               capture_output=True, text=True)\n",
    "                        # show command output (debug)\n",
    "                        print(f\"Command stdout: {process.stdout}\")\n",
    "                        if process.returncode != 0:\n",
    "                            raise Exception(f\"Git command failed: {cmd}\\n{process.stderr}\")\n",
    "                    \n",
    "                    # Copy only the files in the dir specified in DOI/URL\n",
    "                    repo_ds_dir = os.path.join(temp_dir, dataset_name, path) if path else os.path.join(temp_dir, dataset_name)\n",
    "                    files_list = []\n",
    "                    #\n",
    "                    for root, _, files in os.walk(repo_ds_dir):\n",
    "                        for file in files:\n",
    "                            if file.startswith('.git'):\n",
    "                                continue\n",
    "                            src_file = os.path.join(root, file)\n",
    "                            # Create relative path\n",
    "                            rel_path = os.path.relpath(src_file, repo_ds_dir)\n",
    "                            dst_file = os.path.join(dst, rel_path)\n",
    "                            \n",
    "                            # Create destination directory if needed\n",
    "                            os.makedirs(os.path.dirname(dst_file), exist_ok=True)\n",
    "                            \n",
    "                            # Copy the file\n",
    "                            shutil.copy2(src_file, dst_file)\n",
    "                            files_list.append(dst_file)\n",
    "                            print(f\"Copied {rel_path} to ./{dataset_dir}/{rel_path}\")\n",
    "\n",
    "                dataset_tree = {\n",
    "                    'dataset': dataset_name,\n",
    "                    'output_dir': dst,\n",
    "                    'files': files_list,\n",
    "                    'fs_tree': seedir(dst_p, depthlimit=5, printout=False, regex=True, include_files=geofile_regex)\n",
    "                }\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error performing git clone: {e}\")\n",
    "                return None\n",
    "        else:\n",
    "            # It's a single file (raw URL or blob URL)\n",
    "            try:\n",
    "                # Convert blob URL to raw URL if needed\n",
    "                if '/blob/' in doi:\n",
    "                    raw_url = doi.replace('github.com', 'raw.githubusercontent.com').replace('/blob/', '/')\n",
    "                else:\n",
    "                    raw_url = doi\n",
    "                \n",
    "                # Extract filename from URL\n",
    "                filename = os.path.basename(urllib.parse.urlparse(raw_url).path)\n",
    "                local_file_path = os.path.join(dst, filename)\n",
    "                \n",
    "                # Download the file\n",
    "                response = requests.get(raw_url, stream=True)\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                # Check file size\n",
    "                file_size = int(response.headers.get('content-length', 0))\n",
    "                if file_size > max_dl:\n",
    "                    print(f\"File size ({file_size} bytes) exceeds maximum allowed size ({max_dl * 1024 * 1024} MB)\")\n",
    "                    return None\n",
    "                \n",
    "                with open(local_file_path, 'wb') as f:\n",
    "                    for chunk in tqdm(response.iter_content(chunk_size=8192), desc=f\"Downloading {filename}\", unit='KB'):\n",
    "                        f.write(chunk)\n",
    "                print(f\"Downloaded {filename} to {os.path.relpath(local_file_path)}\")\n",
    "                dataset_tree = {\n",
    "                    'dataset': dataset_name,\n",
    "                    'output_dir': dst,\n",
    "                    'files': [local_file_path],\n",
    "                    'fs_tree': seedir(dst_p, depthlimit=5, printout=False, regex=True, include_files=geofile_regex)\n",
    "                }\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error downloading GitHub file: {e}\")\n",
    "\n",
    "    elif repo == 'sciencebase':\n",
    "        # Initialize ScienceBase client\n",
    "        # sb = sciencebasepy.SbSession()\n",
    "        \n",
    "        # # Extract the item ID from the DOI or URL\n",
    "        # # DOIs like 10.5281/zenodo.8038684 or URLs with item ID\n",
    "        # item_id = doi.split('/')[-1] if '/' in doi else doi\n",
    "        \n",
    "        # try:\n",
    "        #     # Get item details\n",
    "        #     item = sb.get_item(item_id)\n",
    "            \n",
    "        #     # Create destination directory\n",
    "        #     os.makedirs(dst, exist_ok=True)\n",
    "            \n",
    "        #     # Download all files associated with the item\n",
    "        #     downloaded_files = []\n",
    "            \n",
    "        #     # Get item files\n",
    "        #     files = sb.get_item_file_info(item_id)\n",
    "            \n",
    "        #     for file_info in files:\n",
    "        #         file_name = file_info['name']\n",
    "        #         file_url = file_info['url']\n",
    "                \n",
    "        #         # Check file size if available\n",
    "        #         if 'size' in file_info and file_info['size'] > max_dl:\n",
    "        #             print(f\"Skipping file {file_name} as it exceeds the maximum download size\")\n",
    "        #             continue\n",
    "                \n",
    "        #         # Download the file\n",
    "        #         local_file_path = os.path.join(dst, file_name)\n",
    "        #         sb.download_file(file_url, local_file_path)\n",
    "                \n",
    "        #         downloaded_files.append(local_file_path)\n",
    "        #         print(f\"Downloaded {file_name} to {local_file_path}\")\n",
    "        print(\"Not Implemented yet\")\n",
    "        return None\n",
    "\n",
    "    print(f\"Fetched {len(dataset_tree['files'])} dataset files for {dataset_name} in {os.path.relpath(dataset_tree['output_dir'])}:\")\n",
    "    print(dataset_tree['fs_tree'])\n",
    "\n",
    "    return dataset_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78eece7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through the selected datasets and fetch files\n",
    "# iterate through the selected datasets and fetch files\n",
    "ds_trees = {}\n",
    "max_mb = int(os.getenv('MAX_LABEL_MB', 100))\n",
    "print(f\"Fetching {len(selected_datasets)} datasets with files of max size {max_mb} MB\")\n",
    "\n",
    "# Create widgets for controlling the fetching process\n",
    "fetch_output = widgets.Output(\n",
    "    layout=widgets.Layout(\n",
    "        width='80%', \n",
    "        border='1px solid #ddd', \n",
    "        padding='10px',\n",
    "        overflow='auto'\n",
    "    )\n",
    ")\n",
    "# Apply direct CSS styling for text wrapping (Note: unvalidated)\n",
    "display(widgets.HTML(\"\"\"\n",
    "<style>\n",
    ".jupyter-widgets-output-area pre {\n",
    "    white-space: pre-wrap !important;       /* CSS3 */\n",
    "    word-wrap: break-word !important;        /* Internet Explorer 5.5+ */\n",
    "    overflow-wrap: break-word !important;\n",
    "    max-width: 100%;\n",
    "}\n",
    "</style>\n",
    "\"\"\"))\n",
    "control_panel = widgets.VBox(layout=widgets.Layout(width='20%', padding='10px', overflow='auto', word_wrap='break-word'))\n",
    "fetch_button = widgets.Button(description=\"Fetch Next Dataset\", button_style=\"primary\")\n",
    "progress_label = widgets.HTML(\"Waiting to start...\")\n",
    "dataset_index = 0\n",
    "\n",
    "# Function to fetch the next dataset\n",
    "def fetch_next_dataset(button=None):\n",
    "    global dataset_index\n",
    "    global dataset_metadata\n",
    "    \n",
    "    if dataset_index >= len(selected_datasets):\n",
    "        with fetch_output:\n",
    "            print(\"All datasets have been fetched!\")\n",
    "            progress_label.value = f\"<b>Completed:</b> {dataset_index}/{len(selected_datasets)} datasets\"\n",
    "        fetch_button.disabled = True\n",
    "        return\n",
    "    \n",
    "    dataset = selected_datasets[dataset_index]\n",
    "    progress_label.value = f\"<b>Fetching:</b> {dataset_index+1}/{len(selected_datasets)}<br><b>Current:</b> {dataset}\"\n",
    "    \n",
    "    with fetch_output:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Fetching dataset files for {dataset} using DOI/URL:\\n {dataset_metadata[dataset]['doi']}\")\n",
    "        ds_tree = fetch_dataset_files(dataset, max_mb=max_mb, force=force_download_checkbox.value)\n",
    "        \n",
    "        if ds_tree:\n",
    "            ds_trees[dataset] = ds_tree\n",
    "            # update metadata dict with local filesystem info\n",
    "            dataset_metadata[dataset]['output_dir'] = ds_tree['output_dir']\n",
    "            dataset_metadata[dataset]['files'] = ds_tree['files']\n",
    "            dataset_metadata[dataset]['tree'] = ds_tree['fs_tree']\n",
    "            # print the dataset file tree\n",
    "        else:\n",
    "            print(f\"Failed to fetch dataset {dataset}\")\n",
    "    \n",
    "    dataset_index += 1\n",
    "    progress_label.value = f\"<b>Completed:</b> {dataset_index}/{len(selected_datasets)}<br><b>Next:</b> {selected_datasets[dataset_index] if dataset_index < len(selected_datasets) else 'Done'}\"\n",
    "\n",
    "# Add a checkbox for force download option\n",
    "force_download_checkbox = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='Force Download',\n",
    "    tooltip='If checked, download will be forced even if files exist locally',\n",
    "    layout=widgets.Layout(width='auto')\n",
    ")\n",
    "\n",
    "# Configure the button callback\n",
    "fetch_button.on_click(fetch_next_dataset)\n",
    "\n",
    "# Create the control panel\n",
    "dataset_progress = widgets.HTML(f\"Datasets selected: {len(selected_datasets)}\")\n",
    "fetch_status = widgets.HTML(\n",
    "    f\"Status: Ready to begin\",\n",
    "    layout=widgets.Layout(margin=\"10px 0\")\n",
    ")\n",
    "\n",
    "# Create the control panel with left alignment\n",
    "control_panel.children = [\n",
    "    widgets.HTML(\"<h3 style='align:left;'>Fetch Control</h3>\"), \n",
    "    dataset_progress,\n",
    "    force_download_checkbox,\n",
    "    widgets.HTML(\"<hr style='margin:10px 0'>\"),\n",
    "    progress_label,\n",
    "    fetch_button\n",
    "]\n",
    "\n",
    "# Add custom CSS to ensure alignment\n",
    "display(widgets.HTML(\"\"\"\n",
    "<style>\n",
    ".widget-html {\n",
    "    text-align: left !important;\n",
    "}\n",
    ".widget-checkbox {\n",
    "    justify-content: flex-start !important;\n",
    "}\n",
    ".widget-button {\n",
    "    width: 100% !important;\n",
    "}\n",
    "</style>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ddc7a4",
   "metadata": {},
   "source": [
    "#### Fetching selected datasets and visualizing metadata and file structure\n",
    "\n",
    "Use the simple UI rendered below to click the \"Next Dataset\" button to initiate the fetching of the selected datasets above. \n",
    "The datasets will avoid redownloading existing files, but the user can force a re-download by checking the \"Force Re-download\" checkbox. \n",
    "\n",
    "We will store the datasets in the `DATA_PATH` variable configured in the repo's `.env` file in the `raw/` subdirectory (to denote data fetched and saved as-is or with minimal processing).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e2f6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if files and fs_tree are empty for selected datasets (ie user did not use fetch UI above or there was an error with one of the fetches)\n",
    "processed_ds_keys = ['output_dir', 'files', 'tree']\n",
    "missing_ds_fetches = [ds for ds in selected_datasets if any(key not in dataset_metadata[ds] for key in processed_ds_keys)]\n",
    "if missing_ds_fetches:\n",
    "    print(f\"Warning: The following datasets were not fetched or processed correctly:\\n{missing_ds_fetches}\")\n",
    "    print(f\"We will attempt to fetch these in a loop in the next cell, but if you did not skip the fetch step, please check the stdout above for errors.\")\n",
    "else:\n",
    "    print(f\"All {len(selected_datasets)} datasets have been fetched and processed correctly (🤞).\")\n",
    "\n",
    "if missing_ds_fetches:\n",
    "    # loop through the datasets that were not fetched and try to fetch them again\n",
    "    for dataset in missing_ds_fetches:\n",
    "        print(f\"Attempting to fetch {dataset} again...\")\n",
    "        ds_tree = fetch_dataset_files(dataset, max_mb=max_mb, force=False)\n",
    "        if ds_tree:\n",
    "            ds_trees[dataset] = ds_tree\n",
    "            # update metadata dict with local filesystem info\n",
    "            dataset_metadata[dataset]['output_dir'] = ds_tree['output_dir']\n",
    "            dataset_metadata[dataset]['files'] = ds_tree['files']\n",
    "            dataset_metadata[dataset]['tree'] = ds_tree['fs_tree']\n",
    "        else:\n",
    "            print(f\"Failed to fetch dataset {dataset} again; Error is likely in the fetch function above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8be5c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(datahugger.get.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218e707e",
   "metadata": {},
   "source": [
    "#### Global inventory of solar PV units (Kruitwagen et al, 2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee193e4",
   "metadata": {},
   "source": [
    "From Zenodo:\n",
    "```\n",
    "Repository contents:\n",
    "\n",
    "trn_tiles.geojson: 18,570 rectangular areas-of-interest used for sampling training patch data.\n",
    "\n",
    "trn_polygons.geojson: 36,882 polygons obtained from OSM in 2017 used to label training patches.\n",
    "\n",
    "cv_tiles.geojson: 560 rectangular areas-of-interest used for sampling cross-validation data seeded from WRI GPPDB\n",
    "\n",
    "cv_polygons.geojson: 6,281 polygons corresponding to all PV solar generating units present in cv_tiles.geojson at the end of 2018.\n",
    "\n",
    "test_tiles.geojson: 122 rectangular regions-of-interest used for building the test set.\n",
    "\n",
    "test_polygons.geojson: 7,263 polygons corresponding to all utility-scale (>10kW) solar generating units present in test_tiles.geojson at the end of 2018.\n",
    "\n",
    "predicted_polygons.geojson: 68,661 polygons corresponding to predicted polygons in global deployment, capturing the status of deployed photovoltaic solar energy generating capacity at the end of 2018.\n",
    "``` \n",
    "\n",
    "**Regarding predicted_polygons.geojson**: \"The final dataset includes 68,661 detections in 131 countries with a mean detection\n",
    "area of approximately 70,000m2. Country-level aggregates are shown in Supplementary\n",
    "Table 10. Any false positives remaining in the dataset are a product of human error. Final precision statistics in Supplementary Figure 6 and Supplementary Table ?? is reported\n",
    "again the test set. We find human error in hand labelling reduced final precision to approximately 98.6%.\" \n",
    "\n",
    "On the confidence column only in that file: \"Sentinel-2 and SPOT pipeline branches are combined into a final\n",
    "vector dataset using a rules-based filter. Where Sentinel-2 and SPOT polygons intersect\n",
    "with a Jaccard index (Intersection-over-union) in excess of 30%, the geometry of the SPOT\n",
    "polygon is retained, inheriting the installation date from the Sentinel-2 detection (confidence level “A”). Detections from only the SPOT and S2 branches are retained with confidence levels “B” and “C” respectively. Where the IoU does not exceed 30%, the union\n",
    "of both geometries are retained, inheriting the S2 installation date (confidence “D”).\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505e2535",
   "metadata": {},
   "source": [
    "#### France West Europe PV Installations 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d566f477",
   "metadata": {},
   "source": [
    "From [research publication](https://doi.org/10.1038/s41597-023-01951-4): \n",
    "```\n",
    "The Git repository contains the raw crowdsourcing data and all the material necessary to re-generate our training dataset and technical validation.  \n",
    "It is structured as follows: the raw subfolder contains the raw annotation data from the two annotation campaigns and the raw PV installations’ metadata.  \n",
    "The replication subfolder contains the compiled data used to generate our segmentation masks.  \n",
    "The validation subfolder contains the compiled data necessary to replicate the analyses presented in the technical validation section.\n",
    "```\n",
    "\n",
    "We will be using the `replication` subfolder to generate our PV polygons geojson file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5ecb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bespoke pre-processing for datsets not directly available in geojson or shapefile format\n",
    "# parse the point or polygon json files with geopandas, transform raw polygons or points features into proper geometry for geojson conversion\n",
    "from shapely.geometry import Polygon, Point, MultiPolygon\n",
    "import json\n",
    "\n",
    "# TODO: make function for processing of france json geometries\n",
    "\n",
    "def france_eur_pv_preprocess(ds_metadata, ds_subdir, metadata_dir='raw', crs=None, geom_type='Polygon'):\n",
    "    ds_dir = Path(ds_metadata['output_dir'])\n",
    "    data_dir = ds_dir / ds_subdir\n",
    "    metadata_file = 'raw-metadata_df.csv' if metadata_dir == 'raw' else 'metadata_df.csv'\n",
    "    metadata_file = ds_dir / metadata_dir / metadata_file\n",
    "    coords_file = \"polygon-analysis.json\" if geom_type == 'Polygon' else \"point-analysis.json\"\n",
    "    # keep files that are in the specified subdir and have the above filename\n",
    "    geom_files = [fpath for fpath in ds_metadata['files'] if fpath.startswith(data_dir) and fpath.endswith(coords_file)]\n",
    "    crs = crs or 'EPSG:4326' # default to WGS84\n",
    "\n",
    "    # load the metadata file\n",
    "    metadata_df = pd.read_csv(metadata_file)\n",
    "    print(f\"Loaded '{metadata_file.split('/')[-1]}' with {len(metadata_df)} rows\")\n",
    "\n",
    "    # load into geopandas, inspect the data, and add metadata_df to separate pd dataframe\n",
    "    raw_features = []\n",
    "    for geom_file_path in geom_files:\n",
    "        campaign_name = Path(geom_file_path).parent.name\n",
    "        print(f\"Processing {campaign_name} campaign...\")\n",
    "        \n",
    "        with open(geom_file_path, 'r') as f:\n",
    "            geom_data = json.load(f)\n",
    "        feat_types = set([f['type'] for f in geom_data])\n",
    "        print(f\"Feature types: {feat_types}\")\n",
    "    \n",
    "        for idx, feature_dict in enumerate(geom_data):\n",
    "            # Skip empty dictionaries\n",
    "            if not feature_dict:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                feature_id = feature_dict.get('id', idx) # Use index if ID is not present\n",
    "\n",
    "                # extract geometry and coordinates\n",
    "                if geom_type == 'Polygon':\n",
    "                    # feat_dict = [{'polygons': [{'points': {'x': <px_coord>, 'y': <px_coord>}, ...}]}, ...]\n",
    "                    coords = feature_dict['polygons']\n",
    "                    if isinstance(coords, list) and len(coords) > 0:\n",
    "                            # Handle multiple polygons\n",
    "                            polygons = []\n",
    "                            for poly_coords in coords:\n",
    "                                if len(poly_coords) >= 3:  # Need at least 3 points for a polygon\n",
    "                                    polygons.append(Polygon(poly_coords))\n",
    "                            \n",
    "                            if len(polygons) == 1:\n",
    "                                geometry = polygons[0]\n",
    "                            else:\n",
    "                                geometry = MultiPolygon(polygons)\n",
    "                                \n",
    "                            # Create feature dictionary with properties\n",
    "                            feature = {\n",
    "                                'id': feature_id,\n",
    "                                'campaign': campaign_name,\n",
    "                                'geometry': geometry\n",
    "                            }\n",
    "                    raw_features.append(feature)\n",
    "                elif geom_type == 'Point':\n",
    "                    # feat_dict = [{'clicks': [{'@type': 'Point', 'x': <px_coord>, 'y': <px_coord>}, ...]}, ...]\n",
    "                    coords = feature_dict['clicks']\n",
    "                    if isinstance(coords, list) and len(coords) > 0:\n",
    "                        points = []\n",
    "                        for point_coords in coords:\n",
    "                            if 'x' not in point_coords or 'y' not in point_coords:\n",
    "                                continue\n",
    "                            else:\n",
    "                                points.append(Point(point_coords['x'], point_coords['y']))\n",
    "                    raw_features.extend(points)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing feature {feature_dict}: {e}\")\n",
    "                continue\n",
    "\n",
    "    if raw_features:\n",
    "        # Convert to GeoDataFrame\n",
    "        gdf = gpd.GeoDataFrame(raw_features, crs=crs)\n",
    "        # add metadata to the gdf\n",
    "        if 'id' in gdf.columns:\n",
    "            gdf['id'] = gdf['id'].astype(str)\n",
    "        # Ensure CRS is set\n",
    "        if gdf.crs is None:\n",
    "            gdf.set_crs(crs, inplace=True)\n",
    "        elif str(gdf.crs) != crs:\n",
    "            gdf = gdf.to_crs(crs)\n",
    "        # need to add geotransform if available to convert pixel coords to lat/lon\n",
    "\n",
    "        # gdf['source_dataset'] = add in calling function\n",
    "    \n",
    "    return gdf, metadata_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503fc6be",
   "metadata": {},
   "source": [
    "### Manual dataset file curation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7659bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep subset of metadata dict for selected datasets\n",
    "selected_metadata = {ds: dataset_metadata[ds] for ds in selected_datasets}\n",
    "get_ds_files = lambda ds: dataset_metadata[ds]['files']\n",
    "get_ds_dir = lambda ds: dataset_metadata[ds]['output_dir']\n",
    "is_ds_ftype = lambda ds, fname: fname.endswith(f\".{dataset_metadata[ds]['label_fmt']}\")\n",
    "get_full_ds_path = lambda ds: DATASET_DIR / 'raw' / 'labels' / ds\n",
    "fra_ds_folder = 'replication'\n",
    "\n",
    "# TODO: refactor this to a function as it'll quickly get out of hand with more datasets and pruning required\n",
    "# make a manual selection of the set of files we'll use from each dataset\n",
    "selected_ds_files = {ds : [f for f in get_ds_files(ds) if is_ds_ftype(ds, f)] for ds in selected_datasets}\n",
    "\n",
    "# ad hoc selection of files for testing (keep files that contain 'solar' and 'WGS84' in filename)\n",
    "selected_ds_files['global_harmonized_large_solar_farms_2020'] = [f for f in selected_ds_files['global_harmonized_large_solar_farms_2020'] if 'solar' in f.split('/')[-1] and 'WGS84' in f and not os.path.isdir(f)]\n",
    "# prediction dataset was human verified thoroughly and meant for downstream applications; only use this file for now\n",
    "selected_ds_files['global_pv_inventory_sent2_spot_2021'] = [f for f in selected_ds_files['global_pv_inventory_sent2_spot_2021'] if 'predicted' in os.path.basename(f)]\n",
    "print(f\"Selected {len(selected_ds_files['global_pv_inventory_sent2_spot_2021'])} files for {selected_datasets[0]}:\\n{selected_ds_files['global_pv_inventory_sent2_spot_2021']}\")\n",
    "\n",
    "# only include files that were not filtered out\n",
    "include_files = [os.path.basename(f) for ds in selected_datasets for f in selected_ds_files[ds]]\n",
    "# don't print out unused directories\n",
    "exclude_folders = [os.path.basename(dir) for dir in os.listdir(DATASET_DIR / 'raw' / 'labels') if dir not in selected_datasets]\n",
    "\n",
    "# build and output tree for selected datasets\n",
    "selected_ds_dirs = [get_ds_dir(ds) for ds in selected_datasets]\n",
    "print(\"All selected datasets have been fetched with the following file tree:\\n\")\n",
    "# TODO: fix unwanted dirs in the tree\n",
    "selected_ds_tree = seedir(DATASET_DIR / 'raw' / 'labels', depthlimit=10, printout=True, regex=False, include_files=include_files, exclude_folders=exclude_folders)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f011a0",
   "metadata": {},
   "source": [
    "# Dataset Processing with GeoPandas and Intro to GeoParquet\n",
    "\n",
    "<div style=\"max-width: 60%; margin: 0 auto; padding-left: 2em; padding-right: 2em; text-align: justify left;\">\n",
    "    <p>GeoParquet is <a href=\"https://geoparquet.org/\">an incubating Open Geospatial Consortium (OGC) standard</a> that simply adds compatible geospatial <a href=\"https://docs.safe.com/fme/html/FME-Form-Documentation/FME-ReadersWriters/geoparquet/Geometry-Support.htm\">geometry types</a> (MultiPoint, Line, Polygon, etc) to the mature and widely adopted <a href=\"https://parquet.apache.org/\">Apache Parquet format</a>, a popular columnar storage format commonly used in big data processing and modern data engineering pipelines and analytics. Despite the geoparquet standard only just recently reaching a v1.X release, Parquet itself is a very mature file format and has a wide ecosystem that GeoParquet seamlessly integrates with. This is analogous to how the GeoTIFF raster format adds geospatial metadata to the longstanding TIFF standard. GeoParquet is designed to be a simple and efficient way to store geospatial <em>vector</em> data in a columnar format, and is designed to be compatible with existing Parquet tools and libraries to enable Cloud <em>Data Warehouse</em> Interoperability.</p>\n",
    "    <p>Parquet is a columnar storage format that is optimized for <strong>analytical workloads</strong> (i.e. not <strong>transactional</strong>) and is designed to work well with large-scale data processing frameworks like Apache Spark, Dask, and Apache Airflow. It is a <em>popular choice for storing large datasets using modern cloud-centric DBMS architectures</em> like data lakes and data warehouses, as it allows for multiple optimizations and efficient querying and analysis of data. Parquet files are <strong>designed to be highly compressed</strong>, which reduces storage costs and improves performance when reading and writing data. The columnar format allows for efficient compression algorithms to be <em>applied to each column independently</em>, resulting in better compression ratios compared to traditional row-based formats like CSV or JSON.</p>\n",
    "    <p>These files are organized in a set of file chunks called \"row groups\". Row groups are logical groups of columns with the same number of rows. Each of these columns is actually a \"column chunk\" which is a contiguous block of data for that column. The schema across row groups must be consistent, i.e. the data types and number of columns must be the same for every row group. The new geospatial standard adds some relevant additional metadata such as the geometry's Coordinate Reference System (CRS), additional metadata for geometry columns, and recent releases have enabled <a href=\"https://medium.com/radiant-earth-insights/geoparquet-1-1-coming-soon-9b72c900fbf2\">initial support for spatial indexing in v1.1</a>. <a href=\"https://towardsdatascience.com/geospatial-data-engineering-spatial-indexing-18200ef9160b\">Spatial indexing</a> is a technique used to optimize spatial queries by indexing or partitioning the data based on its geometry features such that you can make spatial queries (e.g. intersection, within, within x distance, etc) more efficiently.</p>\n",
    "</div>\n",
    "  \n",
    "  \n",
    "\n",
    "<figure style=\"text-align: center\">\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*QEQJjtnDb3JQ2xqhzARZZw.png\" style=\"width:50%; height:auto;\">\n",
    "<figcaption align = \"center\"> Visualization of the layout of a Parquet file </figcaption>\n",
    "</figure>\n",
    "\n",
    "Beyond the file data itself, Parquet also stores metadata at the end of the file that describes the internal \"chunking\" of the file, byte ranges of every column chunks, several column statistics, among other things. \n",
    "\n",
    "<figure style=\"text-align: center\">\n",
    "<img src=\"https://guide.cloudnativegeo.org/images/geoparquet_layout.png\" style=\"width:50%; height:auto;\">\n",
    "<figcaption align = \"center\"> GeoParquet has the same laylout with additional metadata </figcaption>\n",
    "</figure>\n",
    "  \n",
    "  \n",
    "\n",
    "## Features and Advantages\n",
    "\n",
    "- Efficient storage and compression: \n",
    "    - leverages the columnar data format which is more efficient for filtering on columns\n",
    "    - GeoParquet is internally compressed by default, and can be configured to optimize decompression time or storage size depending on the use case\n",
    "    - These make it ideal for applications dealing with _massive_ geospatial datasets and cloud data warehouses\n",
    "- Scalability and High-Performance:\n",
    "    - the nature of the file format is well-suited for parallel and/or distributed processing such as in Spark, Dask, or Hadoop\n",
    "    - Support for data partitioning: \n",
    "        - Parquet files can be partitioned by one or more columns\n",
    "        - In the geospatial context this enables efficient spatial queries and filtering (e.g. partitioning by ISO country code) \n",
    "        - Properly implemented spatial partitions can enable [predicate pushdown](https://medium.com/radiant-earth-insights/geoparquet-1-1-coming-soon-9b72c900fbf2) which can significantly speed up spatial queries over the network by applying filters at the storage level and greatly reducing the amount of data that needs to be transferred. See this impressive example from the [geoparquet v1.1 blog](https://medium.com/radiant-earth-insights/geoparquet-1-1-coming-soon-9b72c900fbf2)\n",
    "- Optimized for *read-heavy workflows*: \n",
    "    - Parquet is an immutable file format, which means taking advantage of cheap reads, and efficient filtering and aggregation operations\n",
    "        - This is ideal for data warehousing and modern analytic workflows \n",
    "        - Best paired with Analytical Databases like Amazon Redshift, Google BigQuery, or DuckDB\n",
    "        - Ideal for OLAP (Online Analytical Processing) and BI (Business Intelligence) workloads that leverage historical and aggregated data that don't require frequent updates\n",
    " - Interoperability and wide ecosystem:\n",
    "    - GeoParquet is designed to be compatible with existing Parquet readers, tools, and libraries\n",
    "    - Facilitates integration into existing data pipelines and workflows\n",
    "    - Broad compatibility:\n",
    "        - support for multiple spatial reference systems \n",
    "        - support for multiple geometry types and multiple geometry columns\n",
    "        - works with both planar and spherical coordinates \n",
    "        - support for 2D and 3D geometries\n",
    "        \n",
    "## Limitations and Disadvantages\n",
    "\n",
    "- Poorly suited for write-heavy workflows:\n",
    "    - Transactional and CRUD (Create, Read, Update, Delete) operations are not well-suited for Parquet files\n",
    "    - Not recommended for applications that require frequent updates or real-time data ingestion\n",
    "- Not a Silver Bullet for all geospatial data:\n",
    "    - deals only with vector data, not raster data\n",
    "    - storage and compression benefits require a certain scale of data to be realized\n",
    "    - performance overhead for small datasets\n",
    "- Limited support for spatial indexing:\n",
    "    - GeoParquet did not implement spatial indexing in the 1.0.0 release\n",
    "    - This is planned to be built-in as a BBOX struct field (4 coords in a single column) for future release in version 1.1 of the standard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac0935a",
   "metadata": {},
   "source": [
    "### GeoPandas Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f160322d",
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERLAP_THRESH = float(os.getenv('GEOM_OVERLAP_THRESHOLD', 0.8))\n",
    "# additional preprocessing specific to each dataset (mostly attaching any included metadata)\n",
    "def global_pv_inventory_spot_processing(gdf, dataset_name, output_dir, subset_bbox=None, geom_type='Polygon', rm_invalid=True):\n",
    "    all_cols = [\n",
    "        'unique_id', 'area', 'confidence', 'install_date', 'iso-3166-1', 'iso-3166-2', 'gti', 'pvout', 'capacity_mw', 'match_id', 'wdpa_10km', 'LC_CLC300_1992', 'LC_CLC300_1993',\n",
    "        'LC_CLC300_1994', 'LC_CLC300_1995', 'LC_CLC300_1996', 'LC_CLC300_1997', 'LC_CLC300_1998', 'LC_CLC300_1999', 'LC_CLC300_2000', 'LC_CLC300_2001', 'LC_CLC300_2002',\n",
    "        'LC_CLC300_2003', 'LC_CLC300_2004', 'LC_CLC300_2005', 'LC_CLC300_2006', 'LC_CLC300_2007', 'LC_CLC300_2008', 'LC_CLC300_2009', 'LC_CLC300_2010', 'LC_CLC300_2011',\n",
    "        'LC_CLC300_2012', 'LC_CLC300_2013', 'LC_CLC300_2014', 'LC_CLC300_2015', 'LC_CLC300_2016', 'LC_CLC300_2017', 'LC_CLC300_2018', 'mean_ai', 'GCR', 'eff', 'ILR',\n",
    "        'area_error', 'lc_mode', 'lc_arid', 'lc_vis', 'geometry', 'aoi_idx', 'aoi', 'id', 'Country', 'Province', 'WRI_ref', 'Polygon Source', 'Date', 'building',\n",
    "        'operator', 'generator_source', 'amenity', 'landuse', 'power_source', 'shop', 'sport', 'tourism', 'way_area', 'access', 'denomination', 'historic',\n",
    "        'leisure', 'man_made', 'natural', 'ref', 'religion', 'surface', 'z_order', 'layer', 'name', 'barrier', 'addr_housenumber', 'office', 'power',  'military'\n",
    "    ]\n",
    "    # these are not included in the prediction_set geojson: ['osm_id', 'Project', 'construction']\n",
    "    # remove unwanted columns\n",
    "    keep_cols = ['geometry', 'unique_id', 'confidence', 'install_date', 'capacity_mw', 'iso-3166-2', 'pvout']\n",
    "    # keep_cols = all_cols\n",
    "    print(f\"Filtering from {len(all_cols)} columns to {len(keep_cols)} columns:\\n{keep_cols}\")\n",
    "    gdf = gdf[keep_cols]\n",
    "    return gdf\n",
    "def global_pv_inventory_sent2_processing(gdf, dataset_name, output_dir, subset_bbox=None, geom_type='Polygon', rm_invalid=True):\n",
    "    return gdf\n",
    "def india_pv_solar_farms_processing(gdf, dataset_name, output_dir, subset_bbox=None, geom_type='Polygon', rm_invalid=True):\n",
    "    return gdf\n",
    "def usa_cali_usgs_pv_processing(gdf, dataset_name, output_dir, subset_bbox=None, geom_type='Polygon', rm_invalid=True):\n",
    "    return gdf\n",
    "def usa_eia_large_scale_pv_processing(gdf, dataset_name, output_dir, subset_bbox=None, geom_type='Polygon', rm_invalid=True):\n",
    "    return gdf\n",
    "def global_harmonized_large_solar_farms_processing(gdf, dataset_name, output_dir, subset_bbox=None, geom_type='Polygon', rm_invalid=True):\n",
    "    all_cols = ['sol_id', 'GID_0', 'panels', 'panel.area', 'landscape.area', 'urban', 'power', 'geometry']\n",
    "    # remove unwanted columns\n",
    "    keep_cols = ['sol_id', 'panels', 'panel.area', 'landscape.area', 'water', 'urban', 'power', 'geometry']\n",
    "    print(f\"Filtering from {len(all_cols)} columns to {len(keep_cols)} columns:\\n{keep_cols}\")\n",
    "    gdf = gdf[keep_cols]\n",
    "    return gdf\n",
    "\n",
    "def filter_gdf_duplicates(gdf, geom_type='Polygon', overlap_thresh=OVERLAP_THRESH):\n",
    "    \"\"\"\n",
    "    Remove duplicate geometries from a GeoDataFrame based on a specified overlap threshold,\n",
    "    keeping the geometry with the smaller area when two overlap substantially.\n",
    "    \n",
    "    Args:\n",
    "        gdf (GeoDataFrame): Input GeoDataFrame.\n",
    "        geom_type (str): Geometry type to filter by. Default is 'Polygon'.\n",
    "        overlap_thresh (float): Overlap threshold for removing duplicates. Default is 0.8.\n",
    "        \n",
    "    Returns:\n",
    "        gdf (GeoDataFrame): GeoDataFrame with duplicates removed.\n",
    "    \"\"\"\n",
    "    # First identify exact duplicates\n",
    "    gdf = gdf.drop_duplicates('geometry')\n",
    "    \n",
    "    # Identify geometries that overlap substantially\n",
    "    overlaps = []\n",
    "    # Use spatial index for efficiency\n",
    "    spatial_index = gdf.sindex\n",
    "    \n",
    "    for idx, geom in enumerate(gdf.geometry):\n",
    "        # Find potential overlaps using the spatial index\n",
    "        possible_matches = list(spatial_index.intersection(geom.bounds))\n",
    "        # Remove self from matches\n",
    "        if idx in possible_matches:\n",
    "            possible_matches.remove(idx)\n",
    "        \n",
    "        for other_idx in possible_matches:\n",
    "            other_geom = gdf.iloc[other_idx].geometry\n",
    "            if geom.intersects(other_geom):\n",
    "                # Calculate overlap percentage (relative to the smaller polygon)\n",
    "                intersection_area = geom.intersection(other_geom).area\n",
    "                min_area = min(geom.area, other_geom.area)\n",
    "                overlap_percentage = intersection_area / min_area if min_area > 0 else 0.0\n",
    "                \n",
    "                # If overlap is significant (e.g., > threshold)\n",
    "                if overlap_percentage > overlap_thresh:\n",
    "                    # Keep the geometry with the smaller area (presumably more precise)\n",
    "                    if geom.area < other_geom.area:\n",
    "                        overlaps.append(other_idx)\n",
    "                        break\n",
    "                    else:\n",
    "                        overlaps.append(idx)\n",
    "    \n",
    "    # Remove overlapping geometries\n",
    "    if overlaps:\n",
    "        print(f\"Removing {len(overlaps)} geometries with >{overlap_thresh*100}% overlap\")\n",
    "        gdf = gdf.drop(gdf.index[overlaps]).reset_index(drop=True)\n",
    "    \n",
    "    return gdf\n",
    "\n",
    "# basic processing for geojson, shapefiles, and already georeferenced data\n",
    "def process_vector_geoms(\n",
    "    geom_files, \n",
    "    dataset_name, \n",
    "    output_dir=None, \n",
    "    subset_bbox=None, \n",
    "    geom_type='Polygon', \n",
    "    rm_invalid=True, \n",
    "    dedup_geoms=False,\n",
    "    overlap_thresh=OVERLAP_THRESH, \n",
    "    out_fmt='geoparquet'):\n",
    "    \"\"\"\n",
    "    Process a GeoJSON file and return a GeoDataFrame.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the GeoJSON file.\n",
    "        dataset_name (str): Name of the dataset.\n",
    "        geom_type (str): Geometry type to filter by. Default is 'Polygon'.\n",
    "        \n",
    "    Returns:\n",
    "        gdf (GeoDataFrame): Processed GeoDataFrame.\n",
    "    \"\"\"\n",
    "    if output_dir and not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    ds_dataframes = []\n",
    "\n",
    "    for fname in geom_files:\n",
    "        # TODO: change to use geofile_regex\n",
    "        if fname.endswith('.geojson') or fname.endswith('.json') or fname.endswith('.shp') or fname.endswith('.gpkg'):\n",
    "            # Check if the file is a valid GeoJSON\n",
    "            try:\n",
    "                gdf = gpd.read_file(fname)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {os.path.relpath(fname)}: {e}\")\n",
    "                continue\n",
    "            ds_dataframes.append(gdf)\n",
    "    \n",
    "    if len(ds_dataframes) == 0:\n",
    "        print(f\"No valid GeoJSON files found in {dataset_name}.\")\n",
    "        print(f\"Skipping dataset {dataset_name}\")\n",
    "        return None\n",
    "        \n",
    "    # Concatenate all dataframes into a single GeoDataFrame\n",
    "    gdf = gpd.GeoDataFrame(pd.concat(ds_dataframes, ignore_index=True))\n",
    "    # make sure the geometry column is included and named correctly\n",
    "    if 'geometry' not in gdf.columns:\n",
    "        gdf['geometry'] = gdf.geometry\n",
    "\n",
    "    # Basic info about the dataset\n",
    "    print(f\"Loaded geodataframe with raw counts of {len(gdf)} PV installations\")\n",
    "    print(f\"Coordinate reference system: {gdf.crs}\")\n",
    "    print(f\"Available columns: {gdf.columns.tolist()}\")\n",
    "    \n",
    "    # Add dataset name as a new column\n",
    "    gdf['dataset'] = dataset_name\n",
    "    \n",
    "    # Convert to WGS84 if not already in that CRS\n",
    "    if gdf.crs is not None and gdf.crs.to_string() != 'EPSG:4326':\n",
    "        # convert to WGS84 in cases of other crs (eg NAD83 for Cali dataset)\n",
    "        gdf = gdf.to_crs(epsg=4326)\n",
    "    if subset_bbox is not None:\n",
    "        # Filter the GeoDataFrame by the georeferenced bounding box\n",
    "        gdf = gdf.cx[subset_bbox[0]:subset_bbox[2], subset_bbox[1]:subset_bbox[3]]\n",
    "    \n",
    "    # DQ and cleaning\n",
    "    # check for missing and invalid geometries\n",
    "    invalid_geoms = gdf[gdf.geometry.is_empty | ~gdf.geometry.is_valid]\n",
    "    if len(invalid_geoms) > 0 and rm_invalid:\n",
    "        print(f\"Warning: {len(invalid_geoms)} invalid or empty geometries found and will be removed.\")\n",
    "        # Optionally remove invalid geometries\n",
    "        gdf = gdf[~gdf.geometry.is_empty & gdf.geometry.is_valid].reset_index(drop=True)\n",
    "    # Eliminating duplicates and geometries that overlap too much\n",
    "    if geom_type == 'Polygon' and dedup_geoms:\n",
    "        gdf = filter_gdf_duplicates(gdf, geom_type=geom_type, overlap_thresh=overlap_thresh)\n",
    "\n",
    "    # perform any additional processing specific to the dataset for metadata and other attributes\n",
    "    if dataset_name == 'global_pv_inventory_sent2_2024':\n",
    "        print(\"Processing global_pv_inventory_sent2_2024 metadata\")\n",
    "        gdf = global_pv_inventory_sent2_processing(gdf, dataset_name, output_dir, subset_bbox=subset_bbox, geom_type=geom_type)\n",
    "    elif dataset_name == 'global_pv_inventory_sent2_spot_2021':\n",
    "        print(\"Processing global_pv_inventory_sent2_spot_2021 metadata\")\n",
    "        gdf = global_pv_inventory_spot_processing(gdf, dataset_name, output_dir, subset_bbox=subset_bbox, geom_type=geom_type)\n",
    "    elif dataset_name == 'ind_pv_solar_farms_2022':\n",
    "        print(\"Processing ind_pv_solar_farms_2022 metadata\")\n",
    "        gdf = india_pv_solar_farms_processing(gdf, dataset_name, output_dir, subset_bbox=subset_bbox, geom_type=geom_type)\n",
    "    elif dataset_name == 'usa_cali_usgs_pv_2016':\n",
    "        print(\"Processing usa_cali_usgs_pv_2016 metadata\")\n",
    "        gdf = usa_cali_usgs_pv_processing(gdf, dataset_name, output_dir, subset_bbox=subset_bbox, geom_type=geom_type)\n",
    "    elif dataset_name == 'usa_eia_large_scale_pv_2023':\n",
    "        print(\"Processing usa_eia_large_scale_pv_2023 metadata\")\n",
    "        gdf = usa_eia_large_scale_pv_processing(gdf, dataset_name, output_dir, subset_bbox=subset_bbox, geom_type=geom_type)\n",
    "    \n",
    "    # Re-project to a projected CRS (e.g. EPSG:3857) for accurate area calculations.\n",
    "    # EPSG:4326 is a geographic CRS (lat/lon in degrees) and is not suitable\n",
    "    # for calculating areas in square meters.\n",
    "    gdf_proj = gdf.geometry.to_crs(epsg=3857)\n",
    "    gdf['area_m2'] = gdf_proj.area\n",
    "\n",
    "    gdf['centroid_lon'] = gdf.geometry.centroid.x\n",
    "    gdf['centroid_lat'] = gdf.geometry.centroid.y\n",
    "    # Optionally, you can also compute bounding boxes:\n",
    "    # gdf['bbox'] = gdf.geometry.apply(lambda geom: geom.bounds)\n",
    "\n",
    "    print(f\"After filtering and cleaning, we have {len(gdf)} PV installations\")\n",
    "    print(f\"Coordinate reference system: {gdf.crs}\")\n",
    "    print(f\"Available columns: {gdf.columns.tolist()}\")\n",
    "\n",
    "    if output_dir:\n",
    "        out_path = os.path.join(output_dir, f\"{dataset_name}_processed.{out_fmt}\")\n",
    "\n",
    "        if out_fmt == 'geoparquet':\n",
    "            gdf.to_parquet(out_path, \n",
    "                index=None, \n",
    "                compression='snappy',\n",
    "                geometry_encoding='WKB', \n",
    "                write_covering_bbox=True,\n",
    "                schema_version='1.1.0')\n",
    "        else:\n",
    "            # geopackage, shapefile, or geojson\n",
    "            fmt_driver_map = {'geojson': 'GeoJSON', 'shp': 'ESRI Shapefile', 'gpkg': 'GPKG'}\n",
    "            gdf.to_file(out_path, driver=fmt_driver_map[out_fmt], index=None)\n",
    "        print(f\"Saved processed GeoDataFrame to {os.path.relpath(out_path)}\")\n",
    "    \n",
    "    return gdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15925bc7",
   "metadata": {},
   "source": [
    "### Convert to GeoParquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11057816",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(selected_datasets)\n",
    "# go through the selected datasets and process them\n",
    "for ds in selected_datasets:\n",
    "    ds_files = selected_ds_files[ds]\n",
    "    ds_dir = get_ds_dir(ds)\n",
    "    ds_tree = dataset_metadata[ds]['tree']\n",
    "    out_dir = DATASET_DIR / 'raw' / 'labels' / 'geoparquet'\n",
    "    print(f\"Processing dataset {ds} with {len(ds_files)} files in {os.path.relpath(ds_dir)}:\\n{ds_files}\")\n",
    "    ds_gdf = process_vector_geoms(\n",
    "                geom_files=ds_files,\n",
    "                dataset_name=ds,\n",
    "                output_dir=out_dir\n",
    "    )\n",
    "    if ds_gdf is not None:\n",
    "        \n",
    "        display(ds_gdf.describe())\n",
    "        # print(ds_gdf.info)\n",
    "        display(ds_gdf.sample(10))\n",
    "    # remove gdf from memory as we only neeeded it for conversion and brief inspection\n",
    "    del ds_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca02877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns for datasts\n",
    "# ind_pv_solar_farms_2022 = ['State', 'Area', 'Latitude', 'Longitude', 'fid', 'geometry', 'dataset', 'area_m2', 'centroid_lon', 'centroid_lat']\n",
    "# global_harmonized_large_solar_farms_2020 =  ['sol_id', 'GID_0', 'panels', 'panel.area', 'landscape.area', 'water', 'urban', 'power', 'geometry']\n",
    "# usa_cali_usgs_pv_2016 = ['polygon_id', 'centroid_latitude', 'centroid_longitude', 'centroid_latitude_pixels', 'centroid_longitude_pixels', 'city', 'area_pixels', 'area_meters', 'image_name', 'nw_corner_of_image_latitude', 'nw_corner_of_image_longitude', 'se_corner_of_image_latitude', 'se_corner_of_image_longitude', 'datum', 'projection_zone', 'resolution', 'jaccard_index', 'polygon_vertices_pixels', 'geometry']\n",
    "# global_pv_inventory_sent2_spot_2021 = ['unique_id', 'area', 'confidence', 'install_date', 'iso-3166-1', 'iso-3166-2', 'gti', 'pvout', 'capacity_mw', 'match_id', 'wdpa_10km', 'LC_CLC300_1992', 'LC_CLC300_1993', 'LC_CLC300_1994', 'LC_CLC300_1995', 'LC_CLC300_1996', 'LC_CLC300_1997', 'LC_CLC300_1998', 'LC_CLC300_1999', 'LC_CLC300_2000', 'LC_CLC300_2001', 'LC_CLC300_2002', 'LC_CLC300_2003', 'LC_CLC300_2004', 'LC_CLC300_2005', 'LC_CLC300_2006', 'LC_CLC300_2007', 'LC_CLC300_2008', 'LC_CLC300_2009', 'LC_CLC300_2010', 'LC_CLC300_2011', 'LC_CLC300_2012', 'LC_CLC300_2013', 'LC_CLC300_2014', 'LC_CLC300_2015', 'LC_CLC300_2016', 'LC_CLC300_2017', 'LC_CLC300_2018', 'mean_ai', 'GCR', 'eff', 'ILR', 'area_error', 'lc_mode', 'lc_arid', 'lc_vis', 'geometry', 'key', 'resolution', 'pad', 'tilesize', 'zone', 'cs_code', 'ti', 'tj', 'proj4', 'wkt', 'ISO_A2', 'ISO_A3', 'idx', 'aoi_idx', 'aoi', 'id', 'Country', 'Province', 'Project', 'WRI_ref', 'Polygon Source', 'Date', 'building', 'operator', 'generator_source', 'amenity', 'landuse', 'power_source', 'shop', 'sport', 'tourism', 'way_area', 'access', 'construction', 'denomination', 'historic', 'leisure', 'man_made', 'natural', 'ref', 'religion', 'surface', 'z_order', 'layer', 'name', 'barrier', 'addr_housenumber', 'office', 'power', 'osm_id', 'military']\n",
    "\n",
    "# could include power/capacity fields and leave as NULL for the others\n",
    "# consolidated = [unify_ids(fid, sol_id/GID_0, polygon_id, unique_id), 'area_m2', 'centroid_lon', 'centroid_lat', 'dataset', 'geometry', 'bbox']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d34ad9",
   "metadata": {},
   "source": [
    "### DuckDB and/or GeoPandas dataset consolidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ac8c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import duckdb\n",
    "from typing import Optional, List\n",
    "\n",
    "def geom_db_consolidate_dataset(\n",
    "    parquet_files: List[str],\n",
    "    out_db_file: str,\n",
    "    table_name: str = \"global_consolidated_pv\",\n",
    "    geom_column: str = \"geometry\",\n",
    "    spatial_index: bool = True,\n",
    "    out_parquet: Optional[str] = None,\n",
    "    printout: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Read a list of GeoParquet files into DuckDB, keeps only a consolidated list \n",
    "    of columns, union these geoparquets into a single table, and filters out duplicate\n",
    "    geometries based on spatial overlap. \n",
    "    \n",
    "    For each file, the following columns are selected:\n",
    "      - unified_id: md5 hash of (COALESCE(fid, sol_id, GID_0, polygon_id, unique_id) + dataset)\n",
    "      - area_m2\n",
    "      - centroid_lon\n",
    "      - centroid_lat\n",
    "      - dataset (derived from the filename)\n",
    "      - bbox: bounding box of the geometry written by geopandas during export to geoparquet\n",
    "      - geometry (converted with ST_GeomFromWKB)\n",
    "\n",
    "    Args:\n",
    "        parquet_files: list of paths to input parquet files.\n",
    "        out_db_file: path to DuckDB database file.\n",
    "        table_name: name of the consolidated table in DuckDB.\n",
    "        geom_column: name of the geometry column in the parquet files.\n",
    "        spatial_index: if True, create a spatial index on the geometry column.\n",
    "        out_parquet: optional path to write consolidated table out as a GeoParquet file.\n",
    "        printout: if True, prints statistics (counts, area distribution, bounding box).\n",
    "    \"\"\"\n",
    "    # remove any existing database file (temporary workaround for failing to load existing db file before loading spatial extension)\n",
    "    if os.path.exists(out_db_file):\n",
    "        os.remove(out_db_file)\n",
    "\n",
    "    print(f\"Connecting to DuckDB database: {out_db_file}\")\n",
    "    conn = duckdb.connect(database=out_db_file, read_only=False)\n",
    "    conn.install_extension(\"spatial\")\n",
    "    conn.load_extension(\"spatial\")\n",
    "    \n",
    "    # drop any existing table\n",
    "    conn.execute(f\"DROP TABLE IF EXISTS {table_name};\")\n",
    "    \n",
    "    scans = []\n",
    "    print(f\"Building temp UNION query from {len(parquet_files)} parquet files...\")\n",
    "    for path in parquet_files:\n",
    "        print(f\"  Processing file: {os.path.basename(path)}\")\n",
    "        # Derive a dataset name from the parquet file's basename\n",
    "        dataset_name = os.path.basename(str(path)).split('.')[0]\n",
    "        import pyarrow.parquet as pq\n",
    "\n",
    "        # Read the parquet schema to get available column names\n",
    "        schema = pq.read_schema(str(path))\n",
    "        cols = set(schema.names)\n",
    "        required_cols = {'area_m2', 'centroid_lon', 'centroid_lat', 'bbox', geom_column}\n",
    "        # print(f\"Available columns in {dataset_name}: \\n{cols}\")\n",
    "\n",
    "        # Check for required columns\n",
    "        missing_cols = required_cols - cols\n",
    "        if missing_cols:\n",
    "            print(f\"    WARNING: Skipping file {path_str}. Missing required columns: {missing_cols}\")\n",
    "            continue\n",
    "        \n",
    "        uid_candidates = ['fid', 'sol_id', 'GID_0', 'polygon_id', 'unique_id']\n",
    "        present_uids = [f\"CAST({uid} AS VARCHAR)\" for uid in uid_candidates if uid in cols]\n",
    "\n",
    "        # If none of the uid columns are present, use NULL\n",
    "        if not present_uids:\n",
    "            present_uids = [\"NULL\"]\n",
    "\n",
    "        uid_expr = \"COALESCE(\" + \", \".join(present_uids) + \", 'NO_ID_' || ROW_NUMBER() OVER ())\" # Add fallback\n",
    "\n",
    "        # TODO: select all columns and still perform geometry conversion\n",
    "        scan_query = f\"\"\"\n",
    "            SELECT \n",
    "              md5(concat_ws('_', {uid_expr}, '{dataset_name}')) AS unified_id,\n",
    "              area_m2,\n",
    "              centroid_lon,\n",
    "              centroid_lat,\n",
    "              '{dataset_name}' AS dataset,\n",
    "              bbox, \n",
    "              ST_GeomFromWKB(TRY_CAST({geom_column} AS WKB_BLOB)) AS geometry\n",
    "            FROM read_parquet('{str(path)}')\n",
    "        \"\"\"\n",
    "        scans.append(scan_query)\n",
    "\n",
    "    if not scans:\n",
    "        print(\"No valid parquet files found. Exiting.\")\n",
    "        conn.close()\n",
    "        return None, None\n",
    "    \n",
    "    union_sql = \"\\nUNION ALL\\n\".join(scans)\n",
    "    # create a temporary table with the unioned results\n",
    "    print(\"Creating temporary table 'tmp' with combined data...\")\n",
    "    try:\n",
    "        conn.execute(f\"CREATE TEMPORARY TABLE tmp AS {union_sql};\")\n",
    "        count_tmp = conn.execute(\"SELECT COUNT(*) FROM tmp\").fetchone()[0]\n",
    "        print(f\"Temporary table 'tmp' created with raw count (before dedup) of {count_tmp} records.\")\n",
    "        if count_tmp == 0:\n",
    "            print(\"ERROR: Temporary table is empty after UNION. Check input files and queries.\")\n",
    "            conn.close()\n",
    "            return None, None\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR creating temporary table: {e}\")\n",
    "        print(\"UNION SQL used:\")\n",
    "        print(union_sql)\n",
    "        conn.close()\n",
    "        return None, None\n",
    "\n",
    "    t1, t2 = None, None\n",
    "    # optionally create a spatial index on the temporary table\n",
    "    if spatial_index:\n",
    "        t1 = time.time()\n",
    "        conn.execute(f\"CREATE INDEX tmp_pv_idx ON tmp USING RTREE (geometry);\")\n",
    "        t2 = time.time()\n",
    "        print(f\"Spatial index created on temporary table (used for dedup) in {t2 - t1:.2f} seconds.\")\n",
    "    \n",
    "    print(f\"Creating final table '{table_name}' by filtering duplicates...\")\n",
    "    \n",
    "    # add a unique row identifier within the temp table in edge case where unified_id is not unique\n",
    "    cte1 = f\"\"\"\n",
    "        SELECT\n",
    "            ROW_NUMBER() OVER () as temp_row_id,\n",
    "            *\n",
    "        FROM tmp\n",
    "    \"\"\"\n",
    "\n",
    "    # select distinct geometries first to reduce comparisons if exact duplicates exist\n",
    "    # this assumes unified_id might not be unique per geometry (initially!) \n",
    "    cte2 = f\"\"\"\n",
    "        SELECT DISTINCT ON (geometry) *\n",
    "        FROM NumberedSource\n",
    "        ORDER BY geometry, unified_id -- Ensure deterministic selection for exact duplicates\n",
    "    \"\"\"\n",
    "\n",
    "    # filter out duplicates based on spatial intersection\n",
    "    # dedup_query = f\"\"\"\n",
    "    # CREATE TABLE {table_name} AS\n",
    "    # WITH NumberedSource AS (\n",
    "    #     {cte1}\n",
    "    # ), IndexedSource AS (\n",
    "    #     {cte2}\n",
    "    # )\n",
    "    \n",
    "    # SELECT DISTINCT ON (a.temp_row_id) a.* EXCLUDE (temp_row_id) -- Select distinct rows based on temp_row_id\n",
    "    # FROM IndexedSource a\n",
    "    # LEFT JOIN IndexedSource b\n",
    "    # ON a.temp_row_id != b.temp_row_id -- Ensure not comparing a row to itself\n",
    "    #    AND a.unified_id > b.unified_id -- Deterministic way to pick which one to keep\n",
    "    #    AND ST_Intersects(a.geometry, b.geometry)\n",
    "    #    AND ST_Area(ST_Intersection(a.geometry, b.geometry)) / ST_Area(a.geometry) > {OVERLAP_THRESH}\n",
    "    # WHERE b.temp_row_id IS NULL; -- Keep 'a' if no 'better' overlapping geometry 'b' was found\n",
    "    # \"\"\"\n",
    "\n",
    "    # V2\n",
    "    # dedup_query = f\"\"\"\n",
    "    #     CREATE TABLE {table_name} AS\n",
    "    #     WITH TmpWithId AS (\n",
    "    #         -- Add row_id to the already filtered tmp table for reliable self-join\n",
    "    #         SELECT ROW_NUMBER() OVER () as temp_row_id, * FROM tmp\n",
    "    #     ), OverlapPairs AS (\n",
    "    #         -- Identify pairs (a, b) where 'a' significantly overlaps 'b', and 'a' has a lower unified_id\n",
    "    #         SELECT\n",
    "    #             b.temp_row_id as b_id_to_remove -- Select the ID of the row to potentially remove\n",
    "    #         FROM TmpWithId a\n",
    "    #         JOIN TmpWithId b ON a.temp_row_id != b.temp_row_id -- Ensure not joining row to itself\n",
    "    #             AND a.unified_id < b.unified_id -- Deterministic: 'a' is preferred over 'b'\n",
    "    #             AND ST_Intersects(a.geometry, b.geometry) -- Use spatial index on tmp efficiently\n",
    "    #             -- Check overlap relative to 'b's area. Assume ST_Area(b.geometry) > 0 due to initial filtering.\n",
    "    #             AND (ST_Area(ST_Intersection(a.geometry, b.geometry)) / ST_Area(b.geometry)) > {OVERLAP_THRESH}\n",
    "    #     ), RowsToRemove AS (\n",
    "    #         -- Get the unique set of temp_row_ids that were identified as 'b' in OverlapPairs\n",
    "    #         SELECT DISTINCT b_id_to_remove FROM OverlapPairs\n",
    "    #     )\n",
    "    #     -- Select rows from the temp table (with added ID) that are NOT marked for removal\n",
    "    #     -- Then, handle any remaining *exact* geometry duplicates by keeping the one with the lowest unified_id\n",
    "    #     SELECT DISTINCT ON (t.geometry) t.* EXCLUDE (temp_row_id)\n",
    "    #     FROM TmpWithId t\n",
    "    #     LEFT JOIN RowsToRemove r ON t.temp_row_id = r.b_id_to_remove\n",
    "    #     WHERE r.b_id_to_remove IS NULL -- Keep only rows that are NOT in the removal set\n",
    "    #     ORDER BY t.geometry, t.unified_id; -- Required for DISTINCT ON: keep the first unified_id for exact geom duplicates\n",
    "    # \"\"\"\n",
    "    # backup if spatial dedup is much slower than geopandas filtering performed above\n",
    "    dedup_query = f\"\"\"\n",
    "        CREATE TABLE {table_name} AS\n",
    "        SELECT DISTINCT ON (geometry) *\n",
    "        FROM tmp\n",
    "        ORDER BY geometry, unified_id; -- Required for DISTINCT ON: keep the first unified_id for exact geom duplicates\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Executing deduplication query...\")\n",
    "    # print(f\"Deduplication Query:\\n{dedup_query}\") # Uncomment for debugging\n",
    "    t1 = time.time()\n",
    "    conn.execute(dedup_query)\n",
    "    t2 = time.time()\n",
    "    print(f\"Deduplication using spatial index and spatial functions completed in {t2 - t1:.2f} seconds.\")\n",
    "    count_final = conn.execute(f\"SELECT COUNT(*) FROM {table_name}\").fetchone()[0]\n",
    "    print(f\"Final table '{table_name}' created with {count_final} deduplicated rows.\")\n",
    "    \n",
    "    if spatial_index:\n",
    "        t1 = time.time()\n",
    "        conn.execute(f\"CREATE INDEX {table_name}_geom_idx ON {table_name} USING RTREE (geometry);\")\n",
    "        t2 = time.time()\n",
    "        print(f\"Spatial index created on '{table_name}' in {t2 - t1:.2f} seconds.\")\n",
    "    \n",
    "    if printout:\n",
    "        # Area distribution (min, max, avg)\n",
    "        area_stats = conn.execute(\n",
    "            f\"SELECT MIN(area_m2), MAX(area_m2), AVG(area_m2) FROM {table_name}\"\n",
    "        ).fetchone()\n",
    "        # Bounding box based on centroidsx``\n",
    "        bbox = conn.execute(\n",
    "            f\"SELECT MIN(centroid_lon), MAX(centroid_lon), MIN(centroid_lat), MAX(centroid_lat) FROM {table_name}\"\n",
    "        ).fetchone()\n",
    "        print(f\"Table '{table_name}' contains {count_final} records.\")\n",
    "        print(f\"Area (m²): min = {area_stats[0]}, max = {area_stats[1]}, avg = {area_stats[2]}\")\n",
    "        print(f\"Centroid extent: lon [{bbox[0]}, {bbox[1]}], lat [{bbox[2]}, {bbox[3]}]\")\n",
    "    \n",
    "    # Optionally export consolidated table to geoparquet\n",
    "    if out_parquet:\n",
    "        # make sure the output directory exists\n",
    "        os.makedirs(os.path.dirname(out_parquet), exist_ok=True)\n",
    "        # DuckDB can export tables or query results to parquet using COPY\n",
    "        conn.execute(f\"COPY {table_name} TO '{out_parquet}' (FORMAT 'parquet', COMPRESSION 'snappy', ROW_GROUP_SIZE 10000);\")\n",
    "        print(f\"Exported consolidated table to {out_parquet}\")\n",
    "\n",
    "    conn.close()\n",
    "    print(f\"Consolidated {len(parquet_files)} files into {out_db_file} → table '{table_name}'\")\n",
    "    if printout:\n",
    "        print(f\"Exported to: {out_parquet}\")\n",
    "\n",
    "    return out_db_file, table_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eac4643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of geoparquet files to be consolidated\n",
    "get_full_gpq_path = lambda f: DATASET_DIR / 'raw' / 'labels' / 'geoparquet' / f\n",
    "parquet_files = [get_full_gpq_path(f) for f in os.listdir(DATASET_DIR / 'raw' / 'labels' / 'geoparquet') if any(os.path.splitext(f)[0].startswith(ds) for ds in selected_datasets)]\n",
    "flist = '\\n-'.join([os.path.relpath(f) for f in parquet_files])\n",
    "print(f\"Consolidating these {len(parquet_files)} files:\\n-{flist}\")\n",
    "out_db_dir = DATASET_DIR / 'prepared' / 'labels' \n",
    "out_consolidated_parquet = out_db_dir / 'geoparquet' / 'global_consolidated_pv.geoparquet'\n",
    "out_consolidated_db = out_db_dir / 'db' / 'global_consolidated_pv.duckdb'\n",
    "# create the output directories if they don't exist\n",
    "print(f\"Creating output directories: {out_consolidated_db.parent}\") \n",
    "os.makedirs(out_consolidated_db.parent, exist_ok=True)\n",
    "print(f\"Creating output directories: {out_consolidated_parquet.parent}\")\n",
    "os.makedirs(out_consolidated_parquet.parent, exist_ok=True)\n",
    "# consolidate the dataset into a single duckdb database that will also be saved as a geoparquet file\n",
    "db_file, table_name = geom_db_consolidate_dataset(\n",
    "    parquet_files=parquet_files,\n",
    "    out_db_file=out_consolidated_db,\n",
    "    table_name=\"global_consolidated_pv\",\n",
    "    geom_column=\"geometry\",\n",
    "    spatial_index=True,\n",
    "    out_parquet=out_consolidated_parquet,\n",
    "    printout=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079e3e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load consolidated geoparquet into geodataframe to use in visualizations\n",
    "ds_gdf = gpd.read_parquet(out_consolidated_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc8613e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(ds_gdf.describe())\n",
    "display(ds_gdf.sample(10))\n",
    "ds = 'global_consolidated_pv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e52b66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform geopandas dedup and check size again\n",
    "ds_gdf = filter_gdf_duplicates(ds_gdf)\n",
    "print(f\"After filtering and cleaning, we have {len(ds_gdf)} PV installations\")\n",
    "display(ds_gdf.describe())\n",
    "display(ds_gdf.sample(10))\n",
    "# save the consolidated geoparquet file in our prepared directory\n",
    "consolidated_dedup_parquet = out_db_dir / 'geoparquet' / 'global_consolidated_pv_dedup.geoparquet'\n",
    "# drop existing bbox column to recalculate with reduced geometries\n",
    "if 'bbox' in ds_gdf.columns:\n",
    "    ds_gdf = ds_gdf.drop(columns=['bbox'])\n",
    "# save the processed geoparquet file\n",
    "ds_gdf.to_parquet(consolidated_dedup_parquet, \n",
    "    index=None, \n",
    "    compression='snappy',\n",
    "    geometry_encoding='WKB', \n",
    "    write_covering_bbox=True,\n",
    "    schema_version='1.1.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ceed0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from maxar_platform.api_keys import generate_key\n",
    "# key = generate_key(\"my new key\")\n",
    "# print(key['api_key'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4343995f",
   "metadata": {},
   "source": [
    "# Fetching vector layers for Spatio-temporal context\n",
    "\n",
    "To better manage our *global* PV datasets for clustered STAC searches, we will also fetch some vector layers that enable us to organize the datasets by country, continent, and provide other spatio-temporal context relevant to our research.\n",
    "\n",
    "**NOTE**: Fetching Overture divisions locally will result in a **~6.4GB file**!; Be aware that you can use blob storage with duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128fab2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# id\tstring\tA feature ID. This may be an ID associated with the Global Entity Reference System (GERS) if—and-only-if the feature represents an entity that is part of GERS.\n",
    "# geometry\tblob\tA WKB representation of the entity's geometry - a Point, Polygon, MultiPolygon, or LineString.\n",
    "# bbox\tarray\tThe bounding box of an entity's geometry, represented with float values, in a xmin, xmax, ymin, ymax format.\n",
    "# version\tinteger\tVersion number of the feature, incremented in each Overture release where the geometry or attributes of this feature changed.\n",
    "# sources\tarray\tThe array of source information for the properties of a given feature, with each entry being a source object which lists the property in JSON Pointer notation and the dataset that specific value came from. All features must have a root level source which is the default source if a specific property's source is not specified.\n",
    "# subtype\tstring\tCategory of the division from a finite, hierarchical, ordered list of categories (e.g. country, region, locality, etc.) similar to a Who's on First placetype.\n",
    "# wikidata\tstring\tA wikidata ID if available, as found on https://www.wikidata.org/.\n",
    "# population\tinteger\tPopulation of the division.\n",
    "# names\tarray\tA primary name of the entity, and a set of optional name translations. Name translations are represented in key, value pairs, where the key is an ISO language code and the value is the translated name.\n",
    "# class\tstring\tA value to represent whether an entity represents a maritime or land feature.\n",
    "# division_ids\tlist\tA list of the two division IDs that share this division boundary.\n",
    "# is_disputed\tboolean\tIndicator if there are entities disputing this division boundary. Information about entities disputing this boundary should be included in perspectives property. This property should also be true if boundary between two entities is unclear and this is \"best guess\". So having it true and no perspectives gives map creators reason not to fully trust the boundary, but use it if they have no other.\n",
    "# perspectives\tarray\tPolitical perspectives from which this division boundary is considered to be an accurate representation. If this property is absent, then this boundary is not known to be disputed from any political perspective. Consequently, there is only one boundary feature representing the entire real world entity. If this property is present, it means the boundary represents one of several alternative perspectives on the same real-world entity.\n",
    "# local_type\tstring\tLocal name for the subtype property, optionally localized. This property is localized using a standard Overture names structure.\n",
    "# country\tstring\tISO 3166-1 alpha-2 country code of the country or country-like entity, that this division represents or belongs to. If the entity this division represents has a country code, the country property contains it. If it does not, the country property contains the country code of the first division encountered by traversing the parent_division_id chain to the root.\n",
    "# region\tstring\tISO 3166-2 principal subdivision code of the subdivision-like entity this division represents or belongs to. If the entity this division represents has a principal subdivision code, the region property contains it. If it does not, the region property contains the principal subdivision code of the first division encountered by traversing the parent_division_id chain to the root.\n",
    "# hierarchies\tArray\tHierarchies in which this division participates.\n",
    "# parent_division_id\tstring\tDivision ID of this division's parent division. Not allowed for top-level divisions (countries) and required for all other divisions. The default parent division is the parent division as seen from the default political perspective, if there is one, and is otherwise chosen somewhat arbitrarily. The hierarchies property can be used to inspect the exhaustive list of parent divisions.\n",
    "# norms\tlist\tCollects information about local norms and rules within the division that are generally useful for mapping and map-related use cases. If the norms property or a desired sub-property of the norms property is missing on a division, but at least one of its ancestor divisions has the norms property and the desired sub-property, then the value from the nearest ancestor division may be assumed.\n",
    "# capital_division_ids\tarray\tDivision IDs of this division's capital divisions. If present, this property will refer to the division IDs of the capital cities, county seats, etc. of a division.\n",
    "# capital_of_divisions\tlist\tDivision ID of the division that this feature is the capital of. If present, this property will refer to the division IDs of a parent county, region, country, etc.\n",
    "# division_id\tstring\tDivision ID of the division this area belongs to.\n",
    "# cartography\tarray\tContains a prominence property, which offers a suggestion for displaying Overture features at specific zoom levels based on it's importance and significance.\n",
    "# is_land\tboolean\tIndicates whether or not the feature geometry represents the non-maritime \"land\" boundary, which can be used for map rendering, cartographic display, and similar purposes.\n",
    "# is_territorial\tboolean\tIndicates whether or not the feature geometry represents the full territorial boundary or claim of a feature.\n",
    "# filename\tstring\tName of the S3 file being queried.\n",
    "# theme\tstring\tName of the Overture theme being queried.\n",
    "# type\tstring\tName of the Overture feature type being queried.\n",
    "\n",
    "# country\n",
    "# dependency\n",
    "# region\n",
    "# county\n",
    "# localadmin\n",
    "# locality\n",
    "# macrohood\n",
    "# neighborhood\n",
    "# subtype\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765b666d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OVERTURE_DIVISION_AREA_S3 = os.getenv(\"OVERTURE_DIVISIONS_S3\")\n",
    "\n",
    "# # use duckdb to fetch overture's division geoparquets from s3 and save to our raw geoparquet labels\n",
    "\n",
    "# def fetch_overture_division_areas(\n",
    "#     s3_path: str,\n",
    "#     out_gpq_path: str, # can be local or blob storage\n",
    "#     s3_region: str = \"us-east-1\",\n",
    "#     geom_col: str = \"geometry\",\n",
    "#     subset_bbox: Optional[List[float]] = None,\n",
    "# ):\n",
    "# \"\"\"\n",
    "# Fetches Overture division area geoparquet files from S3 and saves them to a local path.\n",
    "# \"\"\"\n",
    "#     print(f\"Fetching Overture division areas from S3:\\n {s3_path}\")\n",
    "#     print(f\"WIll be saved to:\\n {out_gpq_path}\")\n",
    "\n",
    "#     conn = None\n",
    "#     try:\n",
    "#         # use in0-memory connection since we'll only be copying the files over\n",
    "#         conn = duckdb.connect(database=':memory:', read_only=True)\n",
    "#         print(\"  Loading DuckDB extensions...\")\n",
    "#         conn.execute(\"INSTALL httpfs;\")\n",
    "#         conn.execute(\"LOAD httpfs;\")\n",
    "#         conn.execute(\"INSTALL spatial;\")\n",
    "#         conn.execute(\"LOAD spatial;\")\n",
    "#         # Configure S3 access (anonymous for public Overture data)\n",
    "#         conn.execute(\"SET s3_use_ssl=true;\")\n",
    "#         conn.execute(f\"SET s3_region='{s3_region}';\") \n",
    "\n",
    "#         overture_file_cnt = conn.execute()\n",
    "#         copy_sql = f\"\"\"\n",
    "#         COPY (\n",
    "#             SELECT\n",
    "#                 *\n",
    "#             FROM\n",
    "#                 read_parquet('{s3_path}', hive_partitioning=1)\n",
    "#         ) TO '{out_gpq_path}';\n",
    "#         \"\"\"\n",
    "#         conn.execute(copy_sql)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # and perform a spatial join with our polygon data\n",
    "# def spatial_join_pv_overture_duckdb(\n",
    "#     pv_gpq_path: str,\n",
    "#     ov_divisions_gpq_path: str,\n",
    "#     out_gpq_path: str,\n",
    "#     pv_geom_col: str = \"geometry\",\n",
    "#     overture_geom_col: str = \"geometry\",\n",
    "#     join_predicate: str = \"ST_Within\", # Or ST_Intersects, ST_Contains etc.\n",
    "#     overture_cols_to_keep: list[str] = [\"id\", \"country\", \"region\", \"locality\", \"sublocality\", \"localityType\", \"isoCountryCodeAlpha2\"]\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Performs a spatial join between a local PV GeoParquet file and Overture\n",
    "#     division areas directly from S3 using DuckDB.\n",
    "\n",
    "#     Args:\n",
    "#         pv_gpq_path: Path to the input GeoParquet file containing PV polygons.\n",
    "#         ov_divisions_gpq_path: S3 glob path to the Overture division area Parquet files.\n",
    "#         out_gpq_path: Path to save the resulting joined GeoParquet file.\n",
    "#         pv_geom_col: Name of the geometry column in the PV data.\n",
    "#         overture_geom_col: Name of the geometry column in the Overture data.\n",
    "#         join_predicate: DuckDB spatial function for the join (e.g., 'ST_Within').\n",
    "#         overture_cols_to_keep: List of columns to keep from the Overture data.\n",
    "#     \"\"\"\n",
    "#     print(f\"Starting DuckDB spatial join...\")\n",
    "#     print(f\"  PV Data in: {os.ppqth(pv_geoparquet_path)}\")\n",
    "#     print(f\"  Overture Data: {ov_divisions_gpq_path}\")\n",
    "#     print(f\"  Output: {output_papqh}\")\n",
    "\n",
    "    \n",
    "\n",
    "#         # Ensure output directory exists\n",
    "#         os.makedirs(os.path.dirname(output_geoparquet_path), exist_ok=True)\n",
    "\n",
    "#         print(\"  Constructing and executing SQL query...\")\n",
    "\n",
    "#         # Ensure overture columns are distinct if overlaps exist in pv data\n",
    "#         select_overture_cols = \", \".join([f\"ov.{col}\" for col in overture_cols_to_keep])\n",
    "\n",
    "#         # Assumes geometries are stored as WKB (common for GeoParquet)\n",
    "#         # If not, adjust the ST_GeomFromWKB part accordingly\n",
    "#         sql = f\"\"\"\n",
    "#         COPY (\n",
    "#             SELECT\n",
    "#                 pv.*,\n",
    "#                 {select_overture_cols}\n",
    "#             FROm pqrquet('{pv_geoparquet_path}') AS pv\n",
    "#             LEFT JOIN read_parquet('{ov_divisions_gpq_path}', hive_partitioning=1) AS ov\n",
    "#             ONoipqte}(\n",
    "#                 ST_GeomFromWKB(pv.\"{pv_geom_col}\"),\n",
    "#                 ST_GeomFromWKB(ov.\"{overture_geom_col}\")\n",
    "#             )\n",
    "#         ) TO '{output_geoparquet_path}' (FORMAT 'PARQUET');\n",
    "#         \"\"\"\n",
    "#         # print(f\"  SQL Query:\\n{sql}\") # Uncomment to debug query\n",
    "#         conn.execute(sql)\n",
    "#         print(f\"  Successfully executed join and saved results to {output_geoparquet_path}\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"An error occurred during DuckDB join: {e}\")\n",
    "#     finally:\n",
    "#         if conn:\n",
    "#             conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f01ce6",
   "metadata": {},
   "source": [
    "# Visualization Functions for PV Data\n",
    "\n",
    "After processing the datasets into standardized geoparquet format, we'll use the following visualization libraries to explore and present the data:\n",
    "\n",
    "- **Folium**: For interactive web maps with various basemaps and markers\n",
    "- **Pydeck**: For high-performance 3D and large-scale visualizations\n",
    "- **Lonboard**: For GPU-accelerated geospatial visualization of large datasets\n",
    "\n",
    "Each library has specific strengths that we'll leverage for different visualization needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf64feb",
   "metadata": {},
   "source": [
    "## Folium Visualization Functions\n",
    "\n",
    "Folium is excellent for creating interactive web maps with various basemaps and markers. It's particularly useful for visualizing geographic distributions and creating choropleth maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3987442e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from folium.plugins import HeatMap, MarkerCluster\n",
    "\n",
    "def create_folium_cluster_map(gdf, zoom_start=19, title=\"PV Installation Clusters\"):\n",
    "    \"\"\"\n",
    "    Create a cluster map of PV installations using Folium.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gdf : GeoDataFrame\n",
    "        GeoDataFrame containing PV installation data with geometry column\n",
    "    zoom_start : int\n",
    "        Initial zoom level for the map\n",
    "    title : str\n",
    "        Title for the map\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    folium.Map\n",
    "        Interactive Folium map with clustered markers\n",
    "    \"\"\"\n",
    "    # Ensure the GeoDataFrame is in WGS84 (EPSG:4326) for Folium compatibility\n",
    "    if gdf.crs != \"EPSG:4326\":\n",
    "        gdf = gdf.to_crs(\"EPSG:4326\")\n",
    "    \n",
    "    # Get centroid of all points to center the map\n",
    "    center_lat = gdf.geometry.centroid.y.mean()\n",
    "    center_lon = gdf.geometry.centroid.x.mean()\n",
    "    \n",
    "    # Create base map\n",
    "    m = folium.Map(location=[center_lat, center_lon], zoom_start=zoom_start,\n",
    "                  tiles='CartoDB positron')\n",
    "    \n",
    "    # Add title\n",
    "    title_html = f'''\n",
    "             <h3 align=\"center\" style=\"font-size:16px\"><b>{title}</b></h3>\n",
    "             '''\n",
    "    m.get_root().html.add_child(folium.Element(title_html))\n",
    "    \n",
    "    # Add marker cluster\n",
    "    marker_cluster = MarkerCluster().add_to(m)\n",
    "    \n",
    "    # Add markers for each PV installation\n",
    "    for idx, row in gdf.iterrows():\n",
    "        # Get the centroid if the geometry is a polygon\n",
    "        if row.geometry.geom_type in ['Polygon', 'MultiPolygon']:\n",
    "            centroid = row.geometry.centroid\n",
    "            popup_text = f\"ID: {idx}\"\n",
    "            \n",
    "            # Add additional information if available in the dataframe\n",
    "            for col in ['capacity_mw', 'area_m2', 'installation_date', 'dataset', 'cen']:\n",
    "                if col in gdf.columns:\n",
    "                    popup_text += f\"<br>{col}: {row[col]}\"\n",
    "            \n",
    "            folium.Marker(\n",
    "                location=[centroid.y, centroid.x],\n",
    "                popup=folium.Popup(popup_text, max_width=300),\n",
    "                icon=folium.Icon(color='green', icon='solar-panel', prefix='fa')\n",
    "            ).add_to(marker_cluster)\n",
    "    \n",
    "    return m\n",
    "\n",
    "def create_folium_choropleth(gdf, column, bins=8, cmap='YlOrRd', \n",
    "                             title=\"PV Installation Density\"):\n",
    "    \"\"\"\n",
    "    Create a choropleth map of PV installations using Folium.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gdf : GeoDataFrame\n",
    "        GeoDataFrame containing PV installation data with geometry column\n",
    "    column : str\n",
    "        Column name to use for choropleth coloring\n",
    "    bins : int\n",
    "        Number of bins for choropleth map\n",
    "    cmap : str\n",
    "        Matplotlib colormap name\n",
    "    title : str\n",
    "        Title for the map\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    folium.Map\n",
    "        Interactive Folium choropleth map\n",
    "    \"\"\"\n",
    "    # Ensure the GeoDataFrame is in WGS84 (EPSG:4326)\n",
    "    if gdf.crs != \"EPSG:4326\":\n",
    "        gdf = gdf.to_crs(\"EPSG:4326\")\n",
    "    \n",
    "    # Get centroid of all points to center the map\n",
    "    center_lat = gdf.geometry.centroid.y.mean()\n",
    "    center_lon = gdf.geometry.centroid.x.mean()\n",
    "    \n",
    "    # Create base map\n",
    "    m = folium.Map(location=[center_lat, center_lon], zoom_start=3,\n",
    "                  tiles='CartoDB positron')\n",
    "    \n",
    "    # Add title\n",
    "    title_html = f'''\n",
    "             <h3 align=\"center\" style=\"font-size:16px\"><b>{title}</b></h3>\n",
    "             '''\n",
    "    m.get_root().html.add_child(folium.Element(title_html))\n",
    "    \n",
    "    # Create choropleth layer\n",
    "    folium.Choropleth(\n",
    "        geo_data=gdf,\n",
    "        name='choropleth',\n",
    "        data=gdf,\n",
    "        columns=[gdf.index.name if gdf.index.name else 'index', column],\n",
    "        key_on='feature.id',\n",
    "        fill_color=cmap,\n",
    "        fill_opacity=0.7,\n",
    "        line_opacity=0.2,\n",
    "        legend_name=column,\n",
    "        bins=bins\n",
    "    ).add_to(m)\n",
    "    \n",
    "    # Add hover functionality\n",
    "    style_function = lambda x: {'fillColor': '#ffffff', \n",
    "                                'color': '#000000', \n",
    "                                'fillOpacity': 0.1, \n",
    "                                'weight': 0.1}\n",
    "    highlight_function = lambda x: {'fillColor': '#000000', \n",
    "                                    'color': '#000000', \n",
    "                                    'fillOpacity': 0.5, \n",
    "                                    'weight': 0.1}\n",
    "    \n",
    "    # Add tooltips\n",
    "    folium.GeoJson(\n",
    "        gdf,\n",
    "        style_function=style_function,\n",
    "        highlight_function=highlight_function,\n",
    "        tooltip=folium.GeoJsonTooltip(\n",
    "            fields=[column],\n",
    "            aliases=[column.replace('_', ' ').title()],\n",
    "            style=(\"background-color: white; color: #333333; font-family: arial; font-size: 12px; padding: 10px;\")\n",
    "        )\n",
    "    ).add_to(m)\n",
    "    \n",
    "    # Add layer control\n",
    "    folium.LayerControl().add_to(m)\n",
    "    \n",
    "    return m\n",
    "\n",
    "def create_folium_heatmap(gdf, intensity_column=None, radius=15, \n",
    "                          title=\"PV Installation Heatmap\"):\n",
    "    \"\"\"\n",
    "    Create a heatmap of PV installations using Folium.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gdf : GeoDataFrame\n",
    "        GeoDataFrame containing PV installation data with geometry column\n",
    "    intensity_column : str, optional\n",
    "        Column name to use for heatmap intensity; if None, all points have equal weight\n",
    "    radius : int\n",
    "        Radius for heatmap points (in pixels)\n",
    "    title : str\n",
    "        Title for the map\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    folium.Map\n",
    "        Interactive Folium heatmap\n",
    "    \"\"\"\n",
    "    # Ensure the GeoDataFrame is in WGS84 (EPSG:4326)\n",
    "    if gdf.crs != \"EPSG:4326\":\n",
    "        gdf = gdf.to_crs(\"EPSG:4326\")\n",
    "    \n",
    "    # Get centroids for all geometries\n",
    "    if any(gdf.geometry.geom_type.isin(['Polygon', 'MultiPolygon'])):\n",
    "        centroids = gdf.geometry.centroid\n",
    "    else:\n",
    "        centroids = gdf.geometry\n",
    "    \n",
    "    # Get coordinates for heatmap\n",
    "    heat_data = [[point.y, point.x] for point in centroids]\n",
    "    \n",
    "    # Add intensity if specified\n",
    "    if intensity_column and intensity_column in gdf.columns:\n",
    "        heat_data = [[point.y, point.x, float(intensity)] \n",
    "                    for point, intensity in zip(centroids, gdf[intensity_column])]\n",
    "    \n",
    "    # Get centroid of all points to center the map\n",
    "    center_lat = sum(point[0] for point in heat_data) / len(heat_data)\n",
    "    center_lon = sum(point[1] for point in heat_data) / len(heat_data)\n",
    "    \n",
    "    # Create base map\n",
    "    m = folium.Map(location=[center_lat, center_lon], zoom_start=4,\n",
    "                  tiles='CartoDB positron')\n",
    "    \n",
    "    # Add title\n",
    "    title_html = f'''\n",
    "             <h3 align=\"center\" style=\"font-size:16px\"><b>{title}</b></h3>\n",
    "             '''\n",
    "    m.get_root().html.add_child(folium.Element(title_html))\n",
    "    \n",
    "    # Add heatmap layer\n",
    "    HeatMap(\n",
    "        heat_data,\n",
    "        radius=radius,\n",
    "        blur=10,\n",
    "        gradient={0.4: 'blue', 0.65: 'lime', 1: 'red'}\n",
    "    ).add_to(m)\n",
    "    \n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3820b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter to only rows without NaN for folium viz\n",
    "# ds_gdf = ds_gdf[ds_gdf['area_m2'].notna()]\n",
    "# Create Folium maps\n",
    "cluster_map = create_folium_cluster_map(ds_gdf, title=f\"{ds} - Cluster Map\")\n",
    "# cluster_map.save(os.path.join(out_dir, f\"{ds}_cluster_map.html\"))\n",
    "cluster_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9ea146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create explicit index column like chloropleth expects\n",
    "# ds_gdf.reset_index(drop=False, inplace=True)\n",
    "# choropleth_map = create_folium_choropleth(ds_gdf, column='area_m2', title=f\"{ds} - Capacity Choropleth\")\n",
    "# choropleth_map.save(os.path.join(out_dir, f\"{ds}_choropleth_map.html\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbe1845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# heatmap = create_folium_heatmap(ds_gdf, intensity_column='area_m2', title=f\"{ds} - Capacity Heatmap\")\n",
    "# heatmap.save(os.path.join(out_dir, f\"{ds}_heatmap.html\"))\n",
    "# heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d04e252",
   "metadata": {},
   "source": [
    "## PyDeck Visualization Functions\n",
    "\n",
    "PyDeck is excellent for high-performance 3D visualizations and handling large datasets. It's particularly useful for creating layered maps with multiple types of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaac7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper Function for Color Mapping ---\n",
    "\n",
    "def get_pydeck_color(df, color_column=None, cmap='viridis', default_color=[0, 128, 0, 180]):\n",
    "    \"\"\"\n",
    "    Generates color mapping instructions for Pydeck layers.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing the data.\n",
    "        color_column (str, optional): Column to base color on. Defaults to None.\n",
    "        cmap (str, optional): Matplotlib colormap name for numeric data. Defaults to 'viridis'.\n",
    "        default_color (list, optional): Default RGBA color if no column specified. Defaults to green.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (Updated DataFrame with color columns if needed, Pydeck color accessor string/list)\n",
    "    \"\"\"\n",
    "    if not color_column or color_column not in df.columns:\n",
    "        return df, default_color # Return default color list\n",
    "\n",
    "    # Ensure color column NaNs are handled (replace with a placeholder or mean/median if appropriate)\n",
    "    if df[color_column].isnull().any():\n",
    "        print(f\"Warning: NaN values found in color column '{color_column}'. Filling with placeholder/mean.\")\n",
    "        if pd.api.types.is_numeric_dtype(df[color_column]):\n",
    "            fill_val = df[color_column].mean() # Or median, or 0\n",
    "            df[color_column] = df[color_column].fillna(fill_val)\n",
    "        else:\n",
    "            fill_val = 'Unknown'\n",
    "            df[color_column] = df[color_column].astype(str).fillna(fill_val)\n",
    "\n",
    "\n",
    "    if pd.api.types.is_numeric_dtype(df[color_column]):\n",
    "        # --- Numeric Column: Use a colormap ---\n",
    "        try:\n",
    "            from matplotlib import colormaps\n",
    "            from matplotlib.colors import Normalize\n",
    "\n",
    "            norm = Normalize(vmin=df[color_column].min(), vmax=df[color_column].max())\n",
    "            colormap = colormaps[cmap]\n",
    "\n",
    "            # Apply colormap and convert to RGBA 0-255 format\n",
    "            colors = colormap(norm(df[color_column].values)) * 255\n",
    "            df['__color_r'] = colors[:, 0].astype(int)\n",
    "            df['__color_g'] = colors[:, 1].astype(int)\n",
    "            df['__color_b'] = colors[:, 2].astype(int)\n",
    "            # Use a fixed alpha or make it dynamic if needed\n",
    "            df['__color_a'] = default_color[3] if len(default_color) == 4 else 180\n",
    "\n",
    "            return df, '[__color_r, __color_g, __color_b, __color_a]'\n",
    "        except ImportError:\n",
    "            print(\"Matplotlib not found. Falling back to default color for numeric column.\")\n",
    "            return df, default_color\n",
    "        except Exception as e:\n",
    "            print(f\"Error applying numeric colormap: {e}. Falling back to default color.\")\n",
    "            return df, default_color\n",
    "\n",
    "    else:\n",
    "        # --- Categorical Column: Assign random colors ---\n",
    "        unique_cats = df[color_column].astype(str).unique()\n",
    "        # Generate pseudo-random but consistent colors for categories\n",
    "        color_map = {\n",
    "            cat: [random.randint(0, 255), random.randint(0, 255), random.randint(0, 255), default_color[3] if len(default_color) == 4 else 180]\n",
    "            for cat in unique_cats\n",
    "        }\n",
    "        # Add 'Unknown' category if needed\n",
    "        if 'Unknown' not in color_map:\n",
    "             color_map['Unknown'] = [128, 128, 128, default_color[3] if len(default_color) == 4 else 180] # Grey for unknowns\n",
    "\n",
    "        df['__color_r'] = df[color_column].astype(str).map(lambda x: color_map.get(x, color_map['Unknown'])[0])\n",
    "        df['__color_g'] = df[color_column].astype(str).map(lambda x: color_map.get(x, color_map['Unknown'])[1])\n",
    "        df['__color_b'] = df[color_column].astype(str).map(lambda x: color_map.get(x, color_map['Unknown'])[2])\n",
    "        df['__color_a'] = df[color_column].astype(str).map(lambda x: color_map.get(x, color_map['Unknown'])[3])\n",
    "\n",
    "        return df, '[__color_r, __color_g, __color_b, __color_a]'\n",
    "\n",
    "\n",
    "# --- Refactored Pydeck Functions ---\n",
    "\n",
    "def create_pydeck_scatterplot(gdf: gpd.GeoDataFrame,\n",
    "                              color_column: Optional[str] = None,\n",
    "                              size_column: Optional[str] = None,\n",
    "                              size_scale: float = 50.0,\n",
    "                              tooltip_cols: Optional[List[str]] = None,\n",
    "                              map_style: str = 'light',\n",
    "                              initial_zoom: int = 3):\n",
    "    \"\"\"\n",
    "    Creates an interactive scatterplot map using PyDeck, plotting centroids.\n",
    "\n",
    "    Args:\n",
    "        gdf: Input GeoDataFrame. Assumed to have a 'geometry' column.\n",
    "        color_column: Column name to use for point color.\n",
    "        size_column: Column name to use for point size (radius).\n",
    "                     Uses square root scaling for better visual perception.\n",
    "        size_scale: Scaling factor for point radius.\n",
    "        tooltip_cols: List of column names to include in the tooltip.\n",
    "                      Defaults to common useful columns if None.\n",
    "        map_style: Pydeck map style (e.g., 'light', 'dark', 'satellite').\n",
    "        initial_zoom: Initial zoom level for the map.\n",
    "\n",
    "    Returns:\n",
    "        pydeck.Deck: A PyDeck map object, or None if input is invalid.\n",
    "    \"\"\"\n",
    "    if gdf is None or gdf.empty:\n",
    "        print(\"Input GeoDataFrame is empty or None.\")\n",
    "        return None\n",
    "    if 'geometry' not in gdf.columns:\n",
    "        print(\"GeoDataFrame must have a 'geometry' column.\")\n",
    "        return None\n",
    "\n",
    "    print(f\"Creating Pydeck scatterplot for {len(gdf)} features...\")\n",
    "\n",
    "    # Ensure correct CRS\n",
    "    if gdf.crs != \"EPSG:4326\":\n",
    "        print(f\"Reprojecting GDF from {gdf.crs} to EPSG:4326...\")\n",
    "        gdf = gdf.to_crs(\"EPSG:4326\")\n",
    "\n",
    "    # Prepare DataFrame for Pydeck\n",
    "    df = pd.DataFrame()\n",
    "    # Use centroid for plotting points\n",
    "    gdf['__centroid'] = gdf.geometry.centroid\n",
    "    df['lon'] = gdf['__centroid'].x\n",
    "    df['lat'] = gdf['__centroid'].y\n",
    "    gdf = gdf.drop(columns=['__centroid']) # Clean up temporary column\n",
    "\n",
    "    # Copy relevant attribute columns\n",
    "    potential_tooltip_cols = ['unified_id', 'dataset', 'area_m2', 'capacity_mw', 'install_date']\n",
    "    if tooltip_cols is None:\n",
    "        tooltip_cols = [col for col in potential_tooltip_cols if col in gdf.columns]\n",
    "    else:\n",
    "        # Ensure requested tooltip columns exist\n",
    "        tooltip_cols = [col for col in tooltip_cols if col in gdf.columns]\n",
    "\n",
    "    # Include color and size columns in the DataFrame if they exist and are needed\n",
    "    cols_to_copy = set(tooltip_cols)\n",
    "    if color_column and color_column in gdf.columns:\n",
    "        cols_to_copy.add(color_column)\n",
    "    if size_column and size_column in gdf.columns:\n",
    "        cols_to_copy.add(size_column)\n",
    "\n",
    "    for col in cols_to_copy:\n",
    "        df[col] = gdf[col]\n",
    "\n",
    "    # --- Color Mapping ---\n",
    "    df, color_accessor = get_pydeck_color(df, color_column)\n",
    "\n",
    "    # --- Size Mapping ---\n",
    "    if size_column and size_column in gdf.columns and pd.api.types.is_numeric_dtype(df[size_column]):\n",
    "        # Use sqrt scaling for better visual perception of area/capacity\n",
    "        # Handle potential negative values if necessary before sqrt\n",
    "        min_val = df[size_column].min()\n",
    "        if min_val < 0:\n",
    "             print(f\"Warning: Negative values found in size column '{size_column}'. Clamping to 0 for sqrt.\")\n",
    "             df['__size_val'] = np.sqrt(df[size_column].clip(lower=0).fillna(0)) * size_scale\n",
    "        else:\n",
    "             df['__size_val'] = np.sqrt(df[size_column].fillna(0)) * size_scale\n",
    "        size_accessor = '__size_val'\n",
    "        print(f\"Using column '{size_column}' for point size (sqrt scaled).\")\n",
    "    else:\n",
    "        if size_column:\n",
    "             print(f\"Warning: Size column '{size_column}' not found or not numeric. Using fixed size.\")\n",
    "        size_accessor = size_scale # Fixed radius if no valid column\n",
    "        print(f\"Using fixed point size: {size_scale}\")\n",
    "\n",
    "\n",
    "    # --- Tooltip ---\n",
    "    tooltip_html = \"\"\n",
    "    for col in tooltip_cols:\n",
    "        # Format column name nicely for display\n",
    "        col_display = col.replace('_', ' ').title()\n",
    "        tooltip_html += f\"<b>{col_display}:</b> {{{col}}}<br>\"\n",
    "    tooltip = {\"html\": tooltip_html.strip('<br>')} if tooltip_html else None\n",
    "\n",
    "\n",
    "    # --- Layer ---\n",
    "    layer = pdk.Layer(\n",
    "        'ScatterplotLayer',\n",
    "        df,\n",
    "        get_position=['lon', 'lat'],\n",
    "        get_radius=size_accessor,\n",
    "        get_fill_color=color_accessor,\n",
    "        pickable=True,\n",
    "        opacity=0.7,\n",
    "        stroked=True,\n",
    "        filled=True,\n",
    "        radius_min_pixels=1,\n",
    "        radius_max_pixels=100,\n",
    "        get_line_color=[0, 0, 0, 100], # Faint black outline\n",
    "        line_width_min_pixels=0.5\n",
    "    )\n",
    "\n",
    "    # --- View State ---\n",
    "    view_state = pdk.ViewState(\n",
    "        longitude=df['lon'].mean(),\n",
    "        latitude=df['lat'].mean(),\n",
    "        zoom=initial_zoom,\n",
    "        pitch=0,\n",
    "        bearing=0\n",
    "    )\n",
    "\n",
    "    # --- Deck ---\n",
    "    deck = pdk.Deck(\n",
    "        layers=[layer],\n",
    "        initial_view_state=view_state,\n",
    "        tooltip=tooltip,\n",
    "        map_style=map_style\n",
    "    )\n",
    "    print(\"Pydeck scatterplot created successfully.\")\n",
    "    return deck\n",
    "\n",
    "\n",
    "def create_pydeck_polygons(gdf: gpd.GeoDataFrame,\n",
    "                           color_column: Optional[str] = None,\n",
    "                           extrusion_column: Optional[str] = None,\n",
    "                           extrusion_scale: float = 1.0,\n",
    "                           tooltip_cols: Optional[List[str]] = None,\n",
    "                           map_style: str = 'light',\n",
    "                           initial_zoom: int = 10,\n",
    "                           sample_frac: float = 1.0, # Add sampling parameter\n",
    "                           where_clause: Optional[str] = None): # Add filtering parameter\n",
    "    \"\"\"\n",
    "    Creates an interactive 3D polygon map using PyDeck.\n",
    "    NOTE: Rendering large numbers of complex polygons can be slow.\n",
    "          Consider using sample_frac or where_clause for large datasets.\n",
    "\n",
    "    Args:\n",
    "        gdf: Input GeoDataFrame. Must contain Polygon/MultiPolygon geometries.\n",
    "        color_column: Column name to use for polygon color.\n",
    "        extrusion_column: Column name to use for polygon height extrusion.\n",
    "                          Uses linear scaling.\n",
    "        extrusion_scale: Scaling factor for extrusion height. Applied AFTER normalization if numeric.\n",
    "        tooltip_cols: List of column names to include in the tooltip.\n",
    "        map_style: Pydeck map style.\n",
    "        initial_zoom: Initial zoom level.\n",
    "        sample_frac: Fraction of data to sample (0.0 to 1.0). 1.0 means no sampling.\n",
    "        where_clause: A Pandas query string to filter the GDF before visualization.\n",
    "\n",
    "    Returns:\n",
    "        pydeck.Deck: A PyDeck map object, or None if input is invalid.\n",
    "    \"\"\"\n",
    "    if gdf is None or gdf.empty:\n",
    "        print(\"Input GeoDataFrame is empty or None.\")\n",
    "        return None\n",
    "    if 'geometry' not in gdf.columns:\n",
    "        print(\"GeoDataFrame must have a 'geometry' column.\")\n",
    "        return None\n",
    "\n",
    "    # --- Filtering and Sampling ---\n",
    "    original_count = len(gdf)\n",
    "    if where_clause:\n",
    "        try:\n",
    "            gdf = gdf.query(where_clause).copy()\n",
    "            print(f\"Filtered GDF using '{where_clause}'. Count: {len(gdf)} (from {original_count})\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error applying where_clause '{where_clause}': {e}. Using original GDF.\")\n",
    "            gdf = gdf.copy() # Ensure we have a copy\n",
    "    else:\n",
    "         gdf = gdf.copy() # Ensure we have a copy\n",
    "\n",
    "    if sample_frac < 1.0 and len(gdf) > 0:\n",
    "        try:\n",
    "            gdf = gdf.sample(frac=sample_frac, random_state=42) # Use random_state for reproducibility\n",
    "            print(f\"Sampled {sample_frac*100:.1f}% of data. Count: {len(gdf)} (from {original_count if not where_clause else 'filtered'})\")\n",
    "        except Exception as e:\n",
    "             print(f\"Error sampling data: {e}. Using full (or filtered) GDF.\")\n",
    "    # --- End Filtering and Sampling ---\n",
    "\n",
    "\n",
    "    # Ensure correct CRS\n",
    "    if gdf.crs != \"EPSG:4326\":\n",
    "        print(f\"Reprojecting GDF from {gdf.crs} to EPSG:4326...\")\n",
    "        gdf = gdf.to_crs(\"EPSG:4326\")\n",
    "\n",
    "    # Filter for valid Polygon/MultiPolygon geometries\n",
    "    gdf = gdf[gdf.geometry.geom_type.isin(['Polygon', 'MultiPolygon']) & gdf.geometry.is_valid & ~gdf.geometry.is_empty]\n",
    "\n",
    "    if gdf.empty:\n",
    "        print(\"No valid Polygon or MultiPolygon geometries found after filtering.\")\n",
    "        return None\n",
    "\n",
    "    print(f\"Creating Pydeck polygon map for {len(gdf)} features...\")\n",
    "\n",
    "    # Prepare data for Pydeck PolygonLayer\n",
    "    # Using __geo_interface__ is generally more efficient than manual iteration\n",
    "    gdf['__geojson_feature'] = gdf.apply(lambda row: row.geometry.__geo_interface__, axis=1)\n",
    "\n",
    "    # Extract properties into the DataFrame for easier access by Pydeck accessors\n",
    "    potential_tooltip_cols = ['unified_id', 'dataset', 'area_m2', 'capacity_mw', 'install_date']\n",
    "    if tooltip_cols is None:\n",
    "        tooltip_cols = [col for col in potential_tooltip_cols if col in gdf.columns]\n",
    "    else:\n",
    "        tooltip_cols = [col for col in tooltip_cols if col in gdf.columns] # Ensure they exist\n",
    "\n",
    "    cols_to_keep = set(tooltip_cols)\n",
    "    if color_column and color_column in gdf.columns:\n",
    "        cols_to_keep.add(color_column)\n",
    "    if extrusion_column and extrusion_column in gdf.columns:\n",
    "        cols_to_keep.add(extrusion_column)\n",
    "    cols_to_keep.add('__geojson_feature') # Add the geometry representation\n",
    "\n",
    "    df_pydeck = gdf[list(cols_to_keep)].copy()\n",
    "\n",
    "\n",
    "    # --- Color Mapping ---\n",
    "    df_pydeck, color_accessor = get_pydeck_color(df_pydeck, color_column, default_color=[0, 128, 0, 150]) # Slightly transparent default\n",
    "\n",
    "    # --- Extrusion Mapping ---\n",
    "    if extrusion_column and extrusion_column in gdf.columns and pd.api.types.is_numeric_dtype(df_pydeck[extrusion_column]):\n",
    "        # Normalize extrusion height for better scaling, handle NaNs\n",
    "        min_val = df_pydeck[extrusion_column].min()\n",
    "        max_val = df_pydeck[extrusion_column].max()\n",
    "        if pd.isna(min_val) or pd.isna(max_val) or max_val == min_val:\n",
    "             print(f\"Warning: Cannot normalize extrusion column '{extrusion_column}'. Using fixed extrusion.\")\n",
    "             df_pydeck['__elevation_val'] = 1.0 # Use a base value\n",
    "        else:\n",
    "             df_pydeck['__elevation_val'] = (df_pydeck[extrusion_column].fillna(min_val) - min_val) / (max_val - min_val)\n",
    "\n",
    "        # Apply overall scale factor\n",
    "        elevation_accessor = f'__elevation_val * {extrusion_scale * 1000}' # Multiply scale for visibility\n",
    "        print(f\"Using column '{extrusion_column}' for extrusion (normalized & scaled by {extrusion_scale*1000}).\")\n",
    "        extruded = True\n",
    "    else:\n",
    "        if extrusion_column:\n",
    "            print(f\"Warning: Extrusion column '{extrusion_column}' not found or not numeric. No extrusion applied.\")\n",
    "        elevation_accessor = 0 # No height\n",
    "        extruded = False\n",
    "        print(\"No extrusion applied.\")\n",
    "\n",
    "\n",
    "    # --- Tooltip ---\n",
    "    tooltip_html = \"\"\n",
    "    for col in tooltip_cols:\n",
    "        col_display = col.replace('_', ' ').title()\n",
    "        tooltip_html += f\"<b>{col_display}:</b> {{{col}}}<br>\"\n",
    "    tooltip = {\"html\": tooltip_html.strip('<br>')} if tooltip_html else None\n",
    "\n",
    "    # --- Layer ---\n",
    "    layer = pdk.Layer(\n",
    "        'GeoJsonLayer', # Use GeoJsonLayer which handles Polygon/MultiPolygon via __geo_interface__\n",
    "        df_pydeck,\n",
    "        opacity=0.7,\n",
    "        stroked=True, # Wireframe\n",
    "        filled=True,\n",
    "        extruded=extruded,\n",
    "        wireframe=True,\n",
    "        get_polygon='__geojson_feature', # Access the geojson geometry\n",
    "        get_fill_color=color_accessor,\n",
    "        get_line_color=[0, 0, 0, 100],\n",
    "        get_line_width=10, # Meters\n",
    "        line_width_min_pixels=0.5,\n",
    "        get_elevation=elevation_accessor if extruded else 0,\n",
    "        pickable=True,\n",
    "        auto_highlight=True\n",
    "    )\n",
    "\n",
    "    # --- View State ---\n",
    "    # Calculate bounds more efficiently\n",
    "    bounds = gdf.total_bounds # [minx, miny, maxx, maxy]\n",
    "    center_lon = (bounds[0] + bounds[2]) / 2\n",
    "    center_lat = (bounds[1] + bounds[3]) / 2\n",
    "\n",
    "    view_state = pdk.ViewState(\n",
    "        longitude=center_lon,\n",
    "        latitude=center_lat,\n",
    "        zoom=initial_zoom,\n",
    "        pitch=45 if extruded else 0, # Pitch only if extruded\n",
    "        bearing=0\n",
    "    )\n",
    "\n",
    "    # --- Deck ---\n",
    "    deck = pdk.Deck(\n",
    "        layers=[layer],\n",
    "        initial_view_state=view_state,\n",
    "        tooltip=tooltip,\n",
    "        map_style=map_style\n",
    "    )\n",
    "    print(\"Pydeck polygon map created successfully.\")\n",
    "    return deck\n",
    "\n",
    "\n",
    "def create_pydeck_heatmap(gdf: gpd.GeoDataFrame,\n",
    "                          weight_column: Optional[str] = None,\n",
    "                          radius_pixels: int = 50,\n",
    "                          intensity: float = 1.0,\n",
    "                          threshold: float = 0.05,\n",
    "                          aggregation: str = 'SUM', # SUM or MEAN\n",
    "                          tooltip_cols: Optional[List[str]] = None, # Tooltip not directly supported by HeatmapLayer\n",
    "                          map_style: str = 'light',\n",
    "                          initial_zoom: int = 3):\n",
    "    \"\"\"\n",
    "    Creates an interactive heatmap using PyDeck.\n",
    "\n",
    "    Args:\n",
    "        gdf: Input GeoDataFrame.\n",
    "        weight_column: Column name to use for heatmap weighting. If None, uses count.\n",
    "        radius_pixels: Radius of influence for each point in pixels.\n",
    "        intensity: Multiplier factor for heatmap intensity.\n",
    "        threshold: Minimum threshold for color rendering (0 to 1).\n",
    "        aggregation: 'SUM' or 'MEAN' - how weights are accumulated.\n",
    "        tooltip_cols: (Currently ignored by HeatmapLayer) List of columns for tooltip.\n",
    "        map_style: Pydeck map style.\n",
    "        initial_zoom: Initial zoom level.\n",
    "\n",
    "    Returns:\n",
    "        pydeck.Deck: A PyDeck map object, or None if input is invalid.\n",
    "    \"\"\"\n",
    "    if gdf is None or gdf.empty:\n",
    "        print(\"Input GeoDataFrame is empty or None.\")\n",
    "        return None\n",
    "    if 'geometry' not in gdf.columns:\n",
    "        print(\"GeoDataFrame must have a 'geometry' column.\")\n",
    "        return None\n",
    "\n",
    "    print(f\"Creating Pydeck heatmap for {len(gdf)} features...\")\n",
    "\n",
    "    # Ensure correct CRS\n",
    "    if gdf.crs != \"EPSG:4326\":\n",
    "        print(f\"Reprojecting GDF from {gdf.crs} to EPSG:4326...\")\n",
    "        gdf = gdf.to_crs(\"EPSG:4326\")\n",
    "\n",
    "    # Prepare DataFrame for Pydeck\n",
    "    df = pd.DataFrame()\n",
    "    gdf['__centroid'] = gdf.geometry.centroid\n",
    "    df['lon'] = gdf['__centroid'].x\n",
    "    df['lat'] = gdf['__centroid'].y\n",
    "\n",
    "    # --- Weight Mapping ---\n",
    "    if weight_column and weight_column in gdf.columns and pd.api.types.is_numeric_dtype(gdf[weight_column]):\n",
    "         # Handle NaNs - fill with 0 or mean/median depending on desired behavior\n",
    "        if gdf[weight_column].isnull().any():\n",
    "            print(f\"Warning: NaN values found in weight column '{weight_column}'. Filling with 0.\")\n",
    "            df['__weight'] = gdf[weight_column].fillna(0)\n",
    "        else:\n",
    "            df['__weight'] = gdf[weight_column]\n",
    "        weight_accessor = '__weight'\n",
    "        print(f\"Using column '{weight_column}' for heatmap weights (Aggregation: {aggregation}).\")\n",
    "    else:\n",
    "        if weight_column:\n",
    "            print(f\"Warning: Weight column '{weight_column}' not found or not numeric. Using count (weight=1).\")\n",
    "        weight_accessor = 1 # Use count if no valid weight column\n",
    "        print(f\"Using count (weight=1) for heatmap.\")\n",
    "\n",
    "    # --- Layer ---\n",
    "    # Viridis color range (adjust alpha as needed)\n",
    "    color_range = [\n",
    "        [68, 1, 84, 255],\n",
    "        [72, 40, 120, 255],\n",
    "        [62, 74, 137, 255],\n",
    "        [49, 104, 142, 255],\n",
    "        [38, 130, 142, 255],\n",
    "        [31, 158, 137, 255],\n",
    "        [53, 183, 121, 255],\n",
    "        [109, 205, 89, 255],\n",
    "        [180, 222, 44, 255],\n",
    "        [253, 231, 37, 255]\n",
    "    ]\n",
    "\n",
    "    layer = pdk.Layer(\n",
    "        'HeatmapLayer',\n",
    "        df,\n",
    "        get_position=['lon', 'lat'],\n",
    "        get_weight=weight_accessor,\n",
    "        opacity=0.8,\n",
    "        radius_pixels=radius_pixels,\n",
    "        intensity=intensity,\n",
    "        threshold=threshold,\n",
    "        aggregation=aggregation.upper(), # Ensure uppercase\n",
    "        color_range=color_range # Use Viridis colormap\n",
    "    )\n",
    "\n",
    "    # --- View State ---\n",
    "    view_state = pdk.ViewState(\n",
    "        longitude=df['lon'].mean(),\n",
    "        latitude=df['lat'].mean(),\n",
    "        zoom=initial_zoom,\n",
    "        pitch=0,\n",
    "        bearing=0\n",
    "    )\n",
    "\n",
    "    # --- Deck ---\n",
    "    # Tooltip is generally not useful for HeatmapLayer\n",
    "    deck = pdk.Deck(\n",
    "        layers=[layer],\n",
    "        initial_view_state=view_state,\n",
    "        map_style=map_style,\n",
    "        tooltip=False # Disable tooltip for heatmap\n",
    "    )\n",
    "    print(\"Pydeck heatmap created successfully.\")\n",
    "    return deck\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4477a4a0",
   "metadata": {},
   "source": [
    "# STAC Collections and fetching samples with cubo\n",
    "\n",
    "[cubo](https://cubo.readthedocs.io/en/latest/index.html) is a Python library for working with STAC (SpatioTemporal Asset Catalog) collections and fetching samples from them. It provides a convenient way to interact with STAC APIs and retrieve geospatial data.\n",
    "\n",
    "Look into managing raster data with duckdb or similar:\n",
    "- https://www.linkedin.com/pulse/querying-stac-load-satellite-imagery-sentinel2-duckdb-alvaro-huarte-yjuzf/?trackingId=wfMPNnd%2BDh2zi1AoiLG2vg%3D%3D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1bdce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cubo \n",
    "import xarray as xr\n",
    "import rioxarray as rxr\n",
    "\n",
    "def percentile_normalize(da, lower_percentile=5, upper_percentile=95):\n",
    "    \"\"\"Normalize using percentiles to improve visualization contrast\"\"\"\n",
    "    # Calculate percentiles per band\n",
    "    mins = da.quantile(lower_percentile/100, dim=('x', 'y'))\n",
    "    maxs = da.quantile(upper_percentile/100, dim=('x', 'y'))\n",
    "    \n",
    "    # Apply normalization\n",
    "    normalized = (da - mins) / (maxs - mins)\n",
    "    return normalized.clip(0, 1)\n",
    "\n",
    "# use cubo to fetch a series of 4 tiles and visualize per their tutorial: https://cubo.readthedocs.io/en/latest/tutorials/getting_started.html\n",
    "def fetch_cubo_stac_rasters_samples(\n",
    "    pv_gdf: gpd.GeoDataFrame,\n",
    "    stac_url='https://planetarycomputer.microsoft.com/api/stac/v1',\n",
    "    collection='sentinel-2-l2a',\n",
    "    bands=['B02', 'B03', 'B04'],\n",
    "    start_date='2023-01-01',\n",
    "    end_date='2023-03-31',\n",
    "    units=\"m\",\n",
    "    edge_size=1280,\n",
    "    resolution=10,\n",
    "    sample_size=1,\n",
    "    visualize_set=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Fetch raster samples from a STAC collection using Cubo and vi\n",
    "    Args:\n",
    "        pv_gdf (GeoDataFrame): Input GeoDataFrame with geometry column.\n",
    "        stac_url (str): STAC API URL.\n",
    "        collection (str): STAC collection name.\n",
    "        start_date (str): Start date for filtering.\n",
    "        end_date (str): End date for filtering.\n",
    "        units (str): Units for the raster data.\n",
    "        edge_size (int): Size of the edges of the raster tiles.\n",
    "        resolution (int): Resolution of the raster data.\n",
    "        sample_size (int): Number of samples to fetch.\n",
    "    Returns:\n",
    "        stac_xr_samples (xr.DataArray): Sampled raster data.\n",
    "\"\"\"\n",
    "\n",
    "    # Sort by area descending and sample\n",
    "    sampled_gdf = pv_gdf.sort_values(by='area_m2', ascending=False)\n",
    "    sampled_gdf = sampled_gdf[:1000].sample(sample_size, random_state=42)\n",
    "    # all should be using 'EPSG:4326'\n",
    "    target_epsg = sampled_gdf.crs\n",
    "    default_query = {\"eo:cloud_cover\": {\"lt\": 25}}\n",
    "    \n",
    "    stac_items = []\n",
    "    print(f\"Fetching samples for {len(sampled_gdf)} locations...\")\n",
    "    for idx, row in sampled_gdf.iterrows():\n",
    "        pv_lat, pv_long = row.geometry.centroid.y, row.geometry.centroid.x\n",
    "        print(f\"Fetching samples for {idx}: {pv_lat}, {pv_long} with area {row['area_m2']:.2f} m2\")\n",
    "        # Fetch samples using Cubo\n",
    "        try:\n",
    "            # Fetch samples using Cubo\n",
    "            # Pass the EPSG code as an integer, not a string with \"EPSG:\" prefix\n",
    "            pv_da = cubo.create(\n",
    "                lat=pv_lat,\n",
    "                lon=pv_long,\n",
    "                stac=stac_url,\n",
    "                collection=collection,\n",
    "                bands=bands,\n",
    "                start_date=start_date,\n",
    "                end_date=end_date,\n",
    "                edge_size=1280,\n",
    "                units=units,\n",
    "                resolution=resolution,\n",
    "                # Use integer EPSG code instead of string with 'EPSG:' prefix\n",
    "                # stackstac_kw=dict( # stackstac keyword arguments\n",
    "                #     # xy_coords='center',\n",
    "                #     epsg=32611)\n",
    "                query=default_query\n",
    "            )\n",
    "\n",
    "            # return pv_da\n",
    "            \n",
    "            stac_items.append(pv_da)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching or visualizing data for {idx}: {e}\")\n",
    "    \n",
    "\n",
    "    if not stac_items:\n",
    "        print(\"No valid data items fetched.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "    # visualize only last sample for now\n",
    "    if visualize_set and pv_da is not None:\n",
    "        # Check that the 'time' coordinate exists and has data\n",
    "        if \"time\" in pv_da.coords and len(pv_da.time) > 0:\n",
    "            # Calculate number of columns for the visualization grid\n",
    "            axes = min(4, len(pv_da.time))\n",
    "            # Display RGB composite by selecting red, green, blue bands and scaling\n",
    "            pv_da = pv_da.groupby('time').first()\n",
    "            rgb_da = percentile_normalize(pv_da.sel(band=[\"B04\", \"B03\", \"B02\"]))\n",
    "            rgb_da.plot.imshow(col=\"time\", col_wrap=axes)\n",
    "        \n",
    "        else:\n",
    "            print(f\"No data available for location {pv_lat}, {pv_long}\")\n",
    "    \n",
    "    try:\n",
    "        # First ensure all datasets have same dimensions and variables\n",
    "        aligned_items = []\n",
    "        for item in stac_items:\n",
    "            # Ensure all datasets use the same CRS\n",
    "            if item.rio.crs is None:\n",
    "                item = item.rio.write_crs(\"EPSG:4326\")\n",
    "            \n",
    "            # Add to aligned list\n",
    "            aligned_items.append(item)\n",
    "        \n",
    "        # Use rioxarray's merge functionality which handles spatial contexts better\n",
    "        # First combine by time (similar to your original approach)\n",
    "        merged_data = xr.concat(aligned_items, dim='time')\n",
    "        \n",
    "        # For spatial merging with overlapping tiles, you could use:\n",
    "        # merged_data = rxr.merge.merge_arrays(aligned_items)\n",
    "        \n",
    "        return merged_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error merging datasets: {e}\")\n",
    "        # If merge fails, return the list of items instead\n",
    "        print(\"Returning list of individual DataArrays without merging\")\n",
    "        return stac_items\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443f7f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stac_items = fetch_cubo_stac_rasters_samples(ds_gdf, visualize_set=True, sample_size=3)\n",
    "stac_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3260f202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize one of the samples\n",
    "np.random.seed(42)\n",
    "# print(pv_da)\n",
    "# pv_da = samples.isel(time=0)\n",
    "pv_uniq = stac_items.groupby('time').first()\n",
    "(pv_uniq.sel(band=[\"B04\",\"B03\",\"B02\"])/5000).clip(0,1).plot.imshow(col=\"time\",col_wrap = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263c5aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_temporal_animation(stacked_data, output_path=\"pv_animation.gif\"):\n",
    "    \"\"\"Create an animated GIF showing temporal changes\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.animation as animation\n",
    "    \n",
    "    # Prepare RGB data\n",
    "    rgb_data = []\n",
    "    for t in stacked_data.time.values:\n",
    "        rgb = visualize_enhanced_rgb(stacked_data.sel(time=t))\n",
    "        rgb_data.append(rgb.transpose(\"y\", \"x\", \"band\").values)\n",
    "    \n",
    "    # Create animation\n",
    "    fig, ax = plt.figure(figsize=(10, 10)), plt.gca()\n",
    "    \n",
    "    # Create initial image\n",
    "    im = ax.imshow(rgb_data[0])\n",
    "    ax.set_axis_off()\n",
    "    \n",
    "    # Define update function for animation\n",
    "    def update(frame):\n",
    "        im.set_array(rgb_data[frame])\n",
    "        ax.set_title(f\"Date: {stacked_data.time.values[frame]}\")\n",
    "        return [im]\n",
    "    \n",
    "    # Create and save animation\n",
    "    ani = animation.FuncAnimation(fig, update, frames=len(rgb_data), interval=500)\n",
    "    ani.save(output_path, writer='pillow')\n",
    "    \n",
    "    return ani"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1a5ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "crerate_temporal_animation(stac_items, output_path=\"pv_animation.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdded308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample a single centroid pv polygon from the gdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0271880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pystac_client\n",
    "# import pystac\n",
    "# import geopandas as gpd\n",
    "# from shapely.geometry import box\n",
    "# import matplotlib.pyplot as plt\n",
    "# import warnings\n",
    "\n",
    "# # Suppress specific UserWarning from pystac_client about ItemSearch link\n",
    "# warnings.filterwarnings(\"ignore\", message=\"Server does not conform to ITEM_SEARCH.*\", category=UserWarning)\n",
    "\n",
    "# # --- Configuration ---\n",
    "# MAXAR_OPEN_DATA_CATALOG_URL = \"https://maxar-opendata.s3.amazonaws.com/events/catalog.json\"\n",
    "\n",
    "# # --- STAC Traversal ---\n",
    "# print(f\"Connecting to catalog: {MAXAR_OPEN_DATA_CATALOG_URL}...\")\n",
    "# all_items = []\n",
    "# try:\n",
    "#     # Open the root STAC Catalog\n",
    "#     root_catalog = pystac_client.Client.open(MAXAR_OPEN_DATA_CATALOG_URL)\n",
    "#     print(\"Successfully connected to root catalog.\")\n",
    "\n",
    "#     # Get top-level children (likely Collections or Catalogs representing events)\n",
    "#     print(\"Fetching top-level children (Events)...\")\n",
    "#     top_children = list(root_catalog.get_children())\n",
    "#     print(f\"Found {len(top_children)} top-level children (Events).\")\n",
    "\n",
    "#     # Iterate through the top-level children (Events)\n",
    "#     for event_child in top_children:\n",
    "#         print(f\"\\nProcessing Event Child: {event_child.id} ({type(event_child)})...\")\n",
    "#         try:\n",
    "#             # Attempt to get items directly from this event child\n",
    "#             # get_items() should work for both Catalog and Collection objects if they contain items\n",
    "#             print(f\"  Fetching items from {event_child.id}...\")\n",
    "#             # Resolve the child first if it's just a link (get_children often returns resolved)\n",
    "#             # event_child.resolve_stac_object() # Might be needed if get_children returns unresolved links\n",
    "\n",
    "#             items_in_event = list(event_child.get_items(recursive=False)) # Use recursive=True just in case\n",
    "#             if items_in_event:\n",
    "#                  print(f\"    Found {len(items_in_event)} items directly in {event_child.id}.\")\n",
    "#                  all_items.extend(items_in_event)\n",
    "#           #   else:\n",
    "#           #        # If no items directly, check if it has children itself (deeper nesting)\n",
    "#           #        print(f\"    No items found directly in {event_child.id}. Checking for sub-children...\")\n",
    "#           #        try:\n",
    "#           #             sub_children = list(event_child.get_children())\n",
    "#           #             if sub_children:\n",
    "#           #                  print(f\"    Found {len(sub_children)} sub-children in {event_child.id}.\")\n",
    "#           #                  for sub_child in sub_children:\n",
    "#           #                       print(f\"      Fetching items from sub-child: {sub_child.id}...\")\n",
    "#           #                       try:\n",
    "#           #                            items_in_sub = list(sub_child.get_items(recursive=True))\n",
    "#           #                            print(f\"        Found {len(items_in_sub)} items in {sub_child.id}.\")\n",
    "#           #                            all_items.extend(items_in_sub)\n",
    "#           #                       except Exception as sub_item_err:\n",
    "#           #                            print(f\"        Error fetching items from {sub_child.id}: {sub_item_err}\")\n",
    "#           #             else:\n",
    "#           #                  print(f\"    No sub-children found in {event_child.id}.\")\n",
    "#           #        except Exception as sub_child_err:\n",
    "#           #             print(f\"    Error fetching sub-children for {event_child.id}: {sub_child_err}\")\n",
    "\n",
    "#         except AttributeError as ae:\n",
    "#              # Handle cases where an object might not have get_items or get_children\n",
    "#              print(f\"  Skipping {event_child.id}, does not appear to be a Catalog/Collection or error occurred: {ae}\")\n",
    "#         except Exception as event_err:\n",
    "#             print(f\"  Error processing {event_child.id}: {event_err}\")\n",
    "\n",
    "\n",
    "#     print(f\"\\nTotal items fetched from all events: {len(all_items)}\")\n",
    "\n",
    "#     # --- Data Extraction and Preparation --- (Same as before)\n",
    "#     if not all_items:\n",
    "#         print(\"No items found after traversing. Exiting.\")\n",
    "#     else:\n",
    "#         print(\"Extracting bounding boxes...\")\n",
    "#         bboxes = []\n",
    "#         item_ids = []\n",
    "#         for item in all_items:\n",
    "#             if item.bbox:\n",
    "#                 bboxes.append(item.bbox)\n",
    "#                 item_ids.append(item.id)\n",
    "#             else:\n",
    "#                 # It's also possible geometry exists but not bbox\n",
    "#                 if item.geometry:\n",
    "#                      try:\n",
    "#                           # Calculate bbox from geometry (more expensive)\n",
    "#                           min_lon, min_lat, max_lon, max_lat = gpd.GeoSeries([item.geometry], crs=\"EPSG:4326\").total_bounds\n",
    "#                           bboxes.append([min_lon, min_lat, max_lon, max_lat])\n",
    "#                           item_ids.append(item.id)\n",
    "#                           print(f\"Info: Item {item.id} using geometry bounds.\")\n",
    "#                      except Exception as geom_err:\n",
    "#                           print(f\"Warning: Item {item.id} has no bbox and failed to get bounds from geometry: {geom_err}\")\n",
    "#                 else:\n",
    "#                      print(f\"Warning: Item {item.id} has no bbox or geometry.\")\n",
    "\n",
    "\n",
    "#         if not bboxes:\n",
    "#              print(\"No items with bounding boxes found. Cannot plot.\")\n",
    "#         else:\n",
    "#             # Convert bounding boxes (min_lon, min_lat, max_lon, max_lat) to Shapely polygons\n",
    "#             geometries = [box(min_lon, min_lat, max_lon, max_lat) for min_lon, min_lat, max_lon, max_lat in bboxes]\n",
    "\n",
    "#             # Create a GeoDataFrame\n",
    "#             gdf_items = gpd.GeoDataFrame({'id': item_ids, 'geometry': geometries}, crs=\"EPSG:4326\")\n",
    "#             print(f\"Created GeoDataFrame with {len(gdf_items)} item extents.\")\n",
    "#             print(gdf_items.head()) # Display first few rows\n",
    "\n",
    "#             # --- Plotting --- (Same as before)\n",
    "#             print(\"Plotting item extents...\")\n",
    "#             world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "#             fig, ax = plt.subplots(1, 1, figsize=(15, 10))\n",
    "#             world.plot(ax=ax, color='lightgray', edgecolor='black')\n",
    "#             gdf_items.plot(ax=ax, color='red', alpha=0.5, edgecolor='red', linewidth=0.5) # Adjusted linewidth\n",
    "#             ax.set_title(f'Maxar Open Data Catalog Item Extents ({len(gdf_items)} items)')\n",
    "#             ax.set_xlabel('Longitude')\n",
    "#             ax.set_ylabel('Latitude')\n",
    "#             # Consider setting limits based on data extent if it's geographically concentrated\n",
    "#             minx, miny, maxx, maxy = gdf_items.total_bounds\n",
    "#             ax.set_xlim(minx - 2, maxx + 2) # Add padding\n",
    "#             ax.set_ylim(miny - 2, maxy + 2)\n",
    "#             plt.tight_layout() # Adjust layout\n",
    "#             plt.show()\n",
    "#             print(\"Plotting complete.\")\n",
    "\n",
    "# except Exception as e:\n",
    "#     print(f\"An error occurred during catalog processing: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6099559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from maxar_platform.session import session\n",
    "# session.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb234e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from maxar_platform.discovery import open_catalog\n",
    "# mxr_catalog = open_catalog('imagery') # returns the root catalog, or pass a subcatalog name like \"imagery\"\n",
    "# results = mxr_catalog.search(\n",
    "#     max_items=10,\n",
    "#     # rough Puerto Rico Bounding Box\n",
    "#     bbox=[-67.3507516333,17.5877264027,-65.532680798,19.0597280009],\n",
    "#     # from 2017-2024\n",
    "#     datetime=['2017-01-01T00:00:00Z', '2024-12-31T23:59:59Z']\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1b7e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open_catalog('imagery')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee852478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for item in results.items():\n",
    "#     print(item.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b585b453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import rasterio.plot\n",
    "# test_item = next(results.items_as_dicts())\n",
    "# print(f\"Processing item: {test_item['id']}\")\n",
    "# print(f\"Item keys: {test_item.keys()}\")\n",
    "# print(f\"Item assets: {test_item['assets']['browse'].keys()}\")\n",
    "# with rasterio.open(test_item['assets'][\"browse\"]['href']) as dataset:\n",
    "#     rasterio.plot.show(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a71faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Args:\n",
    "#         gdf: Input GeoDataFrame. Assumed to have a 'geometry' column.\n",
    "#         color_column: Column name to use for point color.\n",
    "#         size_column: Column name to use for point size (radius).\n",
    "#                      Uses square root scaling for better visual perception.\n",
    "#         size_scale: Scaling factor for point radius.\n",
    "#         tooltip_cols: List of column names to include in the tooltip.\n",
    "#                       Defaults to common useful columns if None.\n",
    "#         map_style: Pydeck map style (e.g., 'light', 'dark', 'satellite').\n",
    "#         initial_zoom: Initial zoom level for the map.\n",
    "create_pydeck_scatterplot(ds_gdf, color_column='dataset', map_style='light', tooltip_cols=['dataset', 'area_m2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5b8c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# heatmap_scale = 1000\n",
    "# gdf: gpd.GeoDataFrame,\n",
    "# color_column: Optional[str] = None,\n",
    "# extrusion_column: Optional[str] = None,\n",
    "# extrusion_scale: float = 1.0,\n",
    "# tooltip_cols: Optional[List[str]] = None,\n",
    "# map_style: str = 'light',\n",
    "# initial_zoom: int = 10,\n",
    "# sample_frac: float = 1.0, # Add sampling parameter\n",
    "# where_clause: Optional[str] = None): # Add filtering parameter\n",
    "# create_pydeck_polygons(\n",
    "#     gdf=ds_gdf,\n",
    "#     color_column='dataset',\n",
    "#     extrusion_column='area_m2',\n",
    "#     extrusion_scale=1000000.0,\n",
    "#     tooltip_cols=['dataset', 'area_m2'],\n",
    "#     map_style='light',\n",
    "#     initial_zoom=3,\n",
    "#     sample_frac=0.25, # 10% of data\n",
    "#     where_clause=None\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5195e2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see pydeck map styles here: \n",
    "create_pydeck_heatmap(\n",
    "    gdf=ds_gdf,\n",
    "    weight_column='area_m2',\n",
    "    radius_pixels=20,\n",
    "    intensity=3.,\n",
    "    threshold=0.00005,\n",
    "    aggregation='MEAN', # SUM or MEAN\n",
    "    tooltip_cols=['dataset', 'area_m2'],\n",
    "    map_style='light',\n",
    "    initial_zoom=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df04a910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out NaN area\n",
    "# ds_gdf = ds_gdf[ds_gdf['area_m2'].notna()]\n",
    "# # create_pydeck_polygons(ds_gdf, color_column='area_m2', extrusion_column='area_m2', title=f\"{ds} - Capacity 3D Map\")\n",
    "# create_pydeck_polygons(ds_gdf, extrusion_column='area_m2', title=f\"{ds} - PV installations area 3D Map\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f917a060",
   "metadata": {},
   "source": [
    "## Lonboard Visualization Functions\n",
    "\n",
    "Lonboard is a GPU-accelerated geospatial visualization library that's excellent for handling very large datasets. It's particularly useful for creating high-performance interactive visualizations of millions of data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1e5c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lonboard_map(gdf, color_column=None, size_column=None, size_scale=1,\n",
    "                       title=\"PV Installation Map\"):\n",
    "    \"\"\"\n",
    "    Create an interactive map of PV installations using Lonboard.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gdf : GeoDataFrame\n",
    "        GeoDataFrame containing PV installation data with geometry column\n",
    "    color_column : str, optional\n",
    "        Column name to use for point coloring\n",
    "    size_column : str, optional\n",
    "        Column name to use for point sizing\n",
    "    size_scale : float\n",
    "        Scaling factor for point size\n",
    "    title : str\n",
    "        Title for the map\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    lonboard.Map\n",
    "        Interactive Lonboard map\n",
    "    \"\"\"\n",
    "    # Ensure the GeoDataFrame is in WGS84 (EPSG:4326)\n",
    "    if gdf.crs != \"EPSG:4326\":\n",
    "        gdf = gdf.to_crs(\"EPSG:4326\")\n",
    "    \n",
    "    # Handle color mapping if specified\n",
    "    if color_column and color_column in gdf.columns:\n",
    "        color = gdf[color_column]\n",
    "    else:\n",
    "        color = None\n",
    "    \n",
    "    # Handle size mapping if specified\n",
    "    if size_column and size_column in gdf.columns:\n",
    "        size = gdf[size_column] * size_scale\n",
    "    else:\n",
    "        size = size_scale\n",
    "    \n",
    "    # Create the map\n",
    "    m = lonboard.Map()\n",
    "    \n",
    "    # Handle different geometry types\n",
    "    if all(gdf.geometry.geom_type.isin(['Point'])):\n",
    "        # For point geometries\n",
    "        m.add_layer(\n",
    "            lonboard.ScatterplotLayer(\n",
    "                gdf,\n",
    "                get_color=color,\n",
    "                get_radius=size,\n",
    "                opacity=0.8,\n",
    "                pickable=True,\n",
    "                auto_highlight=True\n",
    "            )\n",
    "        )\n",
    "    elif all(gdf.geometry.geom_type.isin(['Polygon', 'MultiPolygon'])):\n",
    "        # For polygon geometries\n",
    "        m.add_layer(\n",
    "            lonboard.GeoJsonLayer(\n",
    "                gdf,\n",
    "                get_fill_color=color,\n",
    "                get_line_color=[0, 0, 0, 200],\n",
    "                get_line_width=2,\n",
    "                opacity=0.8,\n",
    "                pickable=True,\n",
    "                auto_highlight=True\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        # For mixed geometries, convert to points (centroids) for simplicity\n",
    "        gdf_centroids = gdf.copy()\n",
    "        gdf_centroids['geometry'] = gdf_centroids.geometry.centroid\n",
    "        \n",
    "        m.add_layer(\n",
    "            lonboard.ScatterplotLayer(\n",
    "                gdf_centroids,\n",
    "                get_color=color,\n",
    "                get_radius=size,\n",
    "                opacity=0.8,\n",
    "                pickable=True,\n",
    "                auto_highlight=True\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    return m\n",
    "\n",
    "def create_lonboard_heatmap(gdf, weight_column=None, radius=1000,\n",
    "                          intensity=1, title=\"PV Installation Heatmap\"):\n",
    "    \"\"\"\n",
    "    Create a heatmap of PV installations using Lonboard.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gdf : GeoDataFrame\n",
    "        GeoDataFrame containing PV installation data with geometry column\n",
    "    weight_column : str, optional\n",
    "        Column name to use for heatmap weighting\n",
    "    radius : float\n",
    "        Radius of influence for each point (in meters)\n",
    "    intensity : float\n",
    "        Intensity of the heatmap\n",
    "    title : str\n",
    "        Title for the map\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    lonboard.Map\n",
    "        Interactive Lonboard heatmap\n",
    "    \"\"\"\n",
    "    # Ensure the GeoDataFrame is in WGS84 (EPSG:4326)\n",
    "    if gdf.crs != \"EPSG:4326\":\n",
    "        gdf = gdf.to_crs(\"EPSG:4326\")\n",
    "    \n",
    "    # Handle weight mapping if specified\n",
    "    if weight_column and weight_column in gdf.columns:\n",
    "        weight = gdf[weight_column]\n",
    "    else:\n",
    "        weight = 1\n",
    "    \n",
    "    # Get centroids for all geometries\n",
    "    gdf_centroids = gdf.copy()\n",
    "    if not all(gdf.geometry.geom_type.isin(['Point'])):\n",
    "        gdf_centroids['geometry'] = gdf_centroids.geometry.centroid\n",
    "    \n",
    "    # Create the map\n",
    "    m = lonboard.Map()\n",
    "    \n",
    "    # Add heatmap layer\n",
    "    m.add_layer(\n",
    "        lonboard.HeatmapLayer(\n",
    "            gdf_centroids,\n",
    "            get_weight=weight,\n",
    "            radius_pixels=int(radius/100),  # Convert meters to pixels roughly\n",
    "            intensity=intensity,\n",
    "            threshold=0.05,\n",
    "            color_range=[\n",
    "                [1, 152, 189, 255],\n",
    "                [73, 227, 206, 255],\n",
    "                [216, 254, 181, 255],\n",
    "                [254, 237, 177, 255],\n",
    "                [254, 173, 84, 255],\n",
    "                [209, 55, 78, 255]\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return m\n",
    "\n",
    "def create_lonboard_aggregation(gdf, resolution=8, color_scale='viridis',\n",
    "                              title=\"PV Installation Density\"):\n",
    "    \"\"\"\n",
    "    Create a hexbin aggregation map of PV installations using Lonboard.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gdf : GeoDataFrame\n",
    "        GeoDataFrame containing PV installation data with geometry column\n",
    "    resolution : int\n",
    "        Resolution of hexbins (higher = more detailed)\n",
    "    color_scale : str\n",
    "        Matplotlib colormap name for coloring\n",
    "    title : str\n",
    "        Title for the map\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    lonboard.Map\n",
    "        Interactive Lonboard hexbin aggregation map\n",
    "    \"\"\"\n",
    "    # Ensure the GeoDataFrame is in WGS84 (EPSG:4326)\n",
    "    if gdf.crs != \"EPSG:4326\":\n",
    "        gdf = gdf.to_crs(\"EPSG:4326\")\n",
    "    \n",
    "    # Get centroids for all geometries\n",
    "    gdf_centroids = gdf.copy()\n",
    "    if not all(gdf.geometry.geom_type.isin(['Point'])):\n",
    "        gdf_centroids['geometry'] = gdf_centroids.geometry.centroid\n",
    "    \n",
    "    # Create the map\n",
    "    m = lonboard.Map()\n",
    "    \n",
    "    # Add hexbin layer\n",
    "    m.add_layer(\n",
    "        lonboard.H3HexagonLayer(\n",
    "            gdf_centroids,\n",
    "            get_hex_id=lambda row: h3.geo_to_h3(row.geometry.y, row.geometry.x, resolution),\n",
    "            get_fill_color=\"colorScale\",\n",
    "            color_scale=color_scale,\n",
    "            opacity=0.8,\n",
    "            pickable=True,\n",
    "            auto_highlight=True\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0dee37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_lonboard_aggregation(ds_gdf, resolution=7, color_scale='viridis', title=f\"{ds} - Density Map\")\n",
    "# create_lonboard_map(ds_gdf, color_column='area_m2', size_column='area_sqm', size_scale=100, title=f\"{ds} - Capacity Map\")\n",
    "# create_lonboard_heatmap(ds_gdf, weight_column='capacity_mw', radius=1000, intensity=1, title=f\"{ds} - Capacity Heatmap\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1326633e",
   "metadata": {},
   "source": [
    "<!-- from init generated visualization code with copilot -->\n",
    "## Notes\n",
    "\n",
    "These visualization functions provide a comprehensive toolkit for exploring and presenting your PV installation data. Each library has its strengths:\n",
    "\n",
    "- **Folium**: Best for quick interactive web maps with various basemaps and standard visualization types\n",
    "- **PyDeck**: Excellent for 3D visualizations and handling larger datasets with complex visualizations\n",
    "- **Lonboard**: Best performance for very large datasets with GPU acceleration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2d8585",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eo-pv-cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
